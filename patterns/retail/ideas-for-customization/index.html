<!doctype html><html><head><link rel=stylesheet href=https://validatedpatterns.io/sass/patternfly.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://validatedpatterns.io/sass/patternfly-addons.css><link rel=stylesheet href=https://use.fontawesome.com/releases/v6.3.0/css/solid.css><link rel=stylesheet href=https://use.fontawesome.com/releases/v6.3.0/css/fontawesome.css><link rel=stylesheet href=https://use.fontawesome.com/releases/v6.3.0/css/brands.css><title>Ideas for Customization | Validated Patterns</title></head><body><div class=pf-c-page><header class=pf-c-page__header><div class=pf-c-page__header-brand><a href=/ class=pf-c-page__header-brand-link><img src=/images/validated-patterns.png alt="Validated Patterns"></a></div><div class=pf-c-page__header-tools><nav class="pf-c-nav pf-m-horizontal" role=navigation><ul class=pf-c-nav__list><li class="pf-c-nav__item pf-m-current"><a class=pf-c-nav__link href=/patterns/ title=Patterns>Patterns</a></li><li class=pf-c-nav__item><a class=pf-c-nav__link href=/learn/ title="Learn about Validated Patterns">Learn</a></li><li class=pf-c-nav__item><a class=pf-c-nav__link href=/contribute/ title="Contribute to Validated Patterns">Contribute</a></li><li class=pf-c-nav__item><a class=pf-c-nav__link href=/blog/ title=Blog>Blog</a></li><li class=pf-c-nav__item><a class=pf-c-nav__link href=/ci/ title="CI Status">Status</a></li></ul></nav><form id=search action=https://validatedpatterns.io/search/ method=get><div class=pf-c-search-input><div class=pf-c-input-group><div class=pf-c-search-input__bar><span class=pf-c-search-input__text><span class=pf-c-search-input__icon><i class="fas fa-search fa-fw" aria-hidden=true></i></span>
<input id=search-input name=query class=pf-c-search-input__text-input type=text placeholder=Search aria-label=Search></span></div><button form=search class="pf-c-button pf-m-control" type=submit aria-label=Search value=search>
<i class="fas fa-arrow-right" aria-hidden=true></i></button></div></div></form></div></header><div class=pf-c-page__sidebar><div class=pf-c-page__sidebar-body><div class="pf-c-nav pf-m-light"><ul class=pf-c-nav__list><li class=pf-c-nav__item><a href=/patterns/ class=pf-c-nav__link><span class="pf-c-icon pf-m-md pf-m-inline"><span class=pf-c-icon__content><i class="fas fa-angle-left" aria-hidden=true></i></span></span>
<span style=padding-left:10px>Back to Patterns</span></a></li><li class="pf-c-nav__item pf-m-expanded"><a href=https://validatedpatterns.io/patterns/retail/ class=pf-c-nav__link>Retail</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=https://validatedpatterns.io/patterns/retail/getting-started/ class=pf-c-nav__link>Getting Started</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=https://validatedpatterns.io/patterns/retail/store/ class=pf-c-nav__link>Store Sites</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=https://validatedpatterns.io/patterns/retail/application/ class=pf-c-nav__link>Application Demos</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=https://validatedpatterns.io/patterns/retail/components/ class=pf-c-nav__link>Components</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=https://validatedpatterns.io/patterns/retail/troubleshooting/ class=pf-c-nav__link>Troubleshooting</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=https://validatedpatterns.io/patterns/retail/cluster-sizing/ class=pf-c-nav__link>Cluster Sizing</a></li><li class="pf-c-nav__item pf-m-expanded"><a href=https://validatedpatterns.io/patterns/retail/ideas-for-customization/ class="pf-c-nav__link pf-m-current">Ideas for Customization</a></li></ul></div></div></div><main class=pf-c-page__main tabindex=0 data-bs-spy=scroll data-bs-target=#TableOfContents data-bs-offset=50><section class="pf-c-page__main-section pf-m-fill"><div class=pf-u-display-flex><div class=pf-c-content><h1 id=ideas-for-customization>Ideas for Customization</h1></div><aside class="pf-c-jump-links pf-m-vertical sticky pf-m-expandable pf-m-non-expandable-on-2xl" aria-label="Table of contents"><div class=pf-c-jump-links__header><div class=pf-c-jump-links__label><h1>Table of Contents</h1></div></div><nav id=TableOfContents><ul class=pf-c-jump-links__list><li class=pf-c-jump-links__item><a class=pf-c-jump-links__link href=#ideas-for-customization>Ideas for Customization</a></li></ul></nav></aside></div></section><section id=prefooter class="pf-c-page__main-section footer-dark"><div class="pf-l-grid prefooter-menu-grid"><div class="pf-l-grid__item pf-m-12-col-on-sm pf-m-6-col-on-md pf-m-offset-1-col-on-md pf-u-mb-lg pf-u-mb-0-on-sm"><div class="pf-l-grid pf-u-py-xl"><div class="pf-l-grid__item pf-m-6-col-on-sm pf-m-4-col-on-md pf-u-ml-md pf-u-ml-0-on-md pf-u-mb-xl pf-u-mb-0-on-md"><p class="pf-c-title footer-menu-title">QUICKLINKS</p><nav aria-label="Quick Links"><ul class="pf-c-list pf-m-plain"><li><i class="prefooter-icon fa-brands fa-github"></i>
<a class=footer-link aria-label="GitHub repository" href=https://github.com/hybrid-cloud-patterns>GitHub repository</a></li><li><i class="prefooter-icon fas fa-check-circle"></i>
<a class=footer-link aria-label="Available patterns" href=/patterns>Available patterns</a></li></ul></nav></div><div class="pf-l-grid__item pf-m-6-col-on-sm pf-m-4-col-on-md pf-u-mt-lg pf-u-mt-0-on-sm pf-u-ml-md pf-u-ml-0-on-md pf-u-mb-xl pf-u-mb-0-on-md"><p class="pf-c-title footer-menu-title">CONTRIBUTE</p><nav aria-label=Contribute><ul class="pf-c-list pf-m-plain ws-org-pfsite-footer-menu-list"><li><i class="prefooter-icon fas fa-pen"></i>
<a class=footer-link aria-label="Documentation contributor guidelines" href=/contribute/contribute-to-docs/>Documentation</a></li><li><i class="prefooter-icon fas fa-code"></i>
<a class=footer-link aria-label="How to create a new pattern" href=/contribute/creating-a-pattern/>Patterns</a></li></ul></nav></div><div class="pf-l-grid__item pf-m-6-col-on-sm pf-m-4-col-on-md pf-u-mt-lg pf-u-mt-0-on-md pf-u-ml-md pf-u-ml-0-on-md"><p class="pf-c-title footer-menu-title">STAY IN TOUCH</p><nav aria-label="Stay in touch"><ul class="pf-c-list pf-m-plain"><li><i class="prefooter-icon fas fa-envelope"></i>
<a href=https://groups.google.com/g/hybrid-cloud-patterns class=footer-link target=top aria-label="Join the Validated Patterns mailing list">Mailing list</a></li></ul></nav></div></div></div><div class="pf-l-grid__item pf-m-12-col-on-sm pf-m-4-col-on-md"><div class="pf-l-grid pf-u-pt-xl"><div class="pf-l-grid__item pf-u-px-xl"><a class="pf-c-page__header-brand-link pf-c-brand pf-u-pb-md" href=/><img class=pf-c-brand src=/images/validated-patterns.png alt="Validated Patterns"></a><p class=footer-link>Validated Patterns are an evolution of how you deploy applications in a hybrid cloud. With a pattern, you can automatically deploy a full application stack through a GitOps-based framework. With this framework, you can create business-centric solutions while maintaining a level of Continuous Integration (CI) over your application.</p></div></div></div></div></section><footer id=footer class="footer-dark pf-m-no-fill pf-l-flex footer-center"><div class=pf-l-flex__item><a href=//www.redhat.com target=top aria-label="Visit Red Hat.com"><img src=/images/RHlogo.svg alt="Red Hat logo" width=145px height=613px></a><span class=site-copyright>Copyright &copy; 2023 Red Hat, Inc.</span></div><script src=/js/bootstrap.bundle.js></script>
<script>window.store={"https://validatedpatterns.io/learn/about/":{title:"About Validated Patterns",tags:[],content:`About Validated Patterns Validated Patterns and upstream Community Patterns are a natural progression from reference architectures with additional value. Here is a brief video to explain what patterns are all about:
This effort is focused on customer solutions that involve multiple Red Hat products. The patterns include one or more applications that are based on successfully deployed customer examples. Example application code is provided as a demonstration, along with the various open source projects and Red Hat products required to for the deployment to work. Users can then modify the pattern for their own specific application.
How do we select and produce a pattern? We look for novel customer use cases, obtain an open source demonstration of the use case, validate the pattern with its components with the relevant product engineering teams, and create GitOps based automation to make them easily repeatable and extendable.
The automation also enables the solution to be added to Continuous Integration (CI), with triggers for new product versions (including betas), so that we can proactively find and fix breakage and avoid bit-rot.
Who should use these patterns? It is recommended that architects or advanced developers with knowledge of Kubernetes and Red Hat OpenShift Container Platform use these patterns. There are advanced Cloud Native concepts and projects deployed as part of the pattern framework. These include, but are not limited to, OpenShift Gitops (ArgoCD), Advanced Cluster Management (Open Cluster Management), and OpenShift Pipelines (Tekton)
General Structure All patterns assume an OpenShift cluster is available to deploy the application(s) that are part of the pattern. If you do not have an OpenShift cluster, you can use cloud.redhat.com.
The documentation will use the oc command syntax but kubectl can be used interchangeably. For each deployment it is assumed that the user is logged into a cluster using the oc login command or by exporting the KUBECONFIG path.
The diagram below outlines the general deployment flow of a datacenter application.
But first the user must create a fork of the pattern repository. This allows changes to be made to operational elements (configurations etc.) and to application code that can then be successfully made to the forked repository for DevOps continuous integration (CI). Clone the directory to your laptop/desktop. Future changes can be pushed to your fork.
Make a copy of the values file. There may be one or more values files. E.g. values-global.yaml and/or values-datacenter.yaml. While most of these values allow you to specify subscriptions, operators, applications and other application specifics, there are also secrets which may include encrypted keys or user IDs and passwords. It is important that you make a copy and do not push your personal values file to a repository accessible to others!
Deploy the application as specified by the pattern. This may include a Helm command (helm install) or a make command (make deploy).
When the workload is deployed the pattern first deploys OpenShift GitOps. OpenShift GitOps will then take over and make sure that all application and the components of the pattern are deployed. This includes required operators and application code.
Most patterns will have an Advanced Cluster Management operator deployed so that multi-cluster deployments can be managed.
Edge Patterns Some patterns include both a data center and one or more edge clusters. The diagram below outlines the general deployment flow of applications on an edge application. The edge OpenShift cluster is often deployed on a smaller cluster than the datacenter. Sometimes this might be a three node cluster that allows workloads to be deployed on the master nodes. The edge cluster might be a single node cluster (SN0). It might be deployed on bare metal, on local virtual machines or in a public/private cloud. Provision the cluster (see above)
Import/join the cluster to the hub/data center. Instructions for importing the cluster can be found [here]. You&rsquo;re done. When the cluster is imported, ACM on the datacenter will deploy an ACM agent and agent-addon pod into the edge cluster. Once installed and running ACM will then deploy OpenShift GitOps onto the cluster. Then OpenShift GitOps will deploy whatever applications are required for that cluster based on a label.
OpenShift GitOps (a.k.a ArgoCD) When OpenShift GitOps is deployed and running in a cluster (datacenter or edge) you can launch its console by choosing ArgoCD in the upper left part of the OpenShift Console (TO-DO whenry to add an image and clearer instructions here)
`,url:"https://validatedpatterns.io/learn/about/",breadcrumb:"/learn/about/"},"https://validatedpatterns.io/contribute/contribute-to-docs/":{title:"Contributor's guide",tags:[],content:` Contribute to Hybrid Cloud Patterns documentation Different ways to contribute There are a few different ways you can contribute to Hybrid Cloud Patterns documentation:
Email the Hybrid Cloud Patterns team at hybrid-cloud-patterns@googlegroups.com.
Create a GitHub or Jira issue .
Submit a pull request (PR). To create a PR, create a local clone of your own fork of the Hybrid Cloud Patterns docs repository, make your changes, and submit a PR. This option is best if you have substantial changes.
Contribution workflow When you submit a PR, the Hybrid Cloud Patterns Docs team reviews the PR and arranges further reviews by Quality Engineering (QE), subject matter experts (SMEs), and others, as required. If the PR requires changes, updates, or corrections, the reviewers add comments in the PR. The documentation team merges the PR after you have implemented all feedback, and you have squashed all commits.
Repository organization ├── archetypes ├── content │ └── blog | └── contribute | └── learn | └── patterns | └── multicloud-gitops | | └──_index.adoc | | └──mcg-getting-started.adoc | | | └── medical-diagnosis | └──_index.adoc | └── medical-diagnosis-assembly.adoc ├── layouts │ └── _default | └── blog ├── modules │ └── multicloud-gitops-logical-architecture.adoc │ └── multicloud-gitops-physical-architecture.adoc ├── static | └── images ├── themes/patternfly ├── config.yaml └── README.adoc Install and set up the tools and software Create a GitHub account Before you can contribute to Hybrid Cloud Patterns documentation, you must sign up for a GitHub account.
Set up authentication When you have your account set up, follow the instructions to generate and set up SSH keys on GitHub for authentication between your workstation and GitHub.
Confirm authentication is working correctly with the following command:
$ ssh -T git@github.com Fork and clone the Hybrid Cloud Patterns documentation repository You must fork and set up the Hybrid Cloud Patterns documentation repository on your workstation so that you can create PRs and contribute. These steps must only be performed during initial setup.
Fork the https://github.com/hybrid-cloud-patterns/docs repository into your GitHub account from the GitHub UI. Click Fork in the upper right-hand corner.
In the terminal on your workstation, change into the directory where you want to clone the forked repository.
Clone the forked repository onto your workstation with the following command, replacing &lt;user_name&gt; with your actual GitHub username.
$ git clone git@github.com:&lt;user_name&gt;/docs.git Change into the directory for the local repository you just cloned.
$ cd docs Add an upstream pointer back to the Hybrid Cloud Patterns’s remote repository, in this case docs.
$ git remote add upstream git@github.com:hybrid-cloud-patterns/docs.git This ensures that you are tracking the remote repository to keep your local repository in sync with it.
Install Asciidoctor The Hybrid Cloud Patterns documentation is created in AsciiDoc language, and is processed with AsciiDoctor, which is an AsciiDoc language processor.
Prerequisites The following are minimum requirements:
A bash shell environment (Linux and OS X include a bash shell environment)
A web browser (Mozilla Firefox, Google Chrome, or Safari)
Web browser add-ons (preview only)
An editor that can strip trailing whitespace
An Asciidoctor installer
Preview the documentation using a container image You can use the container image to build the Hybrid Cloud Patterns documentation, locally. To do so, ensure that you have installed the make and podman tools.
In the terminal window, navigate to the local instance of the hybrid-cloud-patterns/docs repository and run the following command:
$ make serve Verification A preview is available on your browser at localhost:4000.
Documentation guidelines Documentation guidelines for contributing to the Hybrid Cloud Patterns Docs
General guidelines When authoring content, follow these style guides:
Red Hat Supplementary Style Guide
IBM Style, especially word usage
When asked for an IBMid, Red Hat associates can use their Red Hat e-mail. Modular documentation reference guide
Modular documentation templates
Modular documentation terms Modular doc entity Description Asssembly
An assembly is a collection of modules that describes how to accomplish a user story.
Concept module
A concept contains information to support the tasks that users want to do and must not include task information like commands or numbered steps. In most cases, create your concepts as individual modules and include them in appropriate assemblies. Avoid using gerunds in concept titles. &#34;About &lt;concept&gt;&#34; is a common concept module title.
Procedure module
A procedure contains the steps that users follow to complete a process or task. Procedures contain ordered steps and explicit commands. In most cases, create your procedures as individual modules and include them in appropriate assemblies. Use a gerund in the procedure title, such as &#34;Creating&#34;.
Reference module
A reference module provides data that users might want to look up, but do not need to remember. A reference module has a very strict structure, often in the form of a list or a table. A well-organized reference module enables users to scan it quickly to find the details they want.
Naming conventions for assembly and module files Use lowercase separated by dash. Create assembly and module file names that accurately and closely reflect the title of the assembly or module.
Examples designing-guided-decision-tables.adoc (Assembly of guided decision table modules)
guided-decision-tables.adoc (Concept module)
creating-guided-decision-tables.adoc (Procedure module for creating)
guided-decision-table-examples.adoc (Reference module with examples)
Content type attributes Each .adoc file must contain a :_content-type: attribute in its metadata that indicates its file type. This information is used by some publication processes to sort and label files.
Add the attribute from the following list that corresponds to your file type:
:_content-type: ASSEMBLY
:_content-type: CONCEPT
:_content-type: PROCEDURE
:_content-type: REFERENCE
See, Assembly file metadata and Module file metadata.
Naming conventions for directories Use lowercase. For directory with a multiple-word name, use lowercase separated by dash, for example multicloud-gitops.
Language and grammar Consider the following guidelines:
Use present tense.
Use active voice.
Use second person perspective (you).
Avoid first person perspective (I, we, us).
Be gender neutral.
Use the appropriate tone.
Write for a global audience.
Titles and headings Use sentence-style capitalization in all titles and section headings. Ensure that titles focus on customer tasks instead of the product.
For assemblies and procedure modules, use a gerund form in headings, such as:
Creating
Managing
Using
For modules that do not include any procedure, use a noun phrase, for example Red Hat Process Automation Manager API reference.
Writing assemblies For more information about forming assemblies, see the Red Hat modular docs reference guide and the assembly template.
Assembly file metadata Every assembly file should contain the following metadata at the top, with no line spacing in between, except where noted:
:_content-type: ASSEMBLY (1) [id=&#34;&lt;unique-heading-for-assembly&gt;&#34;] (2) = Assembly title (3) include::_common-docs/common-attributes.adoc[] (4) :context: &lt;unique-context-for-assembly&gt; (5) (6) :toc: (7) 1 The content type for the file. For assemblies, always use :_content-type: ASSEMBLY. Place this attribute before the anchor ID or, if present, the conditional that contains the anchor ID. 2 A unique anchor ID for this assembly. Use lowercase. Example: cli-developer-commands 3 Human readable title (notice the &#39;=&#39; top-level header) 4 Includes attributes common to Hybrid Cloud Patterns docs. 5 Context used for identifying headers in modules that is the same as the anchor ID. Example: cli-developer-commands. 6 A blank line. You must have a blank line here before the toc. 7 The table of contents for the current assembly. After the heading block and a single whitespace line, you can include any content for this assembly.
Writing modules For more information about creating modules, see the Red Hat Modular documentation reference guide.
Module file metadata Every module should be placed in the modules folder and should contain the following metadata at the top:
// * list of assemblies where this module is included (1) :_content-type: &lt;TYPE&gt; (2) [id=&#34;&lt;module-anchor&gt;_{context}&#34;] (3) = Module title (4) 1 The content type for the file. Replace &lt;TYPE&gt; with the actual type of the module, CONCEPT, REFERENCE, or PROCEDURE. Place this attribute before the anchor ID or, if present, the conditional that contains the anchor ID. 2 List of assemblies in which this module is included. 3 A module anchor with {context} that must be lowercase and must match the module’s file name. The {context} variable must be preceded by an underscore (_) when declared in an anchor ID. 4 Human readable title. To ensure consistency in the results of the leveloffset values in include statements, you must use a level one heading ( = ) for the module title. Example:
// Module included in the following assemblies: // // * cli_reference/developer-cli-commands.adoc :_content-type: REFERENCE [id=&#34;cli-basic-commands_{context}&#34;] = Basic CLI commands Attribute files AsciiDoc attributes are variables you can use in common files to:
avoid hard-coding brand-specific information,
share content between multiple brands more easily.
All attribute files must be placed in the a separate attributes file. For example, common-docs directory.
It is acceptable to group related attributes in the common-attributes.adoc file under a comment, as shown in the following example:
//ACM rh-rhacm-product: Red Hat Advanced Cluster Management (RHACM) :rh-shortname: RHACM //GitOps :gitops-product: Red Hat OpenShift GitOps :gitops-shortname: GitOps For more information on attributes, see link: https://docs.asciidoctor.org/asciidoc/latest/key-concepts/#attributes.
Formatting Use the following links to refer to AsciiDoc markup and syntax.
AsciiDoc Mark-up Quick Reference for Red Hat Documentation
AsciiDoc Syntax Quick Reference
If you are graduating to AsciiDoc from Markdown, see the AsciiDoc to Markdown syntax comparison by example.
Formatting commands and code blocks To enable syntax highlighting, use [source,terminal] for any terminal commands, such as oc commands and their outputs. For example:
[source,terminal] ---- $ oc get nodes ---- To enable syntax highlighting for a programming language, use [source] tags used in the code block. For example:
[source,yaml]
[source,go]
[source,javascript]
`,url:"https://validatedpatterns.io/contribute/contribute-to-docs/",breadcrumb:"/contribute/contribute-to-docs/"},"https://validatedpatterns.io/patterns/multicloud-gitops/mcg-getting-started/":{title:"Getting started",tags:[],content:` Deploying the Multicloud GitOps pattern Prerequisites An OpenShift cluster
To create an OpenShift cluster, go to the Red Hat Hybrid Cloud console.
Select OpenShift -&gt; Clusters -&gt; Create cluster.
The cluster must have a dynamic StorageClass to provision PersistentVolumes. See sizing your cluster.
Optional: A second OpenShift cluster for multicloud demonstration.
The git binary. See Installing Git
The Podman tool. See Installing Podman
The use of this pattern depends on having at least one running Red Hat OpenShift cluster. However, consider creating a cluster for deploying the GitOps management hub assets and a separate cluster for the managed cluster.
If you do not have a running Red Hat OpenShift cluster, you can start one on a public or private cloud by using Red Hat Hybrid Cloud Console.
Procedure Install the tooling dependencies.
Fork the multicloud-gitops repository on GitHub.
Clone the forked copy of this repository.
git clone git@github.com:your-username/multicloud-gitops.git Create a local copy of the secret values file that can safely include credentials. Run the following commands:
cp values-secret.yaml.template ~/values-secret-multicloud-gitops.yaml vi ~/values-secret-multicloud-gitops.yaml Do not commit this file. You do not want to push personal credentials to GitHub. If you do not want to customize the secrets, these steps are not needed. The framework generates a random password for the config-demo application.
Customize the deployment for your cluster. Run the following command:
git checkout -b my-branch vi values-global.yaml git add values-global.yaml git commit values-global.yaml git push origin my-branch Deploy the pattern by running ./pattern.sh make install or by using the Validated Patterns Operator.
Deploying the cluster by using the pattern.sh file To deploy the cluster by using the pattern.sh file, complete the following steps:
Login to your cluster by running the following command:
oc login Optional: Set the KUBECONFIG variable for the kubeconfig file path:
export KUBECONFIG=~/&lt;path_to_kubeconfig&gt; Deploy the pattern to your cluster. Run the following command:
./pattern.sh make install Verify that the Operators have been installed.
To verify, in the OpenShift Container Platform web console, navigate to Operators → Installed Operators page.
Check that the Operator is installed in the openshift-operators namespace and its status is Succeeded.
Verify that all applications are synchronized. Under the project multicloud-gitops-hub click the URL for the hub gitops server. The Vault application is not synched.
As part of this pattern, HashiCorp Vault has been installed. Refer to the section on Vault.
Next steps After the management hub is set up and works correctly, attach one or more managed clusters to the architecture.
For instructions on deploying the edge, refer to Attach a managed cluster (edge) to the management hub.
`,url:"https://validatedpatterns.io/patterns/multicloud-gitops/mcg-getting-started/",breadcrumb:"/patterns/multicloud-gitops/mcg-getting-started/"},"https://validatedpatterns.io/patterns/ansible-edge-gitops/getting-started/":{title:"Getting Started",tags:[],content:`Deploying the Ansible Edge GitOps Pattern General Prerequisites An OpenShift cluster ( Go to the OpenShift console). See also sizing your cluster. Currently this pattern only supports AWS. It could also run on a baremetal OpenShift cluster, because OpenShift Virtualization supports that; there would need to be some customizations made to support it as the default is AWS. We hope that GCP and Azure will support provisioning metal workers in due course so this can be a more clearly multicloud pattern. A GitHub account (and, optionally, a token for it with repositories permissions, to read from and write to your forks) The helm binary, see here Ansible, which is used in the bootstrap and provisioning phases of the pattern install (and to configure Ansible Automation Platform). Please note that when run on AWS, this pattern will provision an additional worker node, which will be a metal instance (c5n.metal) to run the Edge Virtual Machines. This worker is provisioned through the OpenShift MachineAPI and will be automatically cleaned up when the cluster is destroyed. The use of this pattern depends on having a running Red Hat OpenShift cluster. It is desirable to have a cluster for deploying the GitOps management hub assets and a separate cluster(s) for the managed cluster(s).
If you do not have a running Red Hat OpenShift cluster you can start one on a public or private cloud by using Red Hat&rsquo;s cloud service.
Credentials Required in Pattern In addition to the openshift cluster, you will need to prepare a number of secrets, or credentials, which will be used in the pattern in various ways. To do this, copy the values-secret.yaml template to your home directory as values-secret.yaml and replace the explanatory text as follows:
A username and SSH Keypair (private key and public key). These will be used to provide access to the Kiosk VMs in the demo. --- # NEVER COMMIT THESE VALUES TO GIT version: &#34;2.0&#34; secrets: - name: kiosk-ssh fields: - name: username value: &#39;Username of user to attach privatekey and publickey to - cloud-user is a typical value&#39; - name: privatekey value: &#39;Private ssh key of the user who will be able to elevate to root to provision kiosks&#39; - name: publickey value: &#39;Public ssh key of the user who will be able to elevate to root to provision kiosks&#39; A Red Hat Subscription Management username and password. These will be used to register Kiosk VM templates to the Red Hat Content Delivery Network and install content on the Kiosk VMs to run the demo. - name: rhsm fields: - name: username value: &#39;username of user to register RHEL VMs&#39; - name: password value: &#39;password of rhsm user in plaintext&#39; Container &ldquo;extra&rdquo; arguments which will set the admin password for the ignition application when it&rsquo;s running. - name: kiosk-extra fields: # Default: &#39;--privileged -e GATEWAY_ADMIN_PASSWORD=redhat&#39; - name: container_extra_params value: &#34;Optional extra params to pass to kiosk ignition container, including admin password&#34; A userData block to use with cloud-init. This will allow console login as the user you specify (traditionally cloud-user) with the password you specify. The value in cloud-init is used as the default; roles in the edge-gitops-vms chart can also specify other secrets to use by referencing them in the role block. - name: cloud-init fields: - name: userData value: |- #cloud-config user: &#39;username of user for console, probably cloud-user&#39; password: &#39;a suitable password to use on the console&#39; chpasswd: { expire: False } A manifest file with an entitlement to run Ansible Automation Platform. This file (which will be a .zip file) will be posted to to Ansible Automation Platform instance to enable its use. Instructions for creating a manifest file can be found here - name: aap-manifest fields: - name: b64content path: &#39;full pathname of file containing Satellite Manifest for entitling Ansible Automation Platform&#39; base64: true Prerequisites for deployment via make install If you are going to install via make install from your workstation, you will need the following tools and packages:
{% include prerequisite-tools.md %}
And additionally, the following ansible collections:
community.okd redhat_cop.controller_configuration awx.awx To see what collections are installed:
ansible-galaxy collection list
To install a collection that is not currently installed:
ansible-galaxy collection install &lt;collection&gt;
How to deploy Login to your cluster using oc login or exporting the KUBECONFIG
oc login or set KUBECONFIG to the path to your kubeconfig file. For example:
export KUBECONFIG=~/my-ocp-env/hub/auth/kubeconfig Fork the ansible-edge-gitops repo on GitHub. It is necessary to fork to preserve customizations you make to the default configuration files.
Clone the forked copy of this repository.
git clone git@github.com:your-username/ansible-edge-gitops.git Create a local copy of the Helm values file that can safely include credentials
WARNING: DO NOT COMMIT THIS FILE
You do not want to push personal credentials to GitHub.
cp values-secret.yaml.template ~/values-secret.yaml vi ~/values-secret.yaml Customize the deployment for your cluster (Optional - the defaults in values-global.yaml are designed to work in AWS):
git checkout -b my-branch vi values-global.yaml git add values-global.yaml git commit values-global.yaml git push origin my-branch Please review the Patterns quick start page. This section describes deploying the pattern using pattern.sh. You can deploy the pattern using the validated pattern operator. If you do use the operator then skip to Validating the Environment below.
(Optional) Preview the changes. If you&rsquo;d like to review what is been deployed with the pattern, pattern.sh provides a way to show what will be deployed.
./pattern.sh make show Apply the changes to your cluster. This will install the pattern via the Validated Patterns Operator, and then run any necessary follow-up steps.
./pattern.sh make install The installation process will take between 45-60 minutes to complete. If you want to know the details of what is happening during that time, the entire process is documented here.
Installation Validation Check the operators have been installed using the OpenShift console
OpenShift Console Web UI -&gt; Installed Operators The screen should like this when installed via make install:
Check all applications are synchronised Under the project ansible-edge-gitops-hub click on the URL for the hubgitopsserver. All applications will sync, but this takes time as ODF has to completely install, and OpenShift Virtualization cannot provision VMs until the metal node has been fully provisioned and ready. Additionally, the Dynamic Provision Kiosk Template in AAP must complete; it can only start once the VMs have provisioned and are running:
While the metal node is building, the VMs in OpenShift console will show as &ldquo;Unschedulable.&rdquo; This is normal and expected, as the VMs themselves cannot run until the metal node completes provisioning and is ready. Under Virtualization &gt; Virtual Machines, the virtual machines will eventually show as &ldquo;Running.&rdquo; Once they are in &ldquo;Running&rdquo; state the Provisioning workflow will run on them, and install Firefox, Kiosk mode, and the Ignition application on them: Finally, the VM Consoles will show the Ignition introduction screen. You can choose any of these options; this tutorial assumes you chose &ldquo;Ignition&rdquo;: You should be able to login to the application with the userid &ldquo;admin&rdquo; and the password you specified as the GATEWAY_ADMIN_PASSWORD in container_extra_params in your values-secret.yaml file. Please see Installation Details for more information on the steps of installation.
Please see Ansible Automation Platform for more information on how this pattern uses the Ansible Automation Platform Operator for OpenShift.
Please see OpenShift Virtualization for more information on how this pattern uses OpenShift Virtualization.
Infrastructure Elements of this Pattern Ansible Automation Platform A fully functional installation of the Ansible Automation Platform operator is installed on your OpenShift cluster to configure and maintain the VMs for this demo. AAP maintains a dynamic inventory of kiosk machines and can configure a VM from template to fully functional kiosk in about 10 minutes.
OpenShift Virtualization OpenShift Virtualization is a Kubernetes-native way to run virtual machine workloads. It is used in this pattern to host VMs simulating an Edge environment; the chart that configures the VMs is designed to be flexible to allow easy customization to model different VM sizes, mixes, versions and profiles for future pattern development.
Inductive Automation Ignition The goal of this pattern is to configure 2 VMs running Firefox in Kiosk mode displaying the demo version of the Ignition application running in a podman container. Ignition is a popular tool in use with Oil and Gas companies; it is included as a real-world example and as an item to spark imagination about what other applications could be installed and managed this way.
The container used for this pattern is the container image published by Inductive Automation.
HashiCorp Vault Vault is used as the authoritative source for the Kiosk ssh pubkey via the External Secrets Operator. As part of this pattern HashiCorp Vault has been installed. Refer to the section on Vault.
Next Steps Help &amp; Feedback Report Bugs `,url:"https://validatedpatterns.io/patterns/ansible-edge-gitops/getting-started/",breadcrumb:"/patterns/ansible-edge-gitops/getting-started/"},"https://validatedpatterns.io/patterns/devsecops/getting-started/":{title:"Getting Started",tags:[],content:`Deploying the Multicluster DevSecOps Pattern Prerequisites An OpenShift cluster (Go to the OpenShift console). Cluster must have a dynamic StorageClass to provision PersistentVolumes. See also sizing your cluster. A second OpenShift cluster for development using secure CI pipelines. A third OpenShift cluster for production. (optional but desirable) A GitHub account (and a token for it with repositories permissions, to read from and write to your forks) Tools Podman and Git. (see below) If you do not have running Red Hat OpenShift clusters you can start one on a public or private cloud by using Red Hat&rsquo;s cloud service.
Credentials Required in Pattern In addition to the openshift cluster, you will need to prepare a number of secrets, or credentials, which will be used in the pattern in various ways. To do this, copy the values-secret.yaml template to your home directory as values-secret.yaml and replace the explanatory text as follows:
Your git repository username and password. The password must be base64 encoded. --- secrets: # NEVER COMMIT THESE VALUES TO GIT git: # Go to: https://github.com/settings/tokens # Then: echo -n &#39;your string value&#39; | base64 username: USERNAME password: &#39;encoded password in single quotes&#39; You application secret. TBD This may change when the application is changed. --- secrets: # NEVER COMMIT THESE VALUES TO GIT config-demo: # Secret used for demonstrating vault storage, external secrets, and ACM distribution secure secret: PLAINTEXT Preparing to deploy Install the installation tooling dependencies. See Patterns quick start
Git command line tool (git) Podman command line tool (podman) Fork the Multicluster DevSecOps repository on GitHub. It is necessary to fork because your fork will be updated as part of the GitOps and DevSecOps processes. The Fork information and pull down menu can be found on the top right of the GitHub page for a pattern. Select the pull down an select Create a new fork.
Clone the forked copy of the multicluster-devsecops repository. Use branch v1.0. (Clone in an appropriate sub-dir)
git clone git@github.com:{your-username}/multicluster-devsecops.git cd multicluster-devsecops git checkout v1.0 You could create your own branch where your specific values will be pushed to:
git checkout -b my-branch A values-secret.yaml file is used to automate setup of secrets needed for:
A Git repository (E.g. Github, GitLab etc.) Quay registry deployment secrets. Any application secrets that are needed. DO NOT COMMIT THIS FILE. You do not want to push personal credentials to GitHub. Instead copy the template file values-secret.yaml.template to your home directory. Change the values in that file to ones that fit your environment.
cp values-secret.yaml.template ~/values-secret.yaml vi ~/values-secret.yaml Customize the deployment for your cluster. Change the appropriate values in values-global.yaml
vi values-global.yaml git add values-global.yaml git commit values-global.yaml git push origin my-branch Getting Started Video Make sure to set up the values-secret.yaml and values-global.yaml correctly (see above). For a demonstration of the deployment, click on the image below to launch the video.
How to deploy Please review the Patterns quick start page. This section describes deploying the pattern using pattern.sh. You can deploy the pattern using the validated pattern operator. If you do use the operator then skip to Validating the Environment below.
Preview the changes. If you&rsquo;d like to review what is been deployed with the pattern, pattern.sh provides a way to show what will be deployed.
./pattern.sh make show Login to your cluster using oc login or exporting the kubernetes kubeconfig file with KUBECONFIG:
oc login or
export KUBECONFIG=~/&lt;path-to-kubeconfig/kubeconfig Apply the changes to your cluster
./pattern.sh make install Validating the Environment Check the operators have been installed
OpenShift Console UI -&gt; Installed Operators Navigate to the OpenShift GitOps instances using the links on the top right hand side of the screen.
The most important ArgoCD instance to examine at this point is hub-gitops-server. This is where all the applications for the hub (datacenter), including the test environment, can be tracked.
Apply the secrets from the values-secret.yaml to the secrets management Vault. This can be done through Vault&rsquo;s UI - manually without the file. The required secrets and scopes are:
secret/git git username &amp; password (GitHub token) secret/quay The admin username and password and email. secret/imageregistry Quay.io or DockerHub username &amp; password Or you can set up the secrets using the command-line by running the following (Ansible) playbook.
scripts/setup-secrets.yaml Using the Vault UI check that the secrets have been setup.
For more information on secrets management see here. For information on Hashicorp&rsquo;s Vault see here
Check all applications are synchronized in OpenShift GitOps.
Check the ACM policy deployment After ACM is installed a message regarding a &ldquo;Web console update is available&rdquo; may be displayed. Click on the &ldquo;Refresh web console&rdquo; link.
Navigate to the ACM hub console. On the upper-left side you&rsquo;ll see a pull down labeled &ldquo;local-cluster&rdquo;. Click on this and select &ldquo;All Clusters&rdquo; from this pull down. This will navigate to the ACM console and to its &ldquo;Clusters&rdquo; section
The Governance dashboard shows high level information on Policy set violations and Policy violations.
Navigate to the Governance page and select the Policy sets Governance tab. There are two policy sets deployed, one for the hub, openshift-plus-hub, and one for managed clusters, openshift-plus-managed.
Explore the Policies tab and select some policies to examine. The image below shows and example of ACM policy status for a three cluster deployment.
Checking the ACS deployment Select the stackrox Project (namespace). Navigate to the OCP Networking-&gt;Routes page. Click on the central route location URL. It might take a few minutes for this link to be active. When it does it will launch a new tab with the ACS Central login page.
Return to the OCP console tab and navigate to the Workload-&gt;Secrets page. Find the central-htpasswd secret and select it.
On the central-htpasswd page, scroll to the Data section and select the copy icon on the right in the password field.
Return to the ACS Central tab and paste the password into the password field. Make sure that the Username is admin.
This will bring you to the ACS Central dashboard page. At first it may not show any clusters showing but as the ACS secured deployment on the hub syncs with ACS central on the hub then information will start to show.
Return to this dashboard later after deploying the development and production clusters so you can see their information in this dashboard. All clusters in this pattern are ACS secured and therefore ought to show up in this dashboard when those clusters join the hub and are fully deployed.
Check the Quay deployment Select the quay-enterprise project (namespace). Navigate to the OCP Networking-&gt;Routes page. Click on the quay-registry-quay route location URL (standard Quay naming apparently). It might take a few minutes for this link to be active. When it does it will launch a new tab with the Quay login page.
An initial quayadmin account has already been created for you as part of the deployment. The password is quayadmin123. If you want to change initial admin user name and password you can do so by editing the charts/hub/quay/values.yaml or by adding those entries to the values-global.yaml file. Log in using the username and password.
After logging in, the private Quay registry dashboard will be displayed.
Completing the Quay Bridge with a bearer token Managed clusters use a Quay Bridge in order to provide integration between the cluster and Quay Enterprise running on the hub/central cluster. The Quay Bridge looks like a local OpenShift registry but acts as a proxy to the Quay Enterprise registry. Currently there is a manual step to completing the Quay Bridge setup for managed clusters.
Log in to Red Hat Quay through the web UI.
Select the organization for which the external application will be configured.
On the navigation pane, select Applications.
Select Create New Application and enter a name for the new application, for example, openshift.
On the OAuth Applications page, select your application, for example, devel-automation.
On the navigation pane, select Generate Token.
Select the following fields and press Generate Access Token at the bottom of the page:
Administer Organization
Administer Repositories
Create Repositories
View all visible repositories
Read/Write to any accessible repositories
Administer User
Read User Information
Review the assigned permissions.
Select Authorize Application and then confirm confirm the authorization by selecting Authorize Application at the bottom of the page.
Save/copy the generated access token.
At a command line prompt that has KUBECONFIG set to the central/hub cluster&rsquo;s auth/kubeconfig file, run the following command with the token that was saved/copied above.
$ oc create secret -n openshift-operators generic quay-integration --from-literal=token=&lt;access_token&gt;
There is a ACM policy that will make sure that this is copied out to the managed clusters. If there are any problems with the managed cluster&rsquo;s Quay Bridge quay-integration token, you can run the same command on the managed cluster.
Creating an ACS/Quay integration Advanced Cluster Security needs to be integrated with Quay Enterprise registry. Currently there is no way to automate this as it requires the above manual step to generate the OAuth token.
On the ACS console, under ==Platform Configuration== on the left hand side, select ==Integrations==.
Under Image Integrations select ==Red Hat Quay.io==
In the Integrations &gt; Quay.io page select ==New Integration== and fill out the form: Give it a name like hub-quay and select Registry as the type. Provide the URL for Quay Enterprise and the OAuth token generated in above. Press ==Save==. Here is an example.
Next Steps Help &amp; Feedback{: .btn .fs-5 .mb-4 .mb-md-0 .mr-2 } Report Bugs{: .btn .btn-red .fs-5 .mb-4 .mb-md-0 .mr-2 }
Once the hub has been setup correctly and confirmed to be working, you can:
Add a dedicated development cluster to deploy the CI pipelines using ACM
Add a dedicated production cluster to deploy production using ACM
Once the hub, production and devel clusters have been deployed you will want to check out and test the Multi-Cluster DevSecOps demo code. You can find that here TBD
a. Making configuration changes with GitOps TBD a. Making application changes using DevOps TBD
Uninstalling Probably wont work
Turn off auto-sync
helm upgrade manuela . --values ~/values-secret.yaml --set global.options.syncPolicy=Manual
Remove the ArgoCD applications (except for manuela-datacenter)
a. Browse to ArgoCD a. Go to Applications a. Click delete a. Type the application name to confirm a. Chose &ldquo;Foreground&rdquo; as the propagation policy a. Repeat
Wait until the deletions succeed
tbd should be the only remaining application
Complete the uninstall
helm delete tbd
Check all namespaces and operators have been removed
`,url:"https://validatedpatterns.io/patterns/devsecops/getting-started/",breadcrumb:"/patterns/devsecops/getting-started/"},"https://validatedpatterns.io/patterns/industrial-edge/getting-started/":{title:"Getting Started",tags:[],content:`Deploying the Industrial Edge Pattern Prerequisites An OpenShift cluster (Go to the OpenShift console). Cluster must have a dynamic StorageClass to provision PersistentVolumes. See also sizing your cluster.
(Optional) A second OpenShift cluster for edge/factory
A GitHub account (and a token for it with repositories permissions, to read from and write to your forks)
A quay account with the following repositories set as public:
http-ionic httpd-ionic iot-anomaly-detection iot-consumer iot-frontend iot-software-sensor The use of this blueprint depends on having at least one running Red Hat OpenShift cluster. It is desirable to have a cluster for deploying the data center assets and a separate cluster(s) for the factory assets.
If you do not have a running Red Hat OpenShift cluster you can start one on a public or private cloud by using Red Hat&rsquo;s cloud service.
Prerequisites For installation tooling dependencies, see Patterns quick start
How to deploy Fork the industrial-edge repository on GitHub. It is necessary to fork because your fork will be updated as part of the GitOps and DevOps processes.
Fork the manuela-dev repository on GitHub. It is necessary to fork this repository because the GitOps framework will push tags to this repository that match the versions of software that it will deploy.
Clone the forked copy of the industrial-edge repository. Create a deployment branch using the branch v2.3.
git clone git@github.com:{your-username}/industrial-edge.git cd industrial-edge git checkout v2.3 git switch -c deploy-v2.3 A values-secret-industrial-edge.yaml file is used to automate setup of secrets needed for:
A git repository hosted on a service such as GitHub, GitLab, or so on. A container image registry (E.g. Quay) S3 storage (E.g. AWS) DO NOT COMMIT THIS FILE. You do not want to push personal credentials to GitHub.
cp values-secret.yaml.template ~/values-secret-industrial-edge.yaml vi ~/values-secret-industrial-edge.yaml Customize the following secret values.
version: &#34;2.0&#34; secrets: - name: imageregistry fields: # E.G. Quay -&gt; Robot Accounts -&gt; Robot Login - name: username value: &lt;Your-Robot-Account&gt; - name: password value: &lt;Your-RobotAccount-Password&gt; - name: git fields: # Go to: https://github.com/settings/tokens - name: username value: &lt;github-user&gt; - name: password value: &lt;github-token&gt; - name: aws fields: - name: aws_access_key_id ini_file: ~/.aws/credentials ini_key: aws_access_key_id - name: aws_secret_access_key ini_file: ~/.aws/credentials ini_key: aws_secret_access_key Customize the deployment for your cluster. Change the appropriate values in values-global.yaml
main: clusterGroupName: datacenter global: pattern: industrial-edge options: useCSV: False syncPolicy: Automatic installPlanApproval: Automatic imageregistry: account: PLAINTEXT hostname: quay.io type: quay git: hostname: github.com account: PLAINTEXT #username: PLAINTEXT email: SOMEWHERE@EXAMPLE.COM dev_revision: main s3: bucket: name: BUCKETNAME region: AWSREGION message: aggregation: count: 50 custom: endpoint: enabled: false vi values-global.yaml git add values-global.yaml git commit -m &#34;Added personal values to values-global&#34; values-global.yaml git push origin deploy-v2.3 You can deploy the pattern using the Validated Patterns Operator directly. If you deploy the pattern using the Validated Patterns Operator, installed through Operator Hub, you will need to run make load-secrets through a terminal session on your laptop or bastion host.
If you deploy the pattern through a terminal session on your laptop or bastion host login to your cluster by using theoc login command or by exporting the KUBECONFIG file.
oc login or
export KUBECONFIG=~/my-ocp-cluster/auth/kubeconfig Apply the changes to your cluster from the root directory of the pattern.
./pattern.sh make install The make install target deploys the Validated Patterns Operator, all the resources that are defined in the values-datacenter.yaml and runs the make load-secrets target to load the secrets configured in your values-secrets-industrial-edge.yaml file.
Validating the Environment In the OpenShift Container Platform web console, navigate to the Operators → OperatorHub page.
Verify that the following Operators are installed on the HUB cluster:
Operator Name Namespace ------------------------------------------------------ advanced-cluster-management open-cluster-management amq-broker-rhel8 manuela-tst-all amq-streams manuela-data-lake red-hat-camel-k manuela-data-lake seldon-operator manuela-ml-workspace openshift-pipelines-operator- openshift-operators opendatahub-operator openshift-operators patterns-operator openshift-operators Access the ArgoCD environment
You can find the ArgoCD application links listed under the Red Hat applications in the OpenShift Container Platform web console.
You can also obtain the ArgoCD URLs and passwords (optional) by displaying the fully qualified domain names, and matching login credentials, for all ArgoCD instances:
ARGO_CMD=\`oc get secrets -A -o jsonpath=&#39;{range .items[*]}{&#34;oc get -n &#34;}{.metadata.namespace}{&#34; routes; oc -n &#34;}{.metadata.namespace}{&#34; extract secrets/&#34;}{.metadata.name}{&#34; --to=-\\\\n&#34;}{end}&#39; | grep gitops-cluster\` CMD=\`echo $ARGO_CMD | sed &#39;s|- oc|-;oc|g&#39;\` eval $CMD The result should look something like:
NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD datacenter-gitops-server datacenter-gitops-server-industrial-edge-datacenter.apps.mycluster.mydomain.com datacenter-gitops-server https passthrough/Redirect None # admin.password REDACTED NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD factory-gitops-server factory-gitops-server-industrial-edge-factory.apps.mycluster.mydomain.com factory-gitops-server https passthrough/Redirect None # admin.password REDACTED NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cluster cluster-openshift-gitops.apps.mycluster.mydomain.com cluster 8080 reencrypt/Allow None kam kam-openshift-gitops.apps.mycluster.mydomain.com kam 8443 passthrough/None None openshift-gitops-server openshift-gitops-server-openshift-gitops.apps.mycluster.mydomain.com openshift-gitops-server https passthrough/Redirect None # admin.password REDACTED The most important ArgoCD instance to examine at this point is data-center-gitops-server. This is where all the applications for the datacenter, including the test environment, can be tracked.
Apply the secrets from the values-secret-industrial-edge.yaml to the secrets management Vault. This can be done through Vault&rsquo;s UI - manually without the file. The required secrets and scopes are:
secret/hub/git git username &amp; password (GitHub token) secret/hub/imageregistry Quay or DockerHub username &amp; password secret/hub/aws - AWS values read from your ~/.aws/credentials Using the Vault UI check that the secrets have been setup.
For more information on secrets management see here. For information on Hashicorp&rsquo;s Vault see here
Check all applications are synchronised
Next Steps Help &amp; Feedback{: .btn .fs-5 .mb-4 .mb-md-0 .mr-2 } Report Bugs{: .btn .btn-red .fs-5 .mb-4 .mb-md-0 .mr-2 }
Once the data center has been setup correctly and confirmed to be working, you can:
Add a dedicated cluster to deploy the factory pieces using ACM
Once the data center and the factory have been deployed you will want to check out and test the Industrial Edge 2.0 demo code. You can find that here
a. Making configuration changes with GitOps a. Making application changes using DevOps a. Making AI/ML model changes with DevOps
Uninstalling We currently do not support uninstalling this pattern.
`,url:"https://validatedpatterns.io/patterns/industrial-edge/getting-started/",breadcrumb:"/patterns/industrial-edge/getting-started/"},"https://validatedpatterns.io/patterns/medical-diagnosis/getting-started/":{title:"Getting Started",tags:[],content:` Prerequisites An OpenShift cluster (Go to the OpenShift console). Cluster must have a dynamic StorageClass to provision PersistentVolumes. See also sizing your cluster.
A GitHub account (and a token for it with repositories permissions, to read from and write to your forks)
S3-capable Storage set up in your public/private cloud for the x-ray images
The helm binary, see here
For installation tooling dependencies, see Patterns quick start.
The use of this pattern depends on having a Red Hat OpenShift cluster. In this version of the validated pattern there is no dedicated Hub / Edge cluster for the Medical Diagnosis pattern.
If you do not have a running Red Hat OpenShift cluster you can start one on a public or private cloud by using Red Hat’s cloud service.
Setting up an S3 Bucket for the xray-images An S3 bucket is required for image processing. Please see the Utilities section below for creating a bucket in AWS S3. The following links provide information on how to create the buckets required for this validated pattern on several cloud providers.
AWS S3
Azure Blob Storage
GCP Cloud Storage
Utilities A number of utilities have been built by the validated patterns team to lower the barrier to entry for using the community or Red Hat Validated Patterns. To use these utilities you will need to export some environment variables for your cloud provider:
For AWS (replace with your keys):
export AWS_ACCESS_KEY_ID=AKXXXXXXXXXXXXX export AWS_SECRET_ACCESS_KEY=gkXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Then we need to create the S3 bucket and copy over the data from the validated patterns public bucket to the created bucket for your demo. You can do this on the cloud providers console or use the scripts provided on validated-patterns-utilities repository.
python s3-create.py -b mytest-bucket -r us-west-2 -p python s3-sync-buckets.py -s com.validated-patterns.xray-source -t mytest-bucket -r us-west-2 The output should look similar to this edited/compressed output.
Keep note of the name of the bucket you created, as you will need it for further pattern configuration. There is some key information you will need to take note of that is required by the &#39;values-global.yaml&#39; file. You will need the URL for the bucket and its name. At the very end of the values-global.yaml file you will see a section for s3: were these values need to be changed.
Preparation Fork the medical-diagnosis repo on GitHub. It is necessary to fork because your fork will be updated as part of the GitOps and DevOps processes.
Clone the forked copy of this repository.
git clone git@github.com:&lt;your-username&gt;/medical-diagnosis.git Create a local copy of the Helm values file that can safely include credentials
DO NOT COMMIT THIS FILE
You do not want to push credentials to GitHub.
cp values-secret.yaml.template ~/values-secret.yaml vi ~/values-secret.yaml values-secret.yaml example
secrets: xraylab: database-user: xraylab database-password: ## Insert your custom password here ## database-root-password: ## Insert your custom password here ## database-host: xraylabdb database-db: xraylabdb database-master-user: xraylab database-master-password: ## Insert your custom password here ## grafana: GF_SECURITY_ADMIN_PASSWORD: ## Insert your custom password here ## GF_SECURITY_ADMIN_USER: root When you edit the file you can make changes to the various DB and Grafana passwords if you wish.
Customize the values-global.yaml for your deployment
git checkout -b my-branch vi values-global.yaml Replace instances of PROVIDE_ with your specific configuration
...omitted datacenter: cloudProvider: PROVIDE_CLOUD_PROVIDER #aws, azure storageClassName: PROVIDE_STORAGECLASS_NAME #gp2 (aws) region: PROVIDE_CLOUD_REGION #us-east-1 clustername: PROVIDE_CLUSTER_NAME #OpenShift clusterName domain: PROVIDE_DNS_DOMAIN #blueprints.rhecoeng.com s3: # Values for S3 bucket access # Replace &lt;region&gt; with AWS region where S3 bucket was created # Replace &lt;cluster-name&gt; and &lt;domain&gt; with your OpenShift cluster values # bucketSource: &#34;https://s3.&lt;region&gt;.amazonaws.com/&lt;s3_bucket_name&gt;&#34; bucketSource: PROVIDE_BUCKET_SOURCE #&#34;https://s3.us-east-2.amazonaws.com/com.validated-patterns.xray-source&#34; # Bucket base name used for xray images bucketBaseName: &#34;xray-source&#34; git add values-global.yaml git commit values-global.yaml git push origin my-branch You can deploy the pattern using the validated pattern operator. If you do use the operator then skip to Validating the Environment below.
Preview the changes that will be made to the Helm charts.
./pattern.sh make show Login to your cluster using oc login or exporting the KUBECONFIG
oc login or set KUBECONFIG to the path to your kubeconfig file. For example export KUBECONFIG=~/my-ocp-env/auth/kubeconfig Check the values files before deployment You can run a check before deployment to make sure that you have the required variables to deploy the Medical Diagnosis Validated Pattern.
You can run make predeploy to check your values. This will allow you to review your values and changed them in the case there are typos or old values. The values files that should be reviewed prior to deploying the Medical Diagnosis Validated Pattern are:
Values File Description values-secret.yaml
This is the values file that will include the xraylab section with all the database secrets
values-global.yaml
File that is used to contain all the global values used by Helm
Make sure you have the correct domain, clustername, externalUrl, targetBucket and bucketSource values.
Deploy Apply the changes to your cluster
./pattern.sh make install If the install fails and you go back over the instructions and see what was missed and change it, then run make update to continue the installation.
This takes some time. Especially for the OpenShift Data Foundation operator components to install and synchronize. The make install provides some progress updates during the install. It can take up to twenty minutes. Compare your make install run progress with the following video showing a successful install.
Check that the operators have been installed in the UI.
To verify, in the OpenShift Container Platform web console, navigate to Operators → Installed Operators page.
Check that the Operator is installed in the openshift-operators namespace and its status is Succeeded.
The main operator to watch is the OpenShift Data Foundation.
Using OpenShift GitOps to check on Application progress You can also check on the progress using OpenShift GitOps to check on the various applications deployed.
Obtain the ArgoCD URLs and passwords.
The URLs and login credentials for ArgoCD change depending on the pattern name and the site names they control. Follow the instructions below to find them, however you choose to deploy the pattern.
Display the fully qualified domain names, and matching login credentials, for all ArgoCD instances:
ARGO_CMD=\`oc get secrets -A -o jsonpath=&#39;{range .items[*]}{&#34;oc get -n &#34;}{.metadata.namespace}{&#34; routes; oc -n &#34;}{.metadata.namespace}{&#34; extract secrets/&#34;}{.metadata.name}{&#34; --to=-\\\\n&#34;}{end}&#39; | grep gitops-cluster\` CMD=\`echo $ARGO_CMD | sed &#39;s|- oc|-;oc|g&#39;\` eval $CMD The result should look something like:
NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD hub-gitops-server hub-gitops-server-medical-diagnosis-hub.apps.wh-medctr.blueprints.rhecoeng.com hub-gitops-server https passthrough/Redirect None # admin.password xsyYU6eSWtwniEk1X3jL0c2TGfQgVpDH NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD cluster cluster-openshift-gitops.apps.wh-medctr.blueprints.rhecoeng.com cluster 8080 reencrypt/Allow None kam kam-openshift-gitops.apps.wh-medctr.blueprints.rhecoeng.com kam 8443 passthrough/None None openshift-gitops-server openshift-gitops-server-openshift-gitops.apps.wh-medctr.blueprints.rhecoeng.com openshift-gitops-server https passthrough/Redirect None # admin.password FdGgWHsBYkeqOczE3PuRpU1jLn7C2fD6 The most important ArgoCD instance to examine at this point is medical-diagnosis-hub. This is where all the applications for the pattern can be tracked.
Check all applications are synchronised. There are thirteen different ArgoCD &#34;applications&#34; deployed as part of this pattern.
Viewing the Grafana based dashboard First we need to accept SSL certificates on the browser for the dashboard. In the OpenShift console go to the Routes for project openshift-storage. Click on the URL for the s3-rgw.
Make sure that you see some XML and not an access denied message.
While still looking at Routes, change the project to xraylab-1. Click on the URL for the image-server. Make sure you do not see an access denied message. You ought to see a Hello World message.
Turn on the image file flow. There are three ways to go about this.
You can go to the command-line (make sure you have KUBECONFIG set, or are logged into the cluster.
oc scale deploymentconfig/image-generator --replicas=1 -n xraylab-1 Or you can go to the OpenShift UI and change the view from Administrator to Developer and select Topology. From there select the xraylab-1 project.
Right click on the image-generator pod icon and select Edit Pod count.
Up the pod count from 0 to 1 and save.
Alternatively, you can have the same outcome on the Administrator console.
Go to the OpenShift UI under Workloads, select Deploymentconfigs for Project xraylab-1. Click on image-generator and increase the pod count to 1.
Making some changes on the dashboard You can change some of the parameters and watch how the changes effect the dashboard.
You can increase or decrease the number of image generators.
oc scale deploymentconfig/image-generator --replicas=2 Check the dashboard.
oc scale deploymentconfig/image-generator --replicas=0 Watch the dashboard stop processing images.
You can also simulate the change of the AI model version - as it’s only an environment variable in the Serverless Service configuration.
oc patch service.serving.knative.dev/risk-assessment --type=json -p &#39;[{&#34;op&#34;:&#34;replace&#34;,&#34;path&#34;:&#34;/spec/template/metadata/annotations/revisionTimestamp&#34;,&#34;value&#34;:&#34;&#39;&#34;$(date +%F_%T)&#34;&#39;&#34;},{&#34;op&#34;:&#34;replace&#34;,&#34;path&#34;:&#34;/spec/template/spec/containers/0/env/0/value&#34;,&#34;value&#34;:&#34;v2&#34;}]&#39; This changes the model version value, as well as the revisionTimestamp in the annotations, which triggers a redeployment of the service.
Next Steps Help &amp; Feedback Report Bugs
`,url:"https://validatedpatterns.io/patterns/medical-diagnosis/getting-started/",breadcrumb:"/patterns/medical-diagnosis/getting-started/"},"https://validatedpatterns.io/patterns/multicloud-gitops-portworx/getting-started/":{title:"Getting Started",tags:[],content:`Deploying the Multicloud GitOps pattern Prerequisite An OpenShift cluster To create an OpenShift cluster, go to the Red Hat Hybrid Cloud console. Select OpenShift -&gt; Clusters -&gt; Create cluster. The cluster must have a dynamic StorageClass to provision PersistentVolumes. See sizing your cluster. Optional: A second OpenShift cluster for multicloud demonstration. The git binary and podman. For details see Installing Git and Installing Podman The use of this pattern depends on having at least one running Red Hat OpenShift cluster. It is desirable to have a cluster for deploying the GitOps management hub assets and a separate cluster(s) for the managed cluster(s).
If you do not have a running Red Hat OpenShift cluster, you can start one on a public or private cloud by using Red Hat&rsquo;s cloud service.
Procedure For installation tooling dependencies, see link:https://hybrid-cloud-patterns.io/learn/quickstart/[Patterns quick start].
{% include prerequisite-tools.md %}
Fork the rh-multicloud-gitops-pxe repository on GitHub. It is recommended to fork because you can update your fork as part of the GitOps and DevOps processes.
Clone the forked copy of this repository.
git clone git@github.com:your-username/multicloud-gitops-pxe.git Create a local copy of the secret values file that can safely include credentials.
Warning: Do not commit this file. You do not want to push personal credentials to GitHub. Note that if you do not want to customize the secrets, these steps are not needed. The framework generates a random password for the config-demo application.
cp values-secret.yaml.template ~/values-secret-multicloud-gitops-pxe.yaml vi ~/values-secret-multicloud-gitops-pxe.yaml Customize the deployment for your cluster.
git checkout -b my-branch vi values-global.yaml git add values-global.yaml git commit values-global.yaml git push origin my-branch You can deploy the pattern by running ./pattern.sh make install or by using the validated pattern operator.
Deploying the cluster by using the pattern.sh file To deploy the cluster by using the pattern.sh file, complete the following steps:
Login to your cluster using oc login or exporting the KUBECONFIG.
oc login or set KUBECONFIG to the path to your kubeconfig file. For example:
export KUBECONFIG=~/my-ocp-env/hub/auth/kubeconfig Deploy the pattern to your cluster.
./pattern.sh make install Verify that the Operators have been installed.
To verify, in the *OpenShift Container Platform web console, navigate to Operators → Installed Operators page. 2.Check that the Operator is installed in the openshift-operators namespace and its status is Succeeded. Verify that all applications are synchronized. Under the project multicloud-gitops-hub click the URL for the hub gitops server. The Vault application is not synched. Multicloud GitOps application demos As part of this pattern, HashiCorp Vault has been installed. Refer to the section on Vault.
Next steps Deploying the managed cluster applications After the management hub is set up and works correctly, attach one or more managed clusters to the architecture (see diagrams below).
For instructions on deploying the edge, refer to Managed Cluster Sites.
Contribute to this pattern: Help &amp; Feedback Report Bugs
`,url:"https://validatedpatterns.io/patterns/multicloud-gitops-portworx/getting-started/",breadcrumb:"/patterns/multicloud-gitops-portworx/getting-started/"},"https://validatedpatterns.io/patterns/retail/getting-started/":{title:"Getting Started",tags:[],content:`Deploying the Retail Pattern Prerequisites An OpenShift cluster (Go to the OpenShift console). Cluster must have a dynamic StorageClass to provision PersistentVolumes. See also sizing your cluster.
(Optional) A second OpenShift cluster for a second store environment, &ldquo;raleigh&rdquo;.
A GitHub account
(Optional) A quay account that can update images; this is if you want to use the pipelines to customize the applications
(Optional) A quay account with the following repositories set as public, and which you can write to:
quarkuscoffeeshop-barista quarkuscoffeeshop-counter quarkuscoffeeshop-customerloyalty quarkuscoffeeshop-customermocker quarkuscoffeeshop-inventory quarkuscoffeeshop-kitchen quarkuscoffeeshop-majestic-monolith quarkuscoffeeshop-web These repos comprise the microservices that are in the demo. The public repos (quay.io/hybrid-cloud-patterns/*) contain pre-built images which will be downloaded and used by default; so the demo will run regardless of whether you choose to rebuild the apps or not. This mechanism is provided for transparency purposes (so you can reproduce the same results); or if you want to customize or change the apps themselves in some way.
The use of this pattern depends on having at least one running Red Hat OpenShift cluster. All of the apps will run on a single cluster; optionally you can use RHACM to apply the store apps to a second cluster.
If you do not have a running Red Hat OpenShift cluster you can start one on a public or private cloud by using Red Hat&rsquo;s cloud service.
Prerequisite Tools Install the installation tooling dependencies. You will need:
{% include prerequisite-tools.md %}
How to deploy Fork the retail repository on GitHub.
Clone the forked copy of the retail repo. Use branch \`v1.0'.
git clone git@github.com:{your-username}/retail.git cd retail git checkout v1.0 You could create your own branch where you specific values will be pushed to:
git checkout -b my-branch A values-secret.yaml file is used to automate setup of secrets needed for:
A container image registry (E.g. Quay) DO NOT COMMIT THIS FILE. You do not want to push personal credentials to GitHub.
cp values-secret.yaml.template ~/values-secret.yaml vi ~/values-secret.yaml Customize the deployment for your cluster. Change the appropriate values in values-global.yaml
vi values-global.yaml git add values-global.yaml git commit values-global.yaml git push origin my-branch In particular, the values that you need to change are under the imageregistry key, to use your own account and hostname. If you like, you can change the git settings (account, email, hostname to reflect your own account settings).
If you plan to customize the build of the applications themselves, there revision and imageTag settings for each of them. The defaults should suffice if you just want to see the apps running.
You can deploy the pattern using the validated pattern operator. If you do use the operator then skip to Validating the Environment below.
Preview the changes
./pattern.sh make show Login to your cluster using oc login or exporting the KUBECONFIG
oc login or
export KUBECONFIG=~/my-ocp-env/retail-hub Apply the changes to your cluster
./pattern-util.sh make install This will execute make install in the team&rsquo;s container, which will take a bit to load the first time. It contains ansible and other dependencies so that you do not need to install them on your workstation.
The default install target will:
Install the pattern via the operator Load the imageregistry secret into the vault Start the application build pipelines If you chose not to put in your registry credential, make install cannot complete successfully because it waits for the secret to be populated before starting the pipelines.
If you do not want to run the (optional) components, another install target is provided:
./common/scripts/pattern-util.sh make install-no-pipelines This skips the vault setup and the pipeline builds, but still installs both Vault and the Pipelines operator, so if you want to run those in your installation later, you can run make install to enable them.
For more information on secrets management see here. For information on Hashicorp&rsquo;s Vault see here
Validating the Environment Check the operators have been installed
UI -&gt; Installed Operators The OpenShift console menu should look like this. We will use it to validate that the pattern is working as expected:
Check on the pipelines, if you chose to run them. They should all complete successfully:
Ensure that the Hub ArgoCD instance shows all of its apps in Healthy and Synced status once all of the images have been built:
We will go to the Landing Page, which will present the applications in the pattern:
Clicking on the Store Web Page will place us in the Quarkus Coffeeshop Demo:
Clicking on the TEST Store Web Page will place us in a separate copy of the same demo.
Clicking on the respective Kafdrop links will go to a Kafdrop instance that allows inspection of each of the respective environments.
Next Steps Help &amp; Feedback{: .btn .fs-5 .mb-4 .mb-md-0 .mr-2 } Report Bugs{: .btn .btn-red .fs-5 .mb-4 .mb-md-0 .mr-2 }
`,url:"https://validatedpatterns.io/patterns/retail/getting-started/",breadcrumb:"/patterns/retail/getting-started/"},"https://validatedpatterns.io/contribute/background-on-pattern-development/":{title:"Background on pattern development",tags:[],content:`Background on pattern development Introduction This section provides details on how to create a new pattern using the validated patterns framework. Creating a new pattern might start from scratch or it may start from an existing deployment that would benefit from a repeatable framework based on GitOps.
This introduction explains some of framework design decisions and why they were chosen. There are some high level concepts that are required for the framework. While those concepts can be implemented using a variety of open source projects, this framework is prescriptive and mentions the project and also (down stream) product that was used. E.g. For development builds we use Tekton (project) and specifically use OpenShift Pipelines (product).
The framework uses popular Cloud Native Computing Foundation (CNCF) projects as much as possible. The CNCF landscape contains many projects that solve the same or similar problem. The validated patterns effort has chosen specific projects but it is not unreasonable for users to switch out one project for another. (See more on Operators below).
There is no desire to replicate efforts already in CNCF. If new a open source project comes out of this framework, the plan would be to contribute that to CNCF.
Who is a pattern developer? Many enterprise class Cloud Native applications are complex and require many different application services integrated together. Organizations can learn from each other on how to create robust, scalable, and maintainable systems. When you find a pattern that seems to work, it makes sense to promote best practices to others in order for them to not repeat the many failures you probably made while getting to your killer pattern.
In the world of DevOps (including DevSecOps and GitOps), teams should include personnel from development, operations, security, and architects. What makes DevOps work is the collaboration of all these IT personnel, the business owners, and others. As DevOps practices move through your organization, best practices are shared and standards evolve.
This validated patterns framework has evolved since it was started in 2019. It will likely continue to evolve. What was learned is that there are some common concepts that need to be addressed once you desire to generalize your organizations framework.
Therefore, the goal is, that developers, operators, security, and architects will use this framework to have secure and repeatable day one deployment mechanism and maintenance automation for day two operations.
A common platform One of the most important goals of this framework is to provide consistency across any cloud provider - public or private. Public cloud providers each have Kubernetes distributions. While they keep up with the Kubernetes release cycle, they are not always running on the same version. Furthermore, each cloud provider has their own sets of services that developers often consume. So while you could automate the handling for each of the cloud providers, the framework utilizes one Kubernetes distribution that runs on public or private clouds - the hybrid and/or multi cloud model.
The framework depends on Red Hat OpenShift Container Platform (OCP). Once you have deployed Red Hat OCP wherever you wish to deploy your cloud native application pattern, then the framework can deploy on that platform in a few easy steps.
Containment beyond containers If you are reading this chances are you are already familiar with Linux containers. But there is more to containers than Linux containers in the Cloud Native environment.
Containers Containers allow you to encapsulate your program/process and all its dependencies in one package called a container image. The container runtime starts an instance of this container using only the Linux kernel and the directory structure, with program and dependencies, provided in the container image. This ensures that the program is running isolated from any other packages, programs, or files loaded on the host system.
Kubernetes, and the Cloud Native community of services, use Linux containers as their basic building block.
Operators While Linux containers provide an incredibly useful way to isolate the dependencies for an application or application service, containers also require some lifecycle management. For example, at start up a container my need to set up access to networks, or extra storage. This type of set up usually happens with a human operator deciding on how the container will connect networks or host storage. The operator may also have to do routine maintenance. For example, if the container contains a database, the human operator may need to do a backup or routine scrubbing of the database.
Kubernetes Operators are an extension to Kubernetes &ldquo;that make sue of custom resources to manage applications and their components.&rdquo; I.e. it provides an extra layer of encapsulation on top of containers that packages up some operation automation with the container. It puts what the human operator would do into an Operator pattern for the service or set of services.
Many software providers/vendors have created operators to manage their application or service lifecycle. Red Hat OpenShift provides a catalog of certified Operators that application develops can consume as part of their overall application. The validated patterns makes use of these certified Operators as much as possible. Having a common platform like Red Hat OpenShift helps reduce risk by using certified Operators.
Validated patterns Assembling operators into a common pattern provides another layer of encapsulation. As with an Operator, where the developer can take advantage of the best practices from a experienced human operator, a validated pattern provides a way of taking advantage of best practices for deploying operators and other assets for a particular type of solution. Rather than starting from scratch to figure out how to deploy and manage a complex set of integrated and dependent containerized services, a developer can take a validated pattern and know that a lot of experience has been put into it.
A validated pattern has been tested and continues to be tested as the lifecycle of individual parts (Operators) change through release cycles. Red Hat&rsquo;s Quality Engineering team provides Continuous Integration of the pattern for new releases of Red Hat products (Operators).
The validated patterns framework takes advantage of automation technology. It uses Cloud Native automation technology as much as possible. Occasionally the framework resorts to some scripts in order to get a pattern up and running faster.
Automation has many layers As mentioned above, gaining consistency and robustness for deploying complex Cloud Native applications requires automation. While many Kubernetes distributions, including OpenShift, provide excellent user interfaces for deploying and managing applications, this is mostly useful during development and/or debugging when things go wrong. Being able to consistently deploy complex applications is critical.
But which automation tool should be used? Or which automation tools, plural? During the development of the validated patterns framework we learned important lessons on the different areas of automation.
Automation for building application code When developing container based Cloud Native applications, a developer needs to build executable code and create a new container image for deployment into their Kubernetes test environment. Once tested, that container image needs to be moved through the continuous integration and continuous deployment (CI/CD) pipeline until it ends up in production. Tekton is a Cloud Native CI/CD project that is build for hybrid-cloud. OpenShift Pipelines is a Red Hat product based on Tekton.
Automation for application operations There are two aspects to consider for operations when doing automation. First, you must be able to package up much of the configuration that is required for deploying Operators and pods. The validated patterns framework started with a project called Kustomize which allows you to assemble complex deployment YAML to apply to your Kubernetes cluster. Kustomize is a powerful tool, and almost achieved what we needed. However it fell short when we needed to propagate variable data into our deployment YAML. Instead we chose Helm because it provides templating and can therefore handle the injection of variable data into the deployment package. See more on templating here.
The second aspect of automation for application automation deals with both workflow and GitOps. Validated patterns requires that a workflow deploys various components of the complex application. Visibility into the success or failure of those application components is really important. After the initial deployment it is important to role out configuration changes in an automated way using a code repository. This is achieved using GitOps. I.e. Using a Git repository as a mechanism to change configuration that triggers the automatic roll-out of those changes.
&ldquo;Application definitions, configurations, and environments should be declarative and version controlled. Application deployment and lifecycle management should be automated, auditable, and easy to understand.&rdquo; - Argo CD project
OpenShift GitOps is based on the Argo CD project. It is a GitOps continuous delivery tool for Kubernetes.
Secret handling Validated patterns often depend on resources that require certificates or keys. These secrets need to be handled carefully. While it&rsquo;s tempting to focus on just the deployment of a pattern and &ldquo;handle security later&rdquo;, that&rsquo;s a bad idea. In the spirit of DevSecOps, the validated patterns effort has decided to &ldquo;shift security left&rdquo;. I.e. build security in early in the lifecycle.
When it comes to security, the approach requires patience and care to set up. There is no avoiding some manual steps but validated patterns tries to automate as much as possible while at the same time taking the lid off so developers can see what was and needs to be done.
There are two approaches to secret handling with validated patterns:
Using special configuration files. This is fine for initial development but not for production. Using a Cloud Native secrets handling tool e.g. Vault or Conjur Some of the validated patterns use configuration files (for now), while others, like the Multicloud GitOps, use Vault. See Vault Setup for more info.
Policy While many enterprise Cloud Native applications are open source, many of the products used require licenses or subscriptions. Policies help enforce license and subscription management and the channels needed to get access to those licenses or subscriptions.
Similarly, in multicloud deployments and complex edge deployments, policies can help define and select the correct GitOps workflows that need to be managed for various sites or clusters. E.g. defining an OpenShift Cluster as a &ldquo;Factory&rdquo; in the Industrial Edge validated pattern provides a simple trigger to roll-out the entire Factory deployment. Policy is a powerful tool in automation.
Validated patterns use Red Hat Advanced Cluster Management for Kubernetes to control clusters and applications from a single console, with built-in security policies.
`,url:"https://validatedpatterns.io/contribute/background-on-pattern-development/",breadcrumb:"/contribute/background-on-pattern-development/"},"https://validatedpatterns.io/patterns/medical-diagnosis/cluster-sizing/":{title:"Cluster Sizing",tags:[],content:` Tested Platforms The Medical Diagnosis pattern has been tested in the following Certified Cloud Providers.
Certified Cloud Providers 4.8 4.9 4.10 4.11 Amazon Web Services
Tested
Tested
Tested
Tested
Google Compute
Microsoft Azure
General OpenShift Minimum Requirements OpenShift 4 has the following minimum requirements for sizing of nodes:
Minimum 4 vCPU (additional are strongly recommended).
Minimum 16 GB RAM (additional memory is strongly recommended, especially if etcd is colocated on Control Planes).
Minimum 40 GB hard disk space for the file system containing /var/.
Minimum 1 GB hard disk space for the file system containing /usr/local/bin/.
Medical Diagnosis Pattern Components Here’s an inventory of what gets deployed by the Medical Diagnosis pattern:
Name Kind Namespace Description Medical Diagnosis Hub
Application
medical-diagnosis-hub
Hub GitOps management
Red Hat OpenShift GitOps
Operator
openshift-operators
OpenShift GitOps
Red Hat OpenShift Data Foundations
Operator
openshift-storage
Cloud Native storage solution
Red Hat AMQStreams (Apache Kafka)
Operator
openshift-operators
AMQ Streams provides Apache Kafka access
Red Hat OpenShift Serverless
Operator
- knative-eventing - knative-serving
Provides access to knative eventing and serving functions
Medical Diagnosis Pattern OpenShift Cluster Size The Medical Diagnosis pattern has been tested with a defined set of configurations that represent the most common combinations that Red Hat OpenShift Container Platform (OCP) customers are using or deploying for the x86_64 architecture.
The OpenShift cluster for the Medical Diagnosis pattern needs to be sized a bit larger to support the compute and storage demands of OpenShift Data Foundations and other operators that make up the pattern. The above cluster sizing is close to a minimum size for an OpenShift cluster supporting this pattern. In the next few sections we take some snapshots of the cluster utilization while the Medical Diagnosis pattern is running. Keep in mind that resources will have to be added as more developers are working building their applications.
The OpenShift cluster is a standard deployment of 3 control plane nodes and 3 or more worker nodes.
Node Type Number of nodes Cloud Provider Instance Type Control Plane/Worker
3
Google Cloud
n1-standard-8
Control Plane/Worker
3
Amazon Cloud Services
m5.2xlarge
Control Plane/Worker
3
Microsoft Azure
Standard_D8s_v3
AWS Instance Types The Medical Diagnosis pattern was tested with the highlighted AWS instances in bold. The OpenShift installer will let you know if the instance type meets the minimum requirements for a cluster.
The message that the openshift installer will give you will be similar to this message
INFO Credentials loaded from default AWS environment variables FATAL failed to fetch Metadata: failed to load asset &#34;Install Config&#34;: [controlPlane.platform.aws.type: Invalid value: &#34;m4.large&#34;: instance type does not meet minimum resource requirements of 4 vCPUs, controlPlane.platform.aws.type: Invalid value: &#34;m4.large&#34;: instance type does not meet minimum resource requirements of 16384 MiB Memory] Below you can find a list of the AWS instance types that can be used to deploy the Medical Diagnosis pattern.
Instance type Default vCPUs Memory (GiB) Hub Factory/Edge 3x3 OCP Cluster
3 Node OCP Cluster
m4.xlarge
4
16
N
N
m4.2xlarge
8
32
Y
Y
m4.4xlarge
16
64
Y
Y
m4.10xlarge
40
160
Y
Y
m4.16xlarge
64
256
Y
Y
m5.xlarge
4
16
Y
N
m5.2xlarge
8
32
Y
Y
m5.4xlarge
16
64
Y
Y
m5.8xlarge
32
128
Y
Y
m5.12xlarge
48
192
Y
Y
m5.16xlarge
64
256
Y
Y
m5.24xlarge
96
384
Y
Y
The OpenShift cluster is made of 3 Control Plane nodes and 3 Workers. For the node sizes we used the m5.4xlarge on AWS and this instance type met the minimum requirements to deploy the Medical Diagnosis pattern successfully.
To understand better what types of nodes you can use on other Cloud Providers we provide some of the details below.
Azure Instance Types The Medical Diagnosis pattern was also deployed on Azure using the Standard_D8s_v3 VM size. Below is a table of different VM sizes available for Azure. Keep in mind that due to limited access to Azure we only used the Standard_D8s_v3 VM size.
The OpenShift cluster is made of 3 Control Plane nodes and 3 Workers.
Type Sizes Description General purpose
B, Dsv3, Dv3, Dasv4, Dav4, DSv2, Dv2, Av2, DC, DCv2, Dv4, Dsv4, Ddv4, Ddsv4
Balanced CPU-to-memory ratio. Ideal for testing and development, small to medium databases, and low to medium traffic web servers.
Compute optimized
F, Fs, Fsv2, FX
High CPU-to-memory ratio. Good for medium traffic web servers, network appliances, batch processes, and application servers.
Memory optimized
Esv3, Ev3, Easv4, Eav4, Ev4, Esv4, Edv4, Edsv4, Mv2, M, DSv2, Dv2
High memory-to-CPU ratio. Great for relational database servers, medium to large caches, and in-memory analytics.
Storage optimized
Lsv2
High disk throughput and IO ideal for Big Data, SQL, NoSQL databases, data warehousing and large transactional databases.
GPU
NC, NCv2, NCv3, NCasT4_v3, ND, NDv2, NV, NVv3, NVv4
Specialized virtual machines targeted for heavy graphic rendering and video editing, as well as model training and inferencing (ND) with deep learning. Available with single or multiple GPUs.
High performance compute
HB, HBv2, HBv3, HC, H
Our fastest and most powerful CPU virtual machines with optional high-throughput network interfaces (RDMA).
For more information please refer to the Azure VM Size Page.
Google Cloud (GCP) Instance Types The Medical Diagnosis pattern was also deployed on GCP using the n1-standard-8 VM size. Below is a table of different VM sizes available for GCP. Keep in mind that due to limited access to GCP we only used the n1-standard-8 VM size.
The OpenShift cluster is made of 3 Control Plane and 3 Workers cluster.
The following table provides VM recommendations for different workloads.
General purpose Workload optimized Cost-optimized
Balanced
Scale-out optimized
Memory-optimized
Compute-optimized
Accelerator-optimized
E2
N2, N2D, N1
T2D
M2, M1
C2
A2
Day-to-day computing at a lower cost
Balanced price/performance across a wide range of VM shapes
Best performance/cost for scale-out workloads
Ultra high-memory workloads
Ultra high performance for compute-intensive workloads
Optimized for high performance computing workloads
For more information please refer to the GCP VM Size Page.
`,url:"https://validatedpatterns.io/patterns/medical-diagnosis/cluster-sizing/",breadcrumb:"/patterns/medical-diagnosis/cluster-sizing/"},"https://validatedpatterns.io/patterns/industrial-edge/factory/":{title:"Factory Sites",tags:[],content:`Having a factory (edge) cluster join the datacenter (hub) Allow ACM to deploy the factory application to a subset of clusters By default the factory applications are deployed on all clusters that ACM knows about.
managedSites: - name: factory clusterSelector: matchExpressions: - key: vendor operator: In values: - OpenShift This is useful for cost-effective demos, but is hardly realistic.
To deploy the factory applications only on managed clusters with the label site=factory, change the site definition in values-datacenter.yaml to:
managedSites: - name: factory clusterSelector: matchLabels: site: factory Remember to commit the changes and push to GitHub so that GitOps can see your changes and apply them.
Deploy a factory cluster For instructions on how to prepare and import a factory cluster please read the section importing a cluster. Use clusterGroup=factory.
You&rsquo;re done That&rsquo;s it! Go to your factory (edge) OpenShift console and check for the open-cluster-management-agent pod being launched. Be patient, it will take a while for the ACM agent and agent-addons to launch. After that, the operator OpenShift GitOps will run. When it&rsquo;s finished coming up launch the OpenShift GitOps (ArgoCD) console from the top right of the OpenShift console.
Next up Work your way through the Industrial Edge 2.0 GitOps/DevOps demos
`,url:"https://validatedpatterns.io/patterns/industrial-edge/factory/",breadcrumb:"/patterns/industrial-edge/factory/"},"https://validatedpatterns.io/patterns/ansible-edge-gitops/installation-details/":{title:"Installation Details",tags:[],content:`Installation Details Installation Steps These are the steps run by make install and what each one does:
operator-deploy The operator-deploy task installs the Validated Patterns Operator, which in turn creates a subscription for the OpenShift GitOps operator and installs both the cluster and hub instances of it. The clustergroup application will then read the values-global.yaml and values-hub.yaml files for other subscriptions and applications to install.
The legacy-install is still provided for users that cannot or do not want to use the Validated Patterns operator. Instead of installing the operator, it installs a helm chart that does the same thing - installs a subscription for OpenShift GitOps and installs a cluster-wide and hub instance of that operator. It then proceeds with installing the clustergroup application.
Note that both the upgrade and legacy-upgrade targets are now equivalent and interchangeable with install and legacy-install (respectively - legacy-install/legacy-upgrade are not compatible with standard install/upgrade. This was not always the case, so both install/upgrade targets are still provided).
Imperative section Part of the operator-deploy process is creating and running the imperative tools as defined in the hub values file. In this pattern, that includes running the playbook to deploy the metal worker.
The real code for this playbook (outside of a shell wrapper) is here.
This script is another Ansible playbook that deploys a node to run the Virtual Machines for the demo. The playbook uses the OpenShift machineset API to provision the node in the first availability zone it finds. Currently, AWS is the only major public cloud provider that offers the deployment of a metal node through the normal provisioning process. We hope that Azure and GCP will support this functionality soon as well.
Please be aware that the metal node is rather more expensive in compute costs than most other AWS machine types. The trade-off is that running the demo without hardware acceleration would take ~4x as long.
It takes about 20-30 minutes for the metal node to become available to run VMs. If you would like to see the current status of the metal node, you can check it this way (assuming your kubeconfig is currently set up to point to your cluster):
oc get -A machineset You will be looking for a machineset with metal-worker in its name:
NAMESPACE NAME DESIRED CURRENT READY AVAILABLE AGE openshift-machine-api mhjacks-aeg-qx25w-metal-worker-us-west-2a 1 1 1 1 19m openshift-machine-api mhjacks-aeg-qx25w-worker-us-west-2a 1 1 1 1 47m openshift-machine-api mhjacks-aeg-qx25w-worker-us-west-2b 1 1 1 1 47m openshift-machine-api mhjacks-aeg-qx25w-worker-us-west-2c 1 1 1 1 47m openshift-machine-api mhjacks-aeg-qx25w-worker-us-west-2d 0 0 47m When the metal-worker is showing &ldquo;READY&rdquo; and &ldquo;AVAILABLE&rdquo;, the virtual machines will begin provisioning on it.
The metal node will be destroyed when the cluster is destroyed. The script is idempotent and will create at most one metal node per cluster.
post-install Note that all the steps of post-install are idempotent. If you want or need to reconfigure vault or AAP, the recommended way to do so is to call make post-install. This may change as we move elements of this pattern into the new imperative framework in common.
Specific processes that are called by post-install include:
vault-init Vault requires extra setup in the form of unseal keys and configuration of secrets. The vault-init task does this. Note that it is safe to run vault-init as it will exit successfully if it can connect to a cluster with a running, unsealed vault.
load-secrets This process (which calls push_secrets) calls an Ansible playbook that reads the values-secret.yaml file and stores the data it finds there in vault as keypairs. These values are then usable in the kubernetes cluster. This pattern uses the ssh pubkey for the kiosk VMs via the external secrets operator.
This script will update secrets in vault if re-run; it is safe to re-run if the secret values have not changed as well.
configure-controller There are two parts to this script - the first part, with the code here, retrieves the admin credentials from OpenShift to enable login to the AAP Controller.
The second part, which is the bulk of the ansible-load-controller process is here and uses the controller configuration framework to configure the Ansible Automation Platform instance that is installed by the helm chart.
This division is so that users can adapt this pattern more easily if they&rsquo;re running AAP, but not on OpenShift.
The script waits until AAP is ready, and then proceeds to:
Install the manifest to entitle AAP Configure the custom Credential Types the demo needs Define an Organization for the Demo Add a Project for the Demo Add the Credentials for jobs to use Configure Host inventory and inventory sources, and smart inventories to define target hosts Configure an Execution environment for the Demo Configure Job Templates for the Demo Configure Schedules for the jobs that need to repeat Note: This script has defaults that it overrides when run as part of make install that it derives from the environment (the repo that it is attached to and the branch that it is on). So if you need to re-run it, the most straightforward way to do this is to run make upgrade when using the make-based installation process.
OpenShift GitOps (ArgoCD) OpenShift GitOps is central to this pattern as it is responsible for installing all of the other components. The installation process is driven through the installation of the clustergroup chart. This in turn reads the repo&rsquo;s global values file, which instructs it to read the hub values file. This is how the pattern knows to apply the Subscriptions and Applications listed further in the pattern.
ODF (OpenShift Data Foundations) ODF is the storage framework that is needed to provide resilient storage for OpenShift Virtualization. It is managed via the helm chart here. This is basically the same chart that our Medical Diagnosis pattern uses (see here for details on the Medical Edge pattern&rsquo;s use of storage).
Please note that this chart will create a Noobaa S3 bucket named nb.epoch_timestamp.cluster-domain which will not be destroyed when the cluster is destroyed.
OpenShift Virtualization (KubeVirt) OpenShift Virtualization is a framework for running virtual machines as native Kubernetes resources. While it can run without hardware acceleration, the performance of virtual machines will suffer terribly; some testing on a similar workload indicated a 4-6x delay running without hardware acceleration, so at present this pattern requires hardware acceleration. The pattern provides a script deploy-kubevirt-worker.sh which will provision a metal worker to run virtual machines for the pattern.
OpenShift Virtualization currently supports only AWS and on-prem clusters; this is because of the way that baremetal resources are provisioned in GCP and Azure. We hope that OpenShift Virtualization can support GCP and Azure soon.
The installation of the OpenShift Virtualization HyperConverged deployment is controlled by the chart here.
OpenShift Virtualization was chosen in this pattern to avoid dealing with the differences in galleries and templates of images between the different public cloud providers. The important thing from this pattern&rsquo;s standpoint is the availability of machine instances to manage (since we are simulating an Edge deployment scenario, which could either be bare metal instances or virtual machines); OpenShift Virtualization was the easiest and most portable way to spin up machine instances. It also provides mechanisms for defining the desired machine set declaratively.
The creation of virtual machines is controlled by the chart here.
More details about the way we use OpenShift Virtualization are available here.
Ansible Automation Platform (AAP, formerly known as Ansible Tower) The use of Ansible Automation Platform is really the centerpiece of this pattern. We have recognized for some time that the notion and design principles of GitOps should apply to things outside of Kubernetes, and we believe this pattern gives us a way to do that.
All of the Ansible interactions are defined in a Git Repository; the Ansible jobs that configure the VMs are designed to be idempotent (and are scheduled to run every 10 minutes on those VMs).
The installation of AAP itself is governed by the chart here. The post-installation configuration of AAP is done via the ansible-load-controller.sh script.
It is very much the intention of this pattern to make it easy to replace the specific Edge management use case with another one. Some ideas on how to do that can be found here.
Specifics of the Ansible content for this pattern can be seen here.
More details of the specifics of how AAP is configured are available here.
Next Steps Help &amp; Feedback Report Bugs `,url:"https://validatedpatterns.io/patterns/ansible-edge-gitops/installation-details/",breadcrumb:"/patterns/ansible-edge-gitops/installation-details/"},"https://validatedpatterns.io/patterns/multicloud-gitops-portworx/managed-cluster/":{title:"Managed cluster sites",tags:[],content:`Attach a managed cluster (edge) to the management hub Understanding Red Hat Advanced Cluster Management requirements Allow Red Hat Advanced Cluster Management (RHACM) to deploy the managed cluster application to a subset of clusters.
By default the clusterGroup applications are deployed on all clusters that RHACM manages. In the value-hub.yaml, file add a managedClusterCgroup for each cluster or group of clusters that you want to manage as one.
managedClusterGroups: - name: region-one helmOverrides: - name: clusterGroup.isHubCluster value: false clusterSelector: matchLabels: clusterGroup: region-one The above YAML file segment deploys the clusterGroup applications on managed clusters with the label clusterGroup=region-one. Specific subscriptions and Operators, applications and projects for that clusterGroup are then managed in a value-region-one.yaml file. For example:
namespaces: - config-demo projects: - config-demo applications: config-demo: name: config-demo namespace: config-demo project: config-demo path: charts/all/config-demo #Subscriptions can be added too - multicloud-gitops at present does not require subscriptions on its managed clusters #subscriptions: # example-subscription: # name: example-operator # namespace: example-namespace # channel: example-channel # csv: example-operator.v1.0.0 subscriptions: Important: Ensure that you commit the changes and push them to GitHub so that GitOps can fetch your changes and apply them.
Deploying a managed cluster Prerequisites An OpenShift cluster To create an OpenShift cluster, go to the Red Hat Hybrid Cloud console. Select OpenShift -&gt; Clusters -&gt; Create cluster. To join the managed cluster to the management hub, you can:
Use the Red Hat Advanced Cluster Management (RHACM) web console Use the cm tool Use the clusteradm tool Using Red Hat Advanced Cluster Management web console to set up managed cluster After RHACM is installed, a message regarding a &ldquo;Web console update is available&rdquo; might be displayed. Click the &ldquo;Refresh web console&rdquo; link.
In the left navigation panel of web console, click local-cluster. Select All Clusters. The RHACM web console is displayed with Cluster* on the left navigation panel.
On the Managed clusters tab, click Import cluster.
On the Import an existing cluster page, enter the cluster name and choose Kubeconfig as the &ldquo;import mode&rdquo;. Add the tag clusterGroup=region-one. Click Import. You can now skip to the section Managed cluster is joined but ignore the part about adding the site tag.
Using the cm tool to set up a managed cluster Install the cm (cluster management) command-line tool. See details here
Obtain the KUBECONFIG file from the managed-cluster cluster.
On the command-line login into the management hub cluster (use oc login or export the KUBECONFIG).
Run the following command:
cm attach cluster --cluster &lt;cluster-name&gt; --cluster-kubeconfig &lt;path-to-KUBECONFIG&gt; Skip to the section Managed cluster is joined
Using the clusteradm tool to set up a managed cluster You can also use clusteradm to join a cluster. The following instructions explain what needs to be done. clusteradm is still in testing.
To deploy a edge cluster you will need to get the management hub cluster&rsquo;s token. You will need to install clusteradm. On the existing management hub cluster:
clusteradm get token
When you run the clusteradm command above it replies with the token and also shows you the command to use on the managed cluster. So first you must login to the managed cluster
oc login or
export KUBECONFIG=~/my-ocp-env/managed-cluster
Then request to that the managed cluster join the management hub
clusteradm join --hub-token &lt;token from clusteradm get token command &gt; &lt;managed cluster name&gt;
Back on the hub cluster accept the join request
clusteradm accept --clusters &lt;managed-cluster-name&gt;
Skip to the section Managed cluster is joined
Managed cluster is joined Designate the new cluster as a managed cluster site Now that ACM is no longer deploying the managed cluster applications everywhere, we need to explicitly indicate that the new cluster has the managed cluster role. If you haven&rsquo;t tagged the cluster as clusterGroup=region-one then we can that here.
We do this by adding the label referenced in the managedSite&rsquo;s clusterSelector.
Find the new cluster
oc get managedcluster.cluster.open-cluster-management.io
Apply the label
oc label managedcluster.cluster.open-cluster-management.io/YOURCLUSTER site=managed-cluster
Verification Go to your managed cluster (edge) OpenShift console and check for the open-cluster-management-agent pod being launched. Be patient, it will take a while for the RHACM agent and agent-addons to launch. After that, the OpenShift GitOps Operator is installed. On successful installation, launch the OpenShift GitOps (ArgoCD) console from the top right of the OpenShift console.
`,url:"https://validatedpatterns.io/patterns/multicloud-gitops-portworx/managed-cluster/",breadcrumb:"/patterns/multicloud-gitops-portworx/managed-cluster/"},"https://validatedpatterns.io/patterns/multicloud-gitops/mcg-managed-cluster/":{title:"Managed cluster sites",tags:[],content:` Understanding Red Hat Advanced Cluster Management requirements By default, Red Hat Advanced Cluster Management (RHACM) manages the clusterGroup applications that are deployed on all clusters. In the value-hub.yaml file, add a managedClusterCgroup for each cluster or group of clusters that you want to manage as one.
managedClusterGroups: - name: region-one helmOverrides: - name: clusterGroup.isHubCluster value: false clusterSelector: matchLabels: clusterGroup: region-one The above YAML file segment deploys the clusterGroup applications on managed clusters with the label clusterGroup=region-one. Specific subscriptions and Operators, applications and projects for that clusterGroup are then managed in a value-region-one.yaml file. For example:
namespaces: - config-demo projects: - config-demo applications: config-demo: name: config-demo namespace: config-demo project: config-demo path: charts/all/config-demo #Subscriptions can be added too - multicloud-gitops at present does not require subscriptions on its managed clusters #subscriptions: . # example-subscription # name: example-operator # namespace: example-namespace # channel: example-channel # csv: example-operator.v1.0.0 subscriptions: Ensure that you commit the changes and push them to GitHub so that GitOps can fetch your changes and apply them.
Deploying a managed cluster by using Red Hat Advanced Cluster Management Prerequistes An OpenShift cluster
To create an OpenShift cluster, go to the Red Hat Hybrid Cloud console.
Select OpenShift → Clusters → Create cluster.
Red Hat Advanced Cluster Management (RHACM) web console to join the managed cluster to the management hub
After RHACM is installed, a message regarding a Web console update is available&#34; might be displayed. Follow the instructions and click the Refresh web console link.
Procedure In the left navigation panel of web console, click local-cluster. Select All Clusters. The RHACM web console is displayed with Cluster* on the left navigation panel.
On the Managed clusters tab, click Import cluster.
On the Import an existing cluster page, enter the cluster name and choose KUBECONFIG as the &#34;import mode&#34;. Add the tag clusterGroup=region-one. Click Import.
Now that RHACM is no longer deploying the managed cluster applications everywhere, you must indicate that the new cluster has the managed cluster role.
Optional: Deploying a managed cluster by using cm-cli tool Prerequistes An OpenShift cluster
To create an OpenShift cluster, go to the Red Hat Hybrid Cloud console.
Select OpenShift -&gt; Clusters -&gt; Create cluster.
The cm-cli tool
Procedure Obtain the KUBECONFIG file from the managed cluster.
Open a shell prompt and login into the management hub cluster by using either of the following methods:
oc login or
export KUBECONFIG=~/&lt;path_to_kubeconfig&gt; Run the following command:
cm attach cluster --cluster &lt;cluster-name&gt; --cluster-kubeconfig &lt;path-to-path_to_kubeconfig&gt; Next steps Designate the new cluster as a managed cluster site
Optional: Deploying a managed cluster by using the clusteradm tool Prerequistes An OpenShift cluster
To create an OpenShift cluster, go to the Red Hat Hybrid Cloud console.
Select OpenShift → Clusters → Create cluster.
The clusteradm tool
Procedure To deploy an edge cluster, you must to get the token from the management hub cluster. Run the following command on the existing management hub or datacenter cluster:
clusteradm get token The command generates a token and shows you the command to use on the managed cluster.
Login to the managed cluster with either of the following methods:
oc login or
export KUBECONFIG=~/&lt;path_to_kubeconfig&gt; To request that the managed join the hub cluster, run the following command:
clusteradm join --hub-token &lt;token_from_clusteradm_get_token_command&gt; &lt;managed_cluster_name&gt; Accept the join request on the hub cluster:
clusteradm accept --clusters &lt;managed_cluster_name&gt; Next steps Designate the new cluster as a managed cluster site
Designate the new cluster as a managed cluster site If you use the command line tools such as clusteradm or cm-cli, you must explicitly indicate that the imported cluster is part of a specific clusterGroup. Some examples of clusterGroup are factory, devel, or prod.
To tag the cluster as clusterGroup=&lt;managed-cluster-group&gt;, complete the following steps.
Procedure To find the new cluster, run the following command:
oc get managedcluster.cluster.open-cluster-management.io To apply the label, run the following command:
oc label managedcluster.cluster.open-cluster-management.io/YOURCLUSTER site=managed-cluster Verification Go to your managed cluster (edge) OpenShift console and check for the open-cluster-management-agent pod being launched. It might take a while for the RHACM agent and agent-addons to launch. After that, the OpenShift GitOps Operator is installed. On successful installation, launch the OpenShift GitOps (ArgoCD) console from the top right of the OpenShift console.
`,url:"https://validatedpatterns.io/patterns/multicloud-gitops/mcg-managed-cluster/",breadcrumb:"/patterns/multicloud-gitops/mcg-managed-cluster/"},"https://validatedpatterns.io/learn/quickstart/":{title:"Patterns quick start",tags:[],content:` Patterns quick start Each pattern can be deployed using the command line. The only requirement is to have git and podman installed. See the Prerequisite installation instructions for more information.
Patterns deployment requires several tools including Helm to install. However, the validated patterns framework removes the need to install and maintain these tools. The pattern.sh script uses a container the includes the necessary tools. The use of that container is why you need to install podman.
Check the values-*.yaml for changes that are needed before deployment. After changing the values-*.yaml files where needed and pushing them to your git repository, you can run ./pattern.sh make install from your local repository directory and that will deploy the datacenter/hub cluster for a pattern. Edge clusters are deployed by joining/importing them into ACM on the hub.
Alternatively to the ./pattern.sh make install method, you can use the validated pattern operator available in the OpenShift console.
For information on using the Validated Patterns Operator, see Using the Validated Pattern Operator.
Follow any other post-install instructions for the pattern on that pattern’s Getting started page.
Prerequisite installation instructions Tested Operating systems The following instructions have been tested on the following operating systems:
Red Hat Enterprise Linux 8 and 9
CentOS 8 and 9
Fedora 36 and onwards
Debian Bookworm
Ubuntu 22.04
Mac OSX Big Sur and onwards
Red Hat Enterprise Linux 8 and 9 Make sure that you have both the appstream and the baseos repositories configured. For example on RHEL 8 you will get the following:
sudo dnf repolist Updating Subscription Management repositories. repo id repo name rhel-8-for-x86_64-appstream-rpms Red Hat Enterprise Linux 8 for x86_64 - AppStream (RPMs) rhel-8-for-x86_64-baseos-rpms Red Hat Enterprise Linux 8 for x86_64 - BaseOS (RPMs) Install podman and git:
sudo dnf install -y podman git Fedora Install podman and git:
sudo dnf install -y podman git Debian and derivatives Install podman and git:
sudo apt-get install -y podman git Mac OSX Install podman and git:
/bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34; brew install podman git # Containers on MacOSX run in a VM which is managed by &#34;podman machine&#34; commands podman machine init -v \${HOME}:\${HOME} -v /private/tmp/:/private/tmp podman machine start `,url:"https://validatedpatterns.io/learn/quickstart/",breadcrumb:"/learn/quickstart/"},"https://validatedpatterns.io/patterns/devsecops/devel-cluster/":{title:"Secured Development Cluster",tags:[],content:`Having a development cluster (devel) join the hub Introduction Development clusters are responsible for building applications and delivering the applications to a secured registry. The development cluster defines a secure pipeline that includes code and image scans and image signing before delivering them to the registry. OpenShift Pipelines is used for the continuous integration (CI). The Quay registry is deployed on the hub and therefore integration is required for the development pipeline to push images to the registry.
Development clusters also need to be secured and so one part of the deployment is Advanced Cluster Security with a secured configuration. This allows ACS to monitor and report on security issues on the cluster. ACS secured sites report to an ACS Central application that is deployed on the hub.
Allow ACM to deploy the devel applications to a subset of clusters By default the devel applications are deployed on any development clusters that ACM knows about.
managedClusterGroups: - name: devel helmOverrides: - name: clusterGroup.isHubCluster value: &#34;false&#34; clusterSelector: matchLabels: clusterGroup: devel matchExpressions: - key: vendor operator: In values: - OpenShift Remember to commit the changes and push to GitHub so that GitOps can see your changes and apply them.
Deploy a development (devel) cluster For instructions on how to prepare and import a development (devel) cluster please read the section importing a cluster. Use clusterGroup=devel. .
You are done importing the development cluster That&rsquo;s it! Go to your devel (edge) OpenShift console and check for the open-cluster-management-agent pod being launched. Be patient, it will take a while for the ACM agent and agent-addons to launch. After that, the operator OpenShift GitOps will run. When it&rsquo;s finished check that all applications have synced in OpenShift GitOps. Select &ldquo;Devel Argo CD&rdquo; from the OpenShift Applications menu.
Then look at the GitOps applications and make sure they have synced completely.
Confirming successful deployment There are a number of steps you can do to check that the components have deployed:
Pipelines should be available in the console on the left hand side.
Run a pipeline and check the build and if the image gets updated in the Quay registry on the Hub.
You should be able to select the route to the demo application in the test environment.
The development cluster name should show up in the ACS Central console.
Next up Deploy the the Multicluster DevSecOps secured production cluster
`,url:"https://validatedpatterns.io/patterns/devsecops/devel-cluster/",breadcrumb:"/patterns/devsecops/devel-cluster/"},"https://validatedpatterns.io/patterns/retail/store/":{title:"Store Sites",tags:[],content:`Having a store (edge) cluster join the datacenter (hub) Allow ACM to deploy the store application to a subset of clusters A store (&ldquo;ATLANTA&rdquo;) is installed on the hub cluster by default. This feature is interesting if you want to see how ACM can manage a remote cluster to install the same application on a different cluster.
The way we apply this is through the managedClusterGroups block in values-hub.yaml:
managedClusterGroups: - name: store clusterSelector: matchLabels: clusterGroup: raleigh matchExpressions: - key: vendor operator: In values: - OpenShift Any cluster joined with the label clusterGroup=raleigh will be assigned the policies that deploy the store app to them.
Deploy a store cluster Rather than provide instructions on creating a store cluster it is assumed that an OpenShift cluster has already been created. Use the openshift-install program provided at cloud.redhat.com
There are a three ways to join the store to the datacenter.
Using the ACM user interface Using the cm tool Using the clusteradm tool Store setup using the ACM UI After ACM is installed a message regarding a &ldquo;Web console update is available&rdquo; may be displayed. Click on the &ldquo;Refresh web console&rdquo; link.
On the upper-left side you&rsquo;ll see a pull down labeled &ldquo;local-cluster&rdquo;. Select &ldquo;All Clusters&rdquo; from this pull down. This will navigate to the ACM console and to its &ldquo;Clusters&rdquo; section
Select the &ldquo;Import cluster&rdquo; option beside the Create Cluster button.
On the &ldquo;Import an existing cluster&rdquo; page, enter the cluster name and choose Kubeconfig as the &ldquo;import mode&rdquo;. Add the tag site=store Press import. Done.
Using this method, you are done. Skip to the section Store is joined but ignore the part about adding the site tag.
Store setup using cm tool Install the cm (cluster management) command-line tool. See details here
Obtain the KUBECONFIG file from the edge/store cluster.
On the command-line login into the hub/datacenter cluster (use oc login or export the KUBECONFIG).
Run the following command:
cm attach cluster --cluster &lt;cluster-name&gt; --cluster-kubeconfig &lt;path-to-KUBECONFIG&gt; Skip to the section Store is joined
Store setup using clusteradm tool You can also use clusteradm to join a cluster. The following instructions explain what needs to be done. clusteradm is still in testing.
To deploy a edge cluster you will need to get the datacenter (or hub) cluster&rsquo;s token. You will need to install clusteradm. On the existing datacenter cluster:
clusteradm get token
When you run the clusteradm command above it replies with the token and also shows you the command to use on the store. So first you must login to the store cluster
oc login or
export KUBECONFIG=~/my-ocp-env/store
Then request to that the store join the datacenter hub
clusteradm join --hub-token &lt;token from clusteradm get token command &gt; &lt;store cluster name&gt;
Back on the hub cluster accept the join request
clusteradm accept --clusters &lt;store-cluster-name&gt;
Skip to the next section, Store is joined
Store is joined You&rsquo;re done That&rsquo;s it! Go to your store (edge) OpenShift console and check for the open-cluster-management-agent pod being launched. Be patient, it will take a while for the ACM agent and agent-addons to launch. After that, the operator OpenShift GitOps will run. When it&rsquo;s finished coming up launch the OpenShift GitOps (ArgoCD) console from the top right of the OpenShift console.
`,url:"https://validatedpatterns.io/patterns/retail/store/",breadcrumb:"/patterns/retail/store/"},"https://validatedpatterns.io/learn/importing-a-cluster/":{title:"Importing a cluster",tags:[],content:` Importing a managed cluster Many validated patterns require importing a cluster into a managed group. These groups have specific application sets that will be deployed and managed. Some examples are factory clusters in the Industrial Edge pattern, or development clusters in Multi-cluster DevSecOps pattern.
Red Hat Advanced Cluster Management (RHACM) can be used to create a cluster of a specific cluster group type. You can deploy a specific cluster that way if you have RHACM set up with credentials for deploying clusters. However in many cases an OpenShift cluster has already been created and will be imported into the set of clusters that RHACM is managing.
While you can create and deploy in this manner this section concentrates on importing an existing cluster and designating a specific managed cluster group type.
To deploy a cluster that can be imported into RHACM, use the openshift-install program provided at console.redhat.com. You will need login credentials.
Importing a cluster using the RHACM User Interface Getting to the RHACM user interface After ACM is installed a message regarding a &#34;Web console update is available&#34; will be displayed. Click on the &#34;Refresh web console&#34; link.
On the upper-left side you’ll see a pull down labeled &#34;local-cluster&#34;. Select &#34;All Clusters&#34; from this pull down. This will navigate to the RHACM console and to its &#34;Clusters&#34; section
Select the &#34;Import cluster&#34; option.
Importing the cluster On the &#34;Import an existing cluster&#34; page, enter the cluster name (arbitrary) and choose Kubeconfig as the &#34;import mode&#34;. Add the tag clusterGroup= using the appropriate cluster group specified in the pattern. Press Import.
Using this method, you are done. Skip to the section in your pattern documentation that describes how you can confirm the pattern deployed correctly on the managed cluster.
Other potential import tools There are a two other known ways to join a cluster to the RHACM hub. These methods are not supported but have been tested once. The patterns team no longer tests these methods. If these methods become supported we will maintain the documentation here.
Using the cm-cli tool
Using the clusteradm tool
Importing a cluster using the cm-cli tool Install the cm-cli (cm) (cluster management) command-line tool. See installation instructions here: cm-cli installation
Obtain the KUBECONFIG file from the managed cluster.
On the command-line login into the hub/datacenter cluster (use oc login or export the KUBECONFIG).
Run the following command:
cm attach cluster --cluster &lt;cluster-name&gt; --cluster-kubeconfig &lt;path-to-KUBECONFIG&gt; Importing a cluster using clusteradm tool You can also use clusteradm to join a cluster. The following instructions explain what needs to be done. clusteradm is still in testing.
To deploy a edge cluster you will need to get the hub/datacenter cluster’s token. You will need to install clusteradm. When it is installed run the following on existing hub/datacenter cluster:
clusteradm get token When you run the clusteradm command above it replies with the token and also shows you the command to use on the managed. Login to the managed cluster with either of the following methods:
oc login or
export KUBECONFIG=~/&lt;path-to-kubeconfig&gt; Then request to that the managed join the datacenter hub.
clusteradm join --hub-token &lt;token from clusteradm get token command &gt; &lt;managed-cluster-name&gt; Back on the hub cluster accept the join request.
clusteradm accept --clusters &lt;managed-cluster-name&gt; Designate the new cluster as a devel site If you use the command line tools above you need to explicitly indicate that the imported cluster is part of a specific clusterGroup. If you haven’t tagged the cluster as clusterGroup=&lt;managed-cluster-group&gt; then do that now. Some examples of clusterGroup are factory, devel, or prod.
We do this by adding the label referenced in the managedSite’s clusterSelector.
Find the new cluster.
oc get managedclusters.cluster.open-cluster-management.io Apply the label.
oc label managedclusters.cluster.open-cluster-management.io/&lt;your-cluster&gt; clusterGroup=&lt;managed-cluster-group&gt; `,url:"https://validatedpatterns.io/learn/importing-a-cluster/",breadcrumb:"/learn/importing-a-cluster/"},"https://validatedpatterns.io/contribute/structure/":{title:"Validated pattern structure",tags:[],content:`Validated pattern structure Framework fundamentals The validated patterns framework uses OpenShift GitOps (ArgoCD) as the primary driver for deploying patterns and keeping them up to date. Validated patterns use Helm charts as the primary artifacts for GitOps. Helm charts provide a mechanism for templating that is powerful when building repeatable, automated deployments across different deployment environments (i.e. clouds, data-centers, edge, etc.)
Many Cloud Native Computing Foundation (CNCF) projects use Operators to manage the lifecycle of their service. Whenever possible, validated patterns will make use of these Operators to deploy the application service.
Red Hat Advanced Cluster Management (ACM) is primarily used to automate the deployment of edge clusters. It provides subscription information for specific deployment sites.
OpenShift Pipelines is used to automate builds and keep image repositories up to date.
Pattern directories tour Examining any of the existing patterns reveals the important organizational part of the validated patterns framework. Let&rsquo;s take a look at a couple of the existing validated patterns: Multicluster GitOps and Industrial Edge.
~/g/multicloud-gitops on main ◦ tree -L 2 . ├── charts │ └── region ├── common │ ├── acm │ ├── clustergroup │ ├── common -&gt; . │ ├── examples │ ├── install │ ├── Makefile │ ├── Makefile.toplevel │ ├── pattern-vault.init │ ├── reference-output.yaml │ ├── scripts │ ├── tests │ └── values-global.yaml ├── Makefile ├── README.md ├── scripts │ └── make_common_subtree.sh ├── values-global.yaml ├── values-hub.yaml └── values-region-one.yaml 11 directories, 11 files First we notice some subdirectories: charts, common, and scripts - along with values- yaml files.
~/g/industrial-edge on stable-2.0 ◦ tree -L 2 . ├── charts │ ├── datacenter │ └── factory ├── common │ ├── acm │ ├── common -&gt; . │ ├── examples │ ├── install │ ├── Makefile │ ├── Makefile.toplevel │ ├── scripts │ ├── site │ ├── values-datacenter.yaml │ └── values-global.yaml ├── docs │ ├── images │ └── old-deployment-map.txt ├── images │ ├── import-cluster.png │ ├── import-with-kubeconfig.png │ └── launch-acm-console.png ├── Makefile ├── README.md ├── scripts │ └── sleep-seed.sh ├── SUPPORT_AGREEMENT.md ├── values-datacenter.yaml ├── values-factory.yaml ├── values-global.yaml └── values-secret.yaml.template 14 directories, 16 files We see the same or similar files in the Industrial Edge pattern above.
The charts directory This is where validated patterns keep the helm charts for a pattern. The helm charts are used deploy and manage the various components of the applications deployed at a site. By convention, the charts are broken out by site location. You may see datacenter, hub, factory, or other site names in there.
Each site has sub-directories based on application or library component groupings.
From Helm documentation: Application charts are a collection of templates that can be packaged into versioned archives to be deployed.
Library charts provide useful utilities or functions for the chart developer. They&rsquo;re included as a dependency of application charts to inject those utilities and functions into the rendering pipeline. Library charts do not define any templates and therefore cannot be deployed.
These groupings are used by OpenShift GitOps to deploy into the cluster. The configurations for each of the components inside an application are synced every three minutes by OpenShift GitOps to make sure that the site is up to date. The configuration can also be synced manually if you do not wish to wait up to three minutes.
The configuration YAML for each component of the application is stored in the templates sub-directory. For example, the Industrial Edge datacenter has a application called Kafka. The configuration for Kafka is stored in kafka/templates.
The common directory There are several common components that are in use across the validated patterns that exist today. E.g. the External Secrets operator and RHACM (Red Hat Advanced Cluster Management). These components are part of the validated patterns framework. They are configured to work together in the GitOps based framework. Rather than duplicating the configuration in each pattern, these common technologies are moved into a common directory. If there are pattern specific post-deployment configurations to be applied, those should be added to the Helm charts in the charts directory structure. This is unlikely for these components. Consider common out of bounds unless you are working on modifying the framework.
The scripts directory Sometimes an Operator and/or the Helm charts still leave some work to be done with regard to final configuration. When extra code is needed to deploy, the extra code is placed in the scripts directory. The majority of the time a consumer of a validated pattern will only use this code through the existing automation. I.e. The Makefile or OpenShift GitOps will make use of these scripts. So - If there is extra massaging required for your application, put the scripts in here and try to run them from within the automation. It is very unlikely you will need to change the scripts directory. Consider scripts out of bounds unless you are modifying the framework.
Applications and values- files Helm uses values.yaml files to pass variables into charts. Variables in the values.yaml file can be overridden in the following ways:
By a values.yaml file in the parent directory By a values file passed into the helm &lt;install/upgrade&gt; command using -f By specifying an override individual value in the the helm command with --set For more information on values files and their usage see the values files section of the Helm documentation.
This section is meant as an introduction to the values- files that the framework uses to override values in the chart templates. In the Getting Started pages there will be more specific usage details.
There are three types of value- files. values-global.yaml: This is used to set variables for helm charts across the pattern. It contains the name of the pattern and sets some other variables for artifacts like, image registry, Git repositories, GitOps syncPolicy etc. values-&lt;site&gt;.yaml: Each specific site requires information regarding what applications and subscriptions are required for that site. This file contains a list of namespaces, applications, subscriptions, the operator versions etc. for that site. values-secret.yaml.template: All patterns require some secrets for artifacts included in the pattern. E.g. credentials for GitHub, AWS, or Quay.io. The framework provides a safe way to load those secrets into a vault for consumption by the pattern. This template file can be copied to your home directory, the secret values applied, and the validated pattern will go look for values-secrets.yaml in your home directory. Do not leave a values-secrets.yaml file in your cloned git directory or it may end up in your (often public) Git repository, like GitHub. Values files can have some overrides. Version overrides can be used to set specific values for OCP versions. E.g. values-hub-4.12.yaml allows you to tweak a specific value for OCP 4.12 on the Hub cluster.
Version overrides can be used to set specific values for specific cloud providers. E.g. values-AWS.yaml would allow you to tweak a specific value for all cluster groups deployed on AWS.
Other combination examples include: values-hub-Azure.yaml only apply this Azure tweak on the hub cluster.
values-4.12.yaml apply these OCP 4.12 tweaks to all cluster groups in this pattern.
Current supported cloud providers include AWS, Azure, and GCP.
Applications &amp; subscriptions Environment values and Helm The reason the above values files exist is to take advantage of Helms ability to use templates and substitute values into your charts. This makes the pattern very portable.
The following messaging-route.yaml example shows how the AMQ messaging service is using values set in the values-global.yaml file for Industrial Edge.
apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: messaging name: messaging spec: host: messaging-manuela-tst-all.apps.{{ .Values.global.datacenter.clustername }}.{{ .Values.global.datacenter.domain }} port: targetPort: 3000-tcp to: kind: Service name: messaging weight: 100 wildcardPolicy: None The values in the values-global.yaml will be substituted when the YAML is applied to the cluster.
global: pattern: industrial-edge ... datacenter: clustername: ipbabble-dc domain: blueprints.rhecoeng.com edge: clustername: ipbabble-f1 domain: blueprints.rhecoeng.com `,url:"https://validatedpatterns.io/contribute/structure/",breadcrumb:"/contribute/structure/"},"https://validatedpatterns.io/patterns/industrial-edge/application/":{title:"Application Demos",tags:[],content:`Demonstrating Industrial Edge example applications Background Up until now the Industrial Edge 2.0 validated patterns has focused primarily on successfully deploying the architectural pattern. Now it is time to see GitOps and DevOps in action as we go through a number of demonstrations to change both configuration information and the applications that we are deploying.
If you have already deployed the data center and optionally a factory (edge) cluster, then you have already seen several applications deployed in the OpenShift GitOps console. If you haven&rsquo;t done this then we recommend you deploy the data center after you have setup the Quay repositories described below.
Prerequisite preparation Quay public registry setup In the Quay.io registry please ensure you have the following repositories and that they are set for public access. Replace your-org with the name of your organization or Quay.io username.
your-org/iot-software-sensor your-org/iot-consumer your-org/iot-frontend your-org/iot-anomaly-detection your-org/http-ionic These repositories are needed in order to provide container images built at the data center to be consumed by the factories (edge).
Local laptop/workstation Make sure you have git and OpenShift&rsquo;s oc command-line clients.
OpenShift Cluster Make sure you have the kubeadmin administrator login for the data center cluster. Use this or the kubeconfig (export the path) to provide administrator access to your data center cluster. It is not required that you have access to the edge (factory) clusters. GitOps and DevOps will take care of the edge clusters.
GitHub account You will need to login into GitHub and be able to fork two repositories.
hybrid-cloud-patterns/industrial-edge hybrid-cloud-patterns/manuela-dev Configuration changes with GitOps There will may be times where you need to change the configuration of some of the edge devices in one or more of your factories. In our example, we have various sensors at the factory. Modification can be made to these sensors using ConfigMaps.
In this demonstration we will turn on a temperature sensor for sensor #2. We will first do this in the data center because this will demonstrate the power of GitOps without having to involve the edge/factory. However if you do have an factory joined using Advanced Cluster Management, then the changes will make their way out to the factory. But it is not necessary for the demo as we have a complete test environment on the data center.
Make sure you are able to see the dashboard application in a tab on your browser. You can find the URL for the dashboard application by looking at the following in your OpenShift console.
Select Networking-&gt;Routes on the left-hand side of the console. Using the Projects pull-down, select manuela-tst-all. Click on the URL under the Location column for the route Name line-dashboard. this will launch the line-dashboard monitoring application in a browser tab. The URL will look like:
line-dashboard-manuela-tst-all.apps.*cluster-name*.*domain*
Once the the application is open in your browser, click on the “Realtime Data” Navigation on the left and wait a bit. Data should be visualized as received. Note that there is only vibration data shown! If you wait a bit more (usually every 2-3 minutes), you will see an anomaly and alert on it.
Now let&rsquo;s turn on the temperature sensor. Using you favorite editor, edit the following file:
industrial-edge/charts/data-center/manuela-test/templates/machine-sensor/machine-sensor-2-configmap.yaml Change SENSOR_TEMPERATURE_ENABLED: &quot;false&quot; to SENSOR_TEMPERATURE_ENABLED: &quot;true&quot;.
Then change and commit this to your git repository so that the change will be picked up by OpenShift GitOps (ArgoCD).
git add industrial-edge-charts/data/center/manuela-test/templates/machine-sensor/machine-sensor-2-configmap.yaml git commit -m &#34;Turned on temprature sensor for machine sensor #2&#34; git push You can track the progress of this commit/push in your OpenShift GitOps console in the manuela-test-all application. You will notice components regarding machine-sensor-2 getting sync-ed. You can speed this up by manually pressing the Refresh button.
The dashboard app should pickup the change automatically, once data from the temperature sensor is received. Sometimes a page/tab refreshed is needed for the change to be picked up.
Application changes using DevOps The line-dashboard application has temperature sensors. In this demonstration we are going to make a simple change to that application, rebuild and redeploy it. In the manuela-dev repository there is a file components/iot-consumer/index.js. This JavaScript program consumes message data coming from the line servers and one of functions it performs is to check the temperature to see if it has exceeded a threshold. There is three lines of code in there that does some Celsius to Fahrenheit conversion.
Depending on the state of your manuela-dev repository this may or may not be commented out. Ideally for the demonstration you would want it uncommented and therefore effective. What this means is that while the labels on the frontend application are showing Celsius, the data is actually in Fahrenheit. This is a good place to start because that data won&rsquo;t make any sense.
Machines running over 120C is not normal. However examining the code explains why. There is an erroneous conversion taking place. What must happen is we remove or comment out this code.
If you haven&rsquo;t deployed the uncommented code it might be best to prepare that before the demonstration. After pointing out the problem, comment out the code.
Now that the erroneous conversion code has been commented out it is is time rebuild and redeploy. First commit and push the code to the repository. While in the directory for your manuela-dev repository run the following commands. The components/iot-consumer/index.js file should be the only changed file.
git add components/iot-consumer/index.js git commit -m &#34;commented out C to F temp conversion&#34; git push Now its time to kick off the CI pipeline. Due to the need for GitHub secrets and Quay secrets as part of this process, we currently can&rsquo;t use the OpenShift console&rsquo;s Pipelines to kick off the pipeline in the demo environment. Instead, use the command-line. While in the industrial-edge repository directory, run the following:
make build-and-test This build takes some time because the pipeline is rebuilding all the images. You can monitor the pipeline&rsquo;s progress in the Openshift console&rsquo;s pipelines section.
Alternatively you can can try and run the shorter build-iot-consumer pipeline run in the OpenShift console. This should just run and test the specific application.
You can also see some updates happening in the manuela-tst application in OpenShift GitOps (ArgoCD).
When the pipeline is complete check the lines-dashboard application again in the browser. More reasonable, Celsius, temperatures are displayed. (Compare with above.)
The steps above have successfully applied the change to the Manuela test environment at the data center. In order for these changes to be pushed out to the factories it must be accepted and pushed to the Git repository. Examine the project in GitHub. There is a new Pull Request (PR) called Pull request created by Tekton task github-add-pull-request. Select that PR and merge the pull request.
OpenShift GitOps will see the new change and apply it out to the factories.
Application AI model changes with DevOps After a successful deployment of Industrial Edge 2.0, check to see that Jupyter Hub is running. To do this go to project manuela-ml-workspace check that jupyterhub pods are up and running.
Then, in the same project manuela-ml-namespace, select Networking/Routes and click on the URL associated with jupyterhub in the Location column.
This will bring you to a web page at an address in the following format:
jupyterhub-manuela-ml-workspace.apps.*clustername*.*your-domain* Options for different types of Jupyter servers are shown. There are two options that are useful for this demo.
Standard Data Science. Select this notebook image for simpler notebooks like Data Analyses.ipynb Tensorflow Notebook Image. Select this notebook image for more a complex notebook that require Tensorflow. E.g. Anomaly Detection-using-TF-and-Deep-Learning.ipynb At the bottom of the screen there is a Start server button. Select the type of Notebook server image and press Start server.
Selecting Tensorflow notebook image:
On the next screen upload the following files from manuela-dev/ml-models/anomaly-detection:
One of the Jupyter notebooks Data-Analyses.ipynb for a somewhat simpler demo Anomaly Detection-using-TF-and-Deep-Learning.ipynb for a Tensorflow demo. raw-data.cvs Open the notebook by double clicking on the notebook file (ending in .ipynb)
After opening the notebook successfully, walk through the demonstration by pressing play and iterating through the commands in the playbook. Jupyter playbooks are interactive and you may make changes and also save those changes. Also, some steps in the notebook take milliseconds, however, other steps can take a long time (up to an hour), so check on the completion of steps.
Remember that changes to the notebook will require downloading, committing, and pushing that notebook to the git repository so that it gets redeployed to the factories.
Turning on event streaming between the edge and the datacenter There is one other area that has not been completed for the overall validated pattern. The unfinished part is the streaming of events from the factory back to the datacenter and adding that data to the data lake for data scientists to apply their &ldquo;magic&rdquo;.
The automation for this is complete. However, there are certificates and/or keys that need to be replaced in the following files for the datacenter and factory templates:
industrial-edge/charts/datacenter/kafka/templates/kafka-tls-certificate-and-key.yaml industrial-edge/charts/factory/templates/factory-kafka-cluster/kafka-tls-certificate-and-key.yaml industrial-edge/charts/factory/templates/factory-mirror-maker/kafka-tls-certificate.yaml See the Yaml files for more details.
After updating these files with the proper certs/keys, apply the changes to the OpenShift cluster using oc apply.
`,url:"https://validatedpatterns.io/patterns/industrial-edge/application/",breadcrumb:"/patterns/industrial-edge/application/"},"https://validatedpatterns.io/patterns/retail/application/":{title:"Application Demos",tags:[],content:`Demonstrating Retail example applications Background Up until now the Retail validated pattern has focused primarily on successfully deploying the architectural pattern. Now it is time to see the actual applications running as we have deployed them.
If you have already deployed the hub cluster, then you have already seen several applications deployed in the OpenShift GitOps console. If you haven&rsquo;t done this then we recommend you deploy the hub after you have setup the Quay repositories described below.
Ordering Items at the Coffeeshop The easiest way to get to the coffeeshop store page is from the OpenShift Console Menu Landing Page entry:
Clicking on the Quarkus Coffeeshop Landing Page link will bring you to this page:
And clicking on either the &ldquo;Store Web Page&rdquo; or &ldquo;TEST Store Web Page&rdquo; links will bring you to a screen that looks like this:
NOTE: The applications are initially identical. The &ldquo;TEST&rdquo; site is deployed to the quarkuscoffeeshop-demo namespace; the regular Store site is deployed to the quarkuscoffeeshop-store namespace.
Each store requires supporting services, in PostgreSQL and Kafka. In our pattern, PostgreSQL is provided by the Crunchy PostgreSQL operator, and Kafka is provided by the Red Hat AMQ Streams operator. Each instance, the regular instance and the TEST instance, has its own instance of each of these supporting services it uses.
To order, click on the &ldquo;Place an Order&rdquo; button on the front page. The menu should look like this:
Click the &ldquo;Add&rdquo; button next to a menu item; the item name will appear. Add a name for the order:
You can add as many orders as you want. On your last item, click the &ldquo;Place Order&rdquo; button on the item dialog:
As the orders are serviced by the barista and kitchen services, you can see their status in the &ldquo;Orders&rdquo; section of the page:
`,url:"https://validatedpatterns.io/patterns/retail/application/",breadcrumb:"/patterns/retail/application/"},"https://validatedpatterns.io/patterns/multicloud-gitops/mcg-cluster-sizing/":{title:"Cluster sizing",tags:[],content:` About OpenShift cluster sizing for the Multicloud GitOps Pattern To understand cluster sizing requirements for the Multicloud GitOps pattern, consider the following components that the Multicloud GitOps pattern deploys on the datacenter or the hub OpenShift cluster:
Name Kind Namespace Description multicloud-gitops-hub
Application
multicloud-gitops-hub
Hub GitOps management
Red Hat Advanced Cluster Management
Operator
open-cluster-management
Advance Cluster Management
Red Hat OpenShift GitOps
Operator
openshift-operators
OpenShift GitOps
The following are the minimum requirements for sizing of nodes for OpenShift Container Platform 4.x:
Minimum 4 vCPU** (additional are strongly recommended)
Minimum 16 GB RAM** (additional memory is strongly recommended, especially if etcd is colocated on Control Planes)
Minimum 40 GB hard disk space for the file system containing /var/
Minimum 1 GB hard disk space for the file system containing /usr/local/bin/
The Multicloud GitOps pattern also includes the Red Hat Advanced Cluster Management (RHACM) supporting operator that is installed by OpenShift GitOps using Argo CD.
Multicloud GitOps pattern with OpenShift datacenter hub cluster size The Multicloud GitOps pattern has been tested with a defined set of specifically tested configurations that represent the most common combinations that Red Hat OpenShift Container Platform customers are using or deploying for the x86_64 architecture.
The datacenter hub OpenShift cluster uses the following the AWS deployment configuration:
Node Type Number of nodes Cloud Provider Instance Type Control Plane
4
Amazon Web Services
m5.xlarge
Worker
3
Amazon Web Services
m5.xlarge
The datacenter hub OpenShift cluster needs to be a bit bigger than the Factory/Edge clusters because this is where the developers will be running pipelines to build and deploy the Multicloud GitOps pattern on the cluster. The above cluster sizing is close to a minimum size for a Datacenter HUB cluster. In the next few sections we take some snapshots of the cluster utilization while the Multicloud GitOps pattern is running. Keep in mind that resources will have to be added as more developers are working building their applications.
Multicloud GitOps pattern with OpenShift managed datacenter cluster size The standard datacenter deployment of an OpenShift cluster is 3 control plane nodes and 3 or more worker nodes.
Node Type Number of nodes Cloud Provider Instance Type Control Plane/Worker
3
Google Cloud
n1-standard-8
Control Plane/Worker
3
Amazon Cloud Services
m5.2xlarge
Control Plane/Worker
3
Microsoft Azure
Standard_D8s_v3
`,url:"https://validatedpatterns.io/patterns/multicloud-gitops/mcg-cluster-sizing/",breadcrumb:"/patterns/multicloud-gitops/mcg-cluster-sizing/"},"https://validatedpatterns.io/patterns/ansible-edge-gitops/cluster-sizing/":{title:"Cluster Sizing",tags:[],content:`OpenShift Cluster Sizing for the Ansible Edge GitOps Pattern Tested Platforms The Ansible Edge GitOps pattern has been tested on AWS:
Certified Cloud Providers 4.9 4.10 Amazon Web Services Tested The pattern is adaptable to running on bare metal/on-prem clusters but has not yet been tested there.
General OpenShift Minimum Requirements OpenShift 4 has the following minimum requirements for sizing of nodes:
Minimum 4 vCPU (additional are strongly recommended). Minimum 16 GB RAM (additional memory is strongly recommended, especially if etcd is colocated on Control Planes). Minimum 40 GB hard disk space for the file system containing /var/. Minimum 1 GB hard disk space for the file system containing /usr/local/bin/. There is one application that comprises the Ansible Edge GitOps pattern. In addition, the Ansible Edge GitOps pattern also includes the Advanced Cluster Management (ACM) supporting operator that is installed by OpenShift GitOps using ArgoCD.
Ansible Edge GitOps Pattern Components Here&rsquo;s an inventory of what gets deployed by the Ansible Edge GitOps pattern on the Datacenter/Hub OpenShift cluster:
Name Kind Namespace Description Ansible Edge GitOps-hub Application Ansible Edge GitOps-hub Hub GitOps management Red Hat OpenShift GitOps Operator openshift-operators OpenShift GitOps Red Hat Ansible Automation Platform Operator ansible-automation-platform Ansible Automation Red Hat OpenShift Data Foundations Operator openshift-storage OpenShift Storage solution Red Hat OpenShift Virtualization Operator openshift-cnv Virtualization software to run VMs Edge GitOps VMs VMs edge-gitops-vms Simulated Edge environment with VMs to manage Hashicorp Vault Operator vault Secrets Storage External Secrets Operator (ESO) Operator golang-external-secrets Abstraction for secrets storage Ansible Edge GitOps Pattern OpenShift Datacenter HUB Cluster Size The Ansible Edge GitOps pattern has been tested with a defined set of specifically tested configurations that represent the most common combinations that Red Hat OpenShift Container Platform (OCP) customers are using or deploying for the x86_64 architecture.
The Hub OpenShift Cluster is made up of the the following on the AWS deployment tested:
Node Type Number of nodes Cloud Provider Instance Type Control Plane 3 Amazon Web Services m5.xlarge Worker 3 Amazon Web Services m5.4xlarge Worker 1 Amazon Web Services c5n.metal The metal node is added to the cluster by the installation process after initial provisioning. The pattern on the hub requires OpenShift Data Fabric to support Virtual Machine storage and is a minimum size for a Hub cluster. In the next few sections we take some snapshots of the cluster utilization while the Ansible Edge GitOps pattern is running. Keep in mind that resources will have to be added as more developers are working building their applications.
Datacenter Cluster utilization Below is a snapshot of the OpenShift cluster utilization while running the Ansible Edge GitOps pattern:
CPU CPU% Memory Memory% 321m 0% 12511Mi 6% 736m 21% 7533Mi 51% 673m 4% 9298Mi 14% 920m 26% 8635Mi 59% 673m 4% 9258Mi 14% 921m 26% 9407Mi 65% 395m 2% 5149Mi 8% AWS Instance Types The Ansible Edge GitOps pattern was tested with the highlighted AWS instances in bold. The OpenShift installer will let you know if the instance type meets the minimum requirements for a cluster.
The message that the openshift installer will give you will be similar to this message
INFO Credentials loaded from default AWS environment variables FATAL failed to fetch Metadata: failed to load asset &#34;Install Config&#34;: [controlPlane.platform.aws.type: Invalid value: &#34;m4.large&#34;: instance type does not meet minimum resource requirements of 4 vCPUs, controlPlane.platform.aws.type: Invalid value: &#34;m4.large&#34;: instance type does not meet minimum resource requirements of 16384 MiB Memory] Below you can find a list of the AWS instance types that can be used to deploy the Ansible Edge GitOps pattern.
Instance type Default vCPUs Memory (GiB) Datacenter Factory/Edge 3x3 OCP Cluster 3 Node OCP Cluster m4.xlarge 4 16 N N m4.2xlarge 8 32 Y Y m4.4xlarge 16 64 Y Y m4.10xlarge 40 160 Y Y m4.16xlarge 64 256 Y Y m5.xlarge 4 16 Y N m5.2xlarge 8 32 Y Y m5.4xlarge 16 64 Y Y m5.8xlarge 32 128 Y Y m5.12xlarge 48 192 Y Y m5.16xlarge 64 256 Y Y m5.24xlarge 96 384 Y Y The OpenShift cluster is made of 3 Control Plane nodes and 4 Workers for the Hub cluster; 3 workers are standard compute nodes and one is c5n.metal. For the node sizes we used the m5.4xlarge on AWS and this instance type met the minimum requirements to deploy the Ansible Edge GitOps pattern successfully on the Hub cluster.
This pattern is currently only usable on AWS because of the integration of OpenShift Virtualization; it would be straightforward to adapt this pattern also to run on bare metal/on-prem clusters. If and when other public cloud providers support metal node provisioning in OpenShift Virtualization, we will document that here.
`,url:"https://validatedpatterns.io/patterns/ansible-edge-gitops/cluster-sizing/",breadcrumb:"/patterns/ansible-edge-gitops/cluster-sizing/"},"https://validatedpatterns.io/patterns/multicloud-gitops-portworx/cluster-sizing/":{title:"Cluster Sizing",tags:[],content:`About OpenShift cluster sizing for the Multicloud GitOps Pattern Support matrix for Multicloud GitOps pattern The Multicloud GitOps pattern has been tested in the following Certified Cloud Providers.
Certified Cloud Providers 4.8 4.9 4.10 Amazon Web Services :heavy_check_mark: Microsoft Azure :heavy_check_mark: Google Cloud Platform :heavy_check_mark: Minimum requirements for OpenShift Container Platform OpenShift 4 has the following minimum requirements for sizing of nodes:
Minimum 4 vCPU** (additional are strongly recommended) Minimum 16 GB RAM** (additional memory is strongly recommended, especially if etcd is colocated on Control Planes) Minimum 40 GB hard disk space for the file system containing /var/ Minimum 1 GB hard disk space for the file system containing /usr/local/bin/ There is one application that comprises the Medical Diagnosis pattern. In addition, the Multicloud GitOps pattern also includes the Red Hat Advanced Cluster Management (RHACM) supporting operator that is installed by OpenShift GitOps using ArgoCD.
Understanding Multicloud GitOps pattern components Here&rsquo;s an inventory of what gets deployed by the Multicloud GitOps pattern on the Datacenter/Hub OpenShift cluster:
Name Kind Namespace Description multicloud-gitops-hub Application multicloud-gitops-hub Hub GitOps management Red Hat Advanced Cluster Management Operator open-cluster-management Advance Cluster Management Red Hat OpenShift GitOps Operator openshift-operators OpenShift GitOps Multicloud GitOps pattern with OpenShift datacenter hub cluster size The Multicloud GitOps pattern has been tested with a defined set of specifically tested configurations that represent the most common combinations that Red Hat OpenShift Container Platform (OCP) customers are using or deploying for the x86_64 architecture.
The datacenter hub OpenShift cluster is made up of the the following on the AWS deployment tested:
Node Type Number of nodes Cloud Provider Instance Type Control Plane 4 Amazon Web Services m5.xlarge Worker 3 Amazon Web Services m5.xlarge The datacenter hub OpenShift cluster needs to be a bit bigger than the Factory/Edge clusters because this is where the developers will be running pipelines to build and deploy the Multicloud GitOps pattern on the cluster. The above cluster sizing is close to a minimum size for a Datacenter HUB cluster. In the next few sections we take some snapshots of the cluster utilization while the Multicloud GitOps pattern is running. Keep in mind that resources will have to be added as more developers are working building their applications.
Datacenter cluster utilization Below is a snapshot of the OpenShift cluster utilization while running the Multicloud GitOps pattern:
CPU Memory File System Network Pod Count Multicloud GitOps pattern with OpenShift managed datacenter cluster size The OpenShift cluster is a standard datacenter deployment of 3 control plane nodes and 3 or more worker nodes.
Node Type Number of nodes Cloud Provider Instance Type Control Plane/Worker 3 Google Cloud n1-standard-8 Control Plane/Worker 3 Amazon Cloud Services m5.2xlarge Control Plane/Worker 3 Microsoft Azure Standard_D8s_v3 Managed Datacenter Cluster Utilization GCP
This is a snapshot of a Google Cloud managed data center cluster running the production Multicloud GitOps pattern.
CPU Memory File System Network Pod Count AWS
This is a snapshot of a Amazon Web Services managed data center cluster running the production Multicloud GitOps pattern.
CPU Memory File System Network Pod Count Azure
This is a snapshot of an Azure managed data center cluster running the production Multicloud GitOps pattern.
CPU Memory File System Network Pod Count AWS Instance Types The Multicloud GitOps pattern was tested with the highlighted AWS instances in bold. The OpenShift installer will let you know if the instance type meets the minimum requirements for a cluster.
The message that the openshift installer will give you will be similar to this message
INFO Credentials loaded from default AWS environment variables FATAL failed to fetch Metadata: failed to load asset &#34;Install Config&#34;: [controlPlane.platform.aws.type: Invalid value: &#34;m4.large&#34;: instance type does not meet minimum resource requirements of 4 vCPUs, controlPlane.platform.aws.type: Invalid value: &#34;m4.large&#34;: instance type does not meet minimum resource requirements of 16384 MiB Memory] Below you can find a list of the AWS instance types that can be used to deploy the Multicloud GitOps pattern.
Instance type Default vCPUs Memory (GiB) Datacenter Factory/Edge 3x3 OCP Cluster 3 Node OCP Cluster m4.xlarge 4 16 N N m4.2xlarge 8 32 Y Y m4.4xlarge 16 64 Y Y m4.10xlarge 40 160 Y Y m4.16xlarge 64 256 Y Y m5.xlarge 4 16 Y N m5.2xlarge 8 32 Y Y m5.4xlarge 16 64 Y Y m5.8xlarge 32 128 Y Y m5.12xlarge 48 192 Y Y m5.16xlarge 64 256 Y Y m5.24xlarge 96 384 Y Y The OpenShift cluster is made of 4 Control Plane nodes and 3 Workers for the Datacenter and the Edge/managed data center cluster are made of 3 Control Plane and 3 Worker nodes. For the node sizes we used the m5.xlarge on AWS and this instance type met the minimum requirements to deploy the Multicloud GitOps pattern successfully on the Datacenter hub. On the managed data center cluster we used the m5.xlarge since the minimum cluster was comprised of 3 nodes. .
To understand better what types of nodes you can use on other Cloud Providers we provide some of the details below.
Azure Instance Types The Multicloud GitOps pattern was also deployed on Azure using the Standard_D8s_v3 VM size. Below is a table of different VM sizes available for Azure. Keep in mind that due to limited access to Azure we only used the Standard_D8s_v3 VM size.
The OpenShift cluster is made of 3 Control Plane nodes and 3 Workers for the Datacenter cluster.
The OpenShift cluster is made of 3 Control Plane nodes and 3 or more workers for each of the managed data center clusters.
Type Sizes Description General purpose B, Dsv3, Dv3, Dasv4, Dav4, DSv2, Dv2, Av2, DC, DCv2, Dv4, Dsv4, Ddv4, Ddsv4 Balanced CPU-to-memory ratio. Ideal for testing and development, small to medium databases, and low to medium traffic web servers. Compute optimized F, Fs, Fsv2, FX High CPU-to-memory ratio. Good for medium traffic web servers, network appliances, batch processes, and application servers. Memory optimized Esv3, Ev3, Easv4, Eav4, Ev4, Esv4, Edv4, Edsv4, Mv2, M, DSv2, Dv2 High memory-to-CPU ratio. Great for relational database servers, medium to large caches, and in-memory analytics. Storage optimized Lsv2 High disk throughput and IO ideal for Big Data, SQL, NoSQL databases, data warehousing and large transactional databases. GPU NC, NCv2, NCv3, NCasT4_v3, ND, NDv2, NV, NVv3, NVv4 Specialized virtual machines targeted for heavy graphic rendering and video editing, as well as model training and inferencing (ND) with deep learning. Available with single or multiple GPUs. High performance compute HB, HBv2, HBv3, HC, H Our fastest and most powerful CPU virtual machines with optional high-throughput network interfaces (RDMA). For more information please refer to the Azure VM Size Page.
Google Cloud (GCP) Instance Types The Multicloud GitOps pattern was also deployed on GCP using the n1-standard-8 VM size. Below is a table of different VM sizes available for GCP. Keep in mind that due to limited access to GCP we only used the n1-standard-8 VM size.
The OpenShift cluster is made of 3 Control Plane and 3 Workers for the Datacenter cluster.
The OpenShift cluster is made of 3 Nodes combining Control Plane/Workers for the Edge/managed data center cluster.
The following table provides VM recommendations for different workloads.
| General purpose | Workload optimized
Cost-optimized Balanced Scale-out optimized Memory-optimized Compute-optimized Accelerator-optimized E2 N2, N2D, N1 T2D M2, M1 C2 A2 Day-to-day computing at a lower cost Balanced price/performance across a wide range of VM shapes Best performance/cost for scale-out workloads Ultra high-memory workloads Ultra high performance for compute-intensive workloads Optimized for high performance computing workloads For more information please refer to the GCP VM Size Page.
`,url:"https://validatedpatterns.io/patterns/multicloud-gitops-portworx/cluster-sizing/",breadcrumb:"/patterns/multicloud-gitops-portworx/cluster-sizing/"},"https://validatedpatterns.io/patterns/retail/components/":{title:"Components",tags:[],content:`Component Details The Quarkus Coffeeshop Store Chart This chart is responsible for deploying the applications, services and routes for the Quarkus Coffeeshop demo. It models a set of microservices that would make sense for a coffeeshop retail operation. The detail of what the microservices do is here.
quarkuscoffeeshop-web Serves as the &ldquo;front end&rdquo; for ordering food and drinks.
quarkuscoffeeshop-counter The counter service receives the orders, persists them in the database, and notifies when they are ready.
quarkuscoffeeshop-barista The barista service is responsible for preparing items from the &ldquo;drink&rdquo; side of the menu.
quarkuscoffeeshop-kitchen The kitchen service is responsible for preparing items from the &ldquo;food&rdquo; side of the menu.
quarkuscoffeeshop-customerloyalty The customerloyalty service is responsible for generating customer loyalty events, when a customer enters the &ldquo;rewards&rdquo; email. This data is not persisted or tracked anywhere.
quarkuscoffeeshop-inventory The inventory service is responsible for tracking food and drink inventory.
quarkuscoffeeshop-customermocker The customermocker can be used to generate test traffic.
quarkuscoffeeshop-majestic-monolith The &ldquo;majestic monolith&rdquo; builds all the apps into a single bundle, to simplify the process of deploying this app on single node systems.
All the components look like this in ArgoCD when deployed:
The chart is designed such that the same chart can be deployed in the hub cluster as the &ldquo;production&rdquo; store, the &ldquo;demo&rdquo; or TEST store, and on a remote cluster.
The Quarkus Coffeeshop Database Chart This installs a database instance suitable for use in the Retail pattern. It uses the Crunchy PostgreSQL Operator to provide PostgreSQL services, which includes high availability and backup services by default, and other features available.
Like the store chart, the Database chart can be deployed in the same different scenarios.
In ArgoCD, it looks like this:
The Quarkus Coffeeshop Kafka Chart This chart installs Kafka for use in the Retail pattern. It uses the Red Hat AMQ Streams operator.
The Quarkus Coffeeshop Pipelines Chart The pipelines chart defines build pipelines using the Red Hat OpenShift Pipelines Operator (tektoncd). Pipelines are provided for all of the application images that ship with the pattern; the pipelines all build the app from source, deploy them to the &ldquo;demo&rdquo; namespace, and push them to the configured image registry.
Like the store and database charts, the kafka chart supports all three modes of deployment.
The Quarkus Coffeeshop Landing Page Chart The Landing Page chart builds the page that presents the links for the demos in the pattern.
`,url:"https://validatedpatterns.io/patterns/retail/components/",breadcrumb:"/patterns/retail/components/"},"https://validatedpatterns.io/contribute/extending-a-pattern/":{title:"Extending an existing pattern",tags:[],content:`Extending an existing pattern Introduction to extending a pattern using a fork Extending an existing pattern usually means adding a new product and/or configuration to the existing pattern. This usually requires four steps:
Adding any required namespace for the product Adding a subscription to install and operator Adding one or more ArgoCD applications to manage the post-install configuration of the product Adding the Helm chart needed to implement the post-install configuration identified in step 3. Sometimes there is no operator in operator hub for the product and it requires installing the product using a Helm chart.
This additions need to be made to the appropriate values-&lt;cluster grouping&gt;.yaml file in the top level pattern directory. If the component is on a hub cluster that would be values-hub.yaml. If it&rsquo;s on a production cluster that would be in values-production.yaml. Look at the pattern architecture and decide where you need to add the product.
In the example below AMQ Streams (Kafka) is chosen as a product to add to a pattern.
Before starting, fork and clone first Visit the github page for the pattern that you wish to extend. E.g. multicloud-gitops. Select “Fork” in the top right corner.
On the create a new fork page. You can choose what owner repository you want and the name of the fork. Most times you will fork into your personal repo and leave the name the same. When you have made the appropriate changes press the &ldquo;Create fork&rdquo; button.
You will need to clone from the new fork onto you laptop/desktop so that you can do the extension work effectively. So on the new fork’s main page elect the green “Code” button and copy the git repo’s ssh address.
In an appropriate directory on your laptop (e.g. ~/git) use git clone on the command line using the ssh address copied above. Then create a branch to extend the pattern. For example if you are extending the multicloud-gitops pattern and adding kafka, you will need to clone your fork of multicloud-gitops and create a branch to add Kafka:
~/git&gt; git clone git@github.com:&lt;your git account&gt;/multicloud-gitops.git ~/git&gt; cd multicloud-gitops ~/git/multicloud-gitops&gt; git checkout -b add-kafka Adding a namespace The first step is to add a namespace in the values-&lt;cluster group&gt;.yaml. Sometimes a specific namespace is expected in other parts of a products configuration. E.g. Red Hat Advanced Cluster Security expects to use the namespace stackrox. While you might try using a different namespace you may encounter issues.
In our example we are just going to add the namespace my-kafka.
--- namespaces: ... # other namespaces above my-kafka - my-kafka Adding a subscription The next step is to add the subscription information for the Kubernetes Operator. Sometimes this subscription needs to be added to a specific namespace, e.g. openshift-operators. Check for any operator namespace requirements. In this example just place it in the newly created my-kafka namespace.
--- subscriptions: ... # other subscriptions amq-streams: name: amq-streams namespace: my-kafka Adding the ArgoCd application The next step is to add the application information. Sometimes you want to group applications in ArgoCD into a project and you can do this by using an existing project grouping or create a new project group. The example below uses an existing project called my-app.
--- applications: kafka: name: kafka namespace: my-kafka project: my-app path: charts/all/kafka Adding the Helm Chart The path: tag in the above kafka application tells ArgoCD where to find the Helm Chart needed to deploy this application. Paths are relative the the top level pattern directory and therefore in my example that is ~/git/multicloud-gitops.
ArgoCD will continuously monitor for changes to artifacts in that location for updates to apply. Each different site type would have its own values- file listing subscriptions and applications.
Helm Charts The previous steps merely instruct ArgoCD to install the operator for AMQ Streams. No Kafka cluster or topics are created. There is more work to be done.
You must add a Chart for Kafka:
A Kafka cluster chart A Kafka topic chart. Because Kafka (AMQ Streams) is often used to communicate across different clusters in multi-cluster and/or multi-cloud environment you are going to add these to the the all sub dir charts/all/kafka/templates directory. In order to do that we must:
~/git/multicloud-gitops&gt; mkdir charts/all/kafka ~/git/multicloud-gitops&gt; mkdir charts/all/kafka/templates Helm requires a Chart.yaml file and a values.yaml file in the kafka directory. Edit these files in the kafka directory and add the following:
--- Chart.yaml: apiVersion: v2 name: kafka-cluster description: A Helm chart for Kubernetes # A chart can be either an &#39;application&#39; or a &#39;library&#39; chart. # # Application charts are a collection of templates that can be packaged into versioned archives # to be deployed. # # Library charts provide useful utilities or functions for the chart developer. They&#39;re included as # a dependency of application charts to inject those utilities and functions into the rendering # pipeline. Library charts do not define any templates and therefore cannot be deployed. type: application # This is the chart version. This version number should be incremented each time you make changes # to the chart and its templates, including the app version. # Versions are expected to follow Semantic Versioning (https://semver.org/) version: 0.1.0 # This is the version number of the application being deployed. This version number should be # incremented each time you make changes to the application. Versions are not expected to # follow Semantic Versioning. They should reflect the version the application is using. # It is recommended to use it with quotes. appVersion: &#34;1.16.0&#34; values.yaml:
--- global: testlab: namespace: lab-kafka Save the files. Having the global.testlab.namespace defined here allows us to override its chart from here or from values-global.yaml.
The Kafka cluster Helm chart Now we need a chart to deploy a kafka cluster instance. We will create a file called kafka-cluster.yaml in the charts/all/kafka/templates directory. Using your favorite editor edit the file, copy/paste the code below, and save the file.
kafka-cluster.yaml:
--- apiVersion: kafka.strimzi.io/v1beta2 kind: Kafka metadata: name: lab-cluster namespace: {{ .Values.global.testlab.namespace }} # annotations: # argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true # # NOTE if needed you can use argocd sync-wave to delay a manifest # argocd.argoproj.io/sync-wave: &#34;3&#34; spec: entityOperator: topicOperator: {} userOperator: {} kafka: config: default.replication.factor: 3 inter.broker.protocol.version: &#39;3.3&#39; min.insync.replicas: 2 offsets.topic.replication.factor: 3 transaction.state.log.min.isr: 2 transaction.state.log.replication.factor: 3 listeners: - name: plain port: 9092 tls: true type: route - name: tls port: 9093 tls: true type: route configuration: bootstrap: host: bootstrap-factory-kafka-cluster.{{ .Values.global.localClusterDomain }} replicas: 3 storage: type: ephemeral version: 3.3.1 zookeeper: replicas: 3 storage: type: ephemeral Topic Helm Chart We also need a chart to deploy a kafka stream. We will create a file called kafka-topic.yaml in the charts/all/kafka/templates directory. Using your favorite editor edit the file, copy/paste the code below, and save the file.
kafka-topic.yaml:
--- apiVersion: kafka.strimzi.io/v1beta2 kind: KafkaTopic metadata: name: lab-stream namespace: {{ .Values.global.testlab.namespace }} labels: strimzi.io/cluster: lab-cluster spec: partitions: 1 replicas: 1 config: retention.ms: 604800000 segment.bytes: 1073741824 Add, Commit &amp; Push Steps:
Use git status to see what&rsquo;s changed that you need to add to your commit and add them using git add Commit the changes to the branch Push the branch to your fork. ~/git/multicloud-gitops&gt; git status ~/git/multicloud-gitops&gt; git add &lt;the assets created/changed&gt; ~/git/multicloud-gitops&gt; git commit -m “Added Kafka using AMQ Stream operator and Helm charts” ~/git/multicloud-gitops&gt; git push origin multicloud-gitops Watch OpenShift GitOps hub cluster UI and see Kafka get deployed Let’s check the OpenShift console. This can take a bit of time for ArgoCD to pick it up and deploy the assets.
Select installed operators. Is AMQ Streams Operator deployed? Select the Red Hat Integration - AMQ Streams operator. Select Kafka tab. Is there a new lab-cluster created? Select the Kafka Topic tab. Is there a lab-streams topic created? This is a very simple and minimal Kafka set up. It is likely you will need to add more manifests to the Chart but it is a good start.
`,url:"https://validatedpatterns.io/contribute/extending-a-pattern/",breadcrumb:"/contribute/extending-a-pattern/"},"https://validatedpatterns.io/learn/infrastructure/":{title:"Infrastructure",tags:[],content:`Infrastructure Background Each validated pattern has infrastructure requirements. The majority of the validated patterns will run Red Hat OpenShift while some parts will run directly on Red Hat Enterprise Linux or (RHEL), more likely, a version of RHEL called RHEL for Edge. It is expected that consumers of validated patterns already have the infrastructure in place using existing reliable and supported deployment tools. For more information and tools head over to console.redhat.com
Sizing In this section we provide general minimum sizing requirements for such infrastructure but it is important to review specific requirements for a specific validated pattern. For example, Industrial Edge 2.0 employs AI/Ml technology that requires large machine instances to support the applications deployed on OpenShift at the datacenter.
`,url:"https://validatedpatterns.io/learn/infrastructure/",breadcrumb:"/learn/infrastructure/"},"https://validatedpatterns.io/patterns/devsecops/production-cluster/":{title:"Secured Production Clusters",tags:[],content:`Having a production cluster join the hub Introduction Production clusters need to be secured and so one part of the deployment is to install the Advanced Cluster Security operator with a secured configuration. This allows ACS central to monitor and report on security issues on the cluster. ACS secured sites report to an ACS Central application that is deployed on the hub.
Allow ACM to deploy the production application to a subset of secured clusters By default the production applications are deployed on all prod clusters that ACM knows about.
- name: secured helmOverrides: - name: clusterGroup.isHubCluster value: &#34;false&#34; clusterSelector: matchLabels: clusterGroup: prod matchExpressions: - key: vendor operator: In values: - OpenShift Remember to commit the changes and push to GitHub so that GitOps can see your changes and apply them.
Deploy a Production (prod) cluster For instructions on how to prepare and import a production (prod) cluster please read the section importing a cluster. Use clusterGroup=prod.
You are done importing the production cluster That&rsquo;s it! Go to your production OpenShift console and check for the open-cluster-management-agent pod being launched. Be patient, it will take a while for the ACM agent and agent-addons to launch. After that, the operator OpenShift GitOps will run. When it&rsquo;s finished coming up launch the OpenShift GitOps (ArgoCD) console from the top right of the OpenShift console.
Next up Work your way through the Multicluster DevSecOps GitOps/DevOps demos (TBD)
`,url:"https://validatedpatterns.io/patterns/devsecops/production-cluster/",breadcrumb:"/patterns/devsecops/production-cluster/"},"https://validatedpatterns.io/learn/ocp-cluster-general-sizing/":{title:"OpenShift General Sizing",tags:[],content:`OpenShift General Sizing Recommended node host practices The OpenShift Container Platform node configuration file contains important options. For example, two parameters control the maximum number of pods that can be scheduled to a node: podsPerCore and maxPods.
When both options are in use, the lower of the two values limits the number of pods on a node. Exceeding these values can result in:
Increased CPU utilization. Slow pod scheduling. Potential out-of-memory scenarios, depending on the amount of memory in the node. Exhausting the pool of IP addresses. Resource overcommitting, leading to poor user application performance. In Kubernetes, a pod that is holding a single container actually uses two containers. The second container is used to set up networking prior to the actual container starting. Therefore, a system running 10 pods will actually have 20 containers running.
podsPerCore sets the number of pods the node can run based on the number of processor cores on the node. For example, if podsPerCore is set to 10 on a node with 4 processor cores, the maximum number of pods allowed on the node will be 40.
kubeletConfig: podsPerCore: 10 Setting podsPerCore to 0 disables this limit. The default is 0. podsPerCore cannot exceed maxPods.
maxPods sets the number of pods the node can run to a fixed value, regardless of the properties of the node.
kubeletConfig: maxPods: 250 For more information about sizing and Red Hat standard host practices see the Official OpenShift Documentation Page for recommended host practices.
Control plane node sizing The control plane node resource requirements depend on the number of nodes in the cluster. The following control plane node size recommendations are based on the results of control plane density focused testing. The control plane tests create the following objects across the cluster in each of the namespaces depending on the node counts:
12 image streams 3 build configurations 6 builds 1 deployment with 2 pod replicas mounting two secrets each 2 deployments with 1 pod replica mounting two secrets 3 services pointing to the previous deployments 3 routes pointing to the previous deployments 10 secrets, 2 of which are mounted by the previous deployments 10 config maps, 2 of which are mounted by the previous deployments Number of worker nodes Cluster load (namespaces) CPU cores Memory (GB) 25 500 4 16 100 1000 8 32 250 4000 16 96 On a cluster with three masters or control plane nodes, the CPU and memory usage will spike up when one of the nodes is stopped, rebooted or fails because the remaining two nodes must handle the load in order to be highly available. This is also expected during upgrades because the masters are cordoned, drained, and rebooted serially to apply the operating system updates, as well as the control plane Operators update. To avoid cascading failures on large and dense clusters, keep the overall resource usage on the master nodes to at most half of all available capacity to handle the resource usage spikes. Increase the CPU and memory on the master nodes accordingly.
The node sizing varies depending on the number of nodes and object counts in the cluster. It also depends on whether the objects are actively being created on the cluster. During object creation, the control plane is more active in terms of resource usage compared to when the objects are in the running phase.
If you used an installer-provisioned infrastructure installation method, you cannot modify the control plane node size in a running OpenShift Container Platform 4.5 cluster. Instead, you must estimate your total node count and use the suggested control plane node size during installation.
The recommendations are based on the data points captured on OpenShift Container Platform clusters with OpenShiftSDN as the network plug-in.
In OpenShift Container Platform 4.5, half of a CPU core (500 millicore) is now reserved by the system by default compared to OpenShift Container Platform 3.11 and previous versions. The sizes are determined taking that into consideration.
For more information about sizing and Red Hat standard host practices see the Official OpenShift Documentation Page for recommended host practices.
Recommended etcd practices For large and dense clusters, etcd can suffer from poor performance if the keyspace grows excessively large and exceeds the space quota. Periodic maintenance of etcd, including defragmentation, must be performed to free up space in the data store. It is highly recommended that you monitor Prometheus for etcd metrics and defragment it when required before etcd raises a cluster-wide alarm that puts the cluster into a maintenance mode, which only accepts key reads and deletes. Some of the key metrics to monitor are etcd_server_quota_backend_bytes which is the current quota limit, etcd_mvcc_db_total_size_in_use_in_bytes which indicates the actual database usage after a history compaction, and etcd_debugging_mvcc_db_total_size_in_bytes which shows the database size including free space waiting for defragmentation. Instructions on defragging etcd can be found in the Defragmenting etcd data section.
Etcd writes data to disk, so its performance strongly depends on disk performance. Etcd persists proposals on disk. Slow disks and disk activity from other processes might cause long fsync latencies, causing etcd to miss heartbeats, inability to commit new proposals to the disk on time, which can cause request timeouts and temporary leader loss. It is highly recommended to run etcd on machines backed by SSD/NVMe disks with low latency and high throughput.
Some of the key metrics to monitor on a deployed OpenShift Container Platform cluster are p99 of etcd disk write ahead log duration and the number of etcd leader changes. Use Prometheus to track these metrics. etcd_disk_wal_fsync_duration_seconds_bucket reports the etcd disk fsync duration, etcd_server_leader_changes_seen_total reports the leader changes. To rule out a slow disk and confirm that the disk is reasonably fast, 99th percentile of the etcd_disk_wal_fsync_duration_seconds_bucket should be less than 10ms.
For more information about sizing and Red Hat standard host practices see the Official OpenShift Documentation Page for recommended host practices.
`,url:"https://validatedpatterns.io/learn/ocp-cluster-general-sizing/",breadcrumb:"/learn/ocp-cluster-general-sizing/"},"https://validatedpatterns.io/learn/rhel-for-edge-general-sizing/":{title:"RHEL for Edge General Sizing",tags:[],content:`RHEL for Edge General Sizing Recommended node host practices TBD
`,url:"https://validatedpatterns.io/learn/rhel-for-edge-general-sizing/",breadcrumb:"/learn/rhel-for-edge-general-sizing/"},"https://validatedpatterns.io/patterns/ansible-edge-gitops/ansible-automation-platform/":{title:"Ansible Automation Platform",tags:[],content:`Ansible Automation Platform How it&rsquo;s installed See the installation details here.
How to Log In The default login user is admin and the password is generated randomly at install time; you will need the password to login in to the AAP interface. You do not have to log in to the interface - the pattern will configure the AAP instance; the pattern retrieves the password using the same technique as the ansible_get_credentials.sh script described below. If you want to inspect the AAP instance, or change any aspects of its configuration, there are two ways to login and look at it. Both mechanisms are equivalent; you get the same password to the same instance using either technique.
Via the OpenShift Console In the OpenShift console, navigate to Workloads &gt; Secrets and select the &ldquo;ansible-automation-platform&rdquo; project if you want to limit the number of Secrets you can see.
The Secret you are looking for is in the ansible-automation-platform project and is named controller-admin-password. If you click on it, you can see the Data.password field. It is shown revealed below to show that it is the same as what is shown by the script method of retrieving it below:
Via ansible_get_credentials.sh With your KUBECONFIG set, you can run ./scripts/ansible-get-credentials.sh from your top-level pattern directory. This will use your OpenShift cluster admin credentials to retrieve the URL for your Ansible Automation Platform instance, as well as the password for its admin user, which is auto-generated by the AAP operator by default. The output of the command looks like this (your password will be different):
./scripts/ansible_get_credentials.sh [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match &#39;all&#39; PLAY [Install manifest on AAP controller] ****************************************************************************** TASK [Retrieve API hostname for AAP] *********************************************************************************** ok: [localhost] TASK [Set ansible_host] ************************************************************************************************ ok: [localhost] TASK [Retrieve admin password for AAP] ********************************************************************************* ok: [localhost] TASK [Set admin_password fact] ***************************************************************************************** ok: [localhost] TASK [Report AAP Endpoint] ********************************************************************************************* ok: [localhost] =&gt; { &#34;msg&#34;: &#34;AAP Endpoint: https://controller-ansible-automation-platform.apps.mhjacks-aeg.blueprints.rhecoeng.com&#34; } TASK [Report AAP User] ************************************************************************************************* ok: [localhost] =&gt; { &#34;msg&#34;: &#34;AAP Admin User: admin&#34; } TASK [Report AAP Admin Password] *************************************************************************************** ok: [localhost] =&gt; { &#34;msg&#34;: &#34;AAP Admin Password: CKollUjlir0EfrQuRrKuOJRLSQhi4a9E&#34; } PLAY RECAP ************************************************************************************************************* localhost : ok=7 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Pattern AAP Configuration Details In this section, we describe the details of the AAP configuration we apply as part of installing the pattern. All of the configuration discussed in this section is applied by the ansible_load_controller.sh script.
Loading a Manifest After validating that AAP is ready to be configured, the first thing the script does is to install the manifest you specify in the values-secret.yaml file in the files.manifest setting. The value of this setting is expected to be a fully-pathed file that represents a Red Hat Satellite manifest file with a valid entitlement for AAP. The only thing this manifest is used for is entitling AAP.
Instructions for creating a suitable manifest file can be found here.
While it is absolutely possible to entitle AAP via a username/password on first login, the automated mechanisms for entitling only support manifests, that is the technique the pattern uses.
Organizations The pattern installs an Organization called HMI Demo is installed. This makes it a bit easier to separate what the pattern is doing versus the default configuration of AAP. The other resources created in AAP as part of the load process are associated with this Organization.
Credential Types (and their Credentials) Kubeconfig (Kubeconfig) The Kubeconfig credential is for holding the OpenShift cluster admin kubeconfig file. This is used to query the edge-gitops-vms namespace for running VM instances. Since the kubeconfig is necessary for installing the pattern and must be available when the load script is running, the load script pulls it into an AAP secret and stores it for later use (and calls it Kubeconfig).
The template for creating the Credential Type was taken from here.
RHSMcredential (rhsm_credential) This credential is required to register the RHEL VMs and configure them for Kiosk mode. The registration process allows them to install packages from the Red Hat Content Delivery Network.
Machine (kiosk-private-key) This is a standard AAP Machine type credential. kiosk-private-key is created with the username and private key from your values-secret.yaml file in the kiosk-ssh.username and kiosk-ssh.privatekey fields.
KioskExtraParams (kiosk_container_extra_params) This CredentialType is considered &ldquo;secret&rdquo; because it includes the admin login password for the Ignition application. This passed to the provisioning playbook(s) as extra_vars.
Inventory The pattern installs an Inventory (HMI Demo), but no inventory sources. This is due to the way that OpenShift Virtualization provides access to virtual machines. The IP address associated with the SSH service that a given VM is running is associated with the Service object on the VM. This is not the way the Kubernetes inventory plugin expects to work. So to make inventory dynamic, we are instead using a play to discover VMs and add them to inventory &ldquo;on the fly&rdquo;. What is unusual about DNS inside a Kubernetes cluster is that resources outside the namespace must use the cluster FQDN - which is resource-name.resource-namespace.svc.
It is also possible to define a static inventory - an example of how this would like is preserved in the pattern repository as hosts.
A standard dynamic inventory script is available here. This will retrieve the object names, but it will not (currently) map the FQDN properly. Because of this limitation, we moved to using the inventory pre-play method.
Templates (key playbooks in the pattern) Dynamic Provision Kiosk Playbook This combines all three key workflows in this pattern:
Dynamic inventory (inventory preplay) Kiosk Mode Podman Playbook It is safe to run multiple times on the same system. It is run on a schedule, every 10 minutes, to demonstrate this.
Kiosk Mode Playbook This playbook runs the kiosk_mode role.
Podman Playbook This playbook runs the container_lifecycle role with overrides suitable for the Ignition application container.
Ping Playbook This playbook is for testing basic connectivity - making sure that you can reach the nodes you wish to manage, and that the credentials you have given will work on them. It will not change anything on the VMs - just gather facts from them (which requires elevating to root).
Schedules Update Project AEG GitOps This job runs every 5 minutes to update the GitOps repository associated with the project. This is necessary when any of the Ansible code (for example, the playbooks or roles associated with the pattern) changes, so that the new code is available to the AAP instance.
Dynamic Provision Kiosk Playbook This job runs every 10 minutes to provision and configure any kiosks it finds to run the Ignition application in a podman container, and configure firefox in kiosk mode to display that application. The playbook is designed to be idempotent, so it is safe to run multiple times on the same targets; it will not make user-visible changes to those targets unless it must.
This playbook combines the inventory_preplay and the Provision Kiosk Playbook.
Execution Environment The pattern includes an execution environment definition that can be found here.
The execution environment includes some additional collections beyond what is provided in the Default execution environment, including:
fedora.linux_system_roles containers.podman community.okd The execution environment definition is provided if you want to customize or change it; if so, you should also change the Execution Environment attributes of the Templates (in the load script, those attributes are set by the variables aap_execution_environment and aap_execution_environment_image).
Roles included in the pattern kiosk_mode This role is responsible does the following:
RHEL node registration Installation of GUI packages Installation of Firefox Configuration of Firefox kiosk mode container_lifecycle This role is responsible for:
Downloading and running a podman image on the system (and configure it to auto-update) Setting the container up to run at boot time Passing any other runtime arguments to the container. In this container&rsquo;s case, that includes specifying an admin password override. Extra Playbooks in the Pattern inventory_preplay.yml This playbook is designed to be included in other plays; its purpose is to discover the desired inventory and add those hosts to inventory at runtime. It uses a kubernetes query via the cluster-admin kube config file.
Provision Kiosk Playbook This does the work of provisioning the kiosk, which configures kiosk mode, and also installs Ignition and configures it to start at boot. It runs the kiosk_mode and container_lifecycle roles.
Next Steps Help &amp; Feedback Report Bugs `,url:"https://validatedpatterns.io/patterns/ansible-edge-gitops/ansible-automation-platform/",breadcrumb:"/patterns/ansible-edge-gitops/ansible-automation-platform/"},"https://validatedpatterns.io/contribute/creating-a-pattern/":{title:"Creating a pattern",tags:[],content:`Creating a pattern The validated patterns community has relied on existing architectures that have been successfully deployed in an enterprise. The architecture itself is a best practice in assembling technologies and projects to provide a working solution. How that solution is deployed and managed is a different matter. It may have evolved over time and may have grown in its deployment such that ongoing maintenance is not sustainable.
The validated patterns framework is much more of a best practice of structuring the various configuration assets and integrating with GitOps and DevOps tools.
Therefore the question really is: how do I move my successful architecture solution into a sustainable GitOps/DevOps framework? And that is what we are going to do in this section.
So how do you take a current application workload and move it to the Validated Pattern framework? One of the first things that you should do is look at your current implementation of your workload and identify the kubernetes manifests that are involved in order to run the workloads.
Prerequisites Please make sure you have read the background section, including the structure section.
You&rsquo;re probably not starting from scratch The validated patterns community has relied on existing architectures that have been successfully deployed in an enterprise. The architecture itself is a best practice in assembling technologies and projects to provide a working solution. How that solution is deployed and managed is a different matter. It may have evolved over time and may have grown in its deployment such that ongoing maintenance is not sustainable.
The validated patterns framework is much more of a best practice of structuring the various configuration assets and integrating with GitOps and DevOps tools.
Therefore the question really is: How do I move my successful architecture solution into a sustainable GitOps/DevOps framework? And that is what we are going to do in this section.
Requirements for creating a new pattern The patterns framework requires some artifacts like OpenShift GitOps (ArgoCD) in order to provide the GitOps automation. All existing patterns use OpenShift GitOps as a starting point. The multicloud-gitops pattern is the most fundamental of patterns and therefore it is recommended to use it as a base pattern. I.e Create a new pattern based on it. Create a new branch on your new pattern to perform the initial changes. Deploy the initial new pattern pattern to the cluster. Moving to the validated patterns framework One of the first things that you should do is look at your current implementation of your workload and identify the Kubernetes manifests that are involved in order to run the workloads.
When and how to use values- files There are 4 values files that make up any Validated Pattern. The values files are:
values-.yaml (e.g. values-datacenter.yaml) values-.yaml (e.g. values-edge-site.yaml or values-factory.yaml) values-global.yaml values-secrets.yaml Operators into framework We begin our journey by identifying what application services are needed to run the workload. The Cloud Native Operator framework provides a way of managing the lifecycle of application services that are needed by the application workload. The validated pattern framework gives you a way to describe these Operators in a values file that is specific to your pattern and the site type.
So for example if we wish deploy Advanced Cluster Management, AMQ (messaging) and AMQ Streams (Kafka) in our datacenter, we would make the following subscription entries in our values-datacenter.yaml file:
namespaces: - open-cluster-management - my-application - backend-storage subscriptions: - name: advanced-cluster-management namespace: open-cluster-management channel: release-2.3 csv: advanced-cluster-management.v2.3.2 - name: amq-streams namespace: my-application channel: amq-streams-1.7.x csv: amqstreams.v1.7.1 - name: amq-broker namespace: my-application channel: 7.8.x csv: amq-broker-operator.v7.8.1-opr-3 This tells the framework which Operators are needed and what namespace they should be deployed in.
Grouping applications for OpenShift GitOps In the same values- file we need to inform OpenShift GitOps (ArgoCD) what applications to deploy and where the Helm Charts are so that they can be applied to the deployment and watched for future changes.
When using GitOps, specifically OpenShift GitOps (ArgoCD), it makes sense to break up applications into different areas of concern, i.e. projects. For example, the main applications for the datacenter might be grouped separately from some storage components:
projects: - datacenter - storage applications: - name: acm namespace: open-cluster-management project: datacenter path: common/acm ignoreDifferences: - group: internal.open-cluster-management.io kind: ManagedClusterInfo jsonPointers: - /spec/loggingCA - name: central-kafka namespace: backend-storage project: storage path: charts/datacenter/kafka ignoreDifferences: - group: apps kind: Deployment jsonPointers: - /spec/replicas - group: route.openshift.io kind: Route jsonPointers: - /status - group: image.openshift.io kind: ImageStream jsonPointers: - /spec/tags - group: apps.openshift.io kind: DeploymentConfig jsonPointers: - /spec/template/spec/containers/0/image - name: cool-app namespace: my-application project: datacenter path: charts/datacenter/my-app plugin: name: helm-with-kustomize In the above example acm (ACM) is part of the main datacenter deployment, as is cool-app. However, central-kafka is part of backend-storage.
The path: tag tells OpenShift GitOps where to find the Helm charts needed to deploy this application (refer back to the charts directory description for more details). OpenShift GitOps will continuously monitor for changes to artifacts in that location for updates to apply.
Each different site type would have its own values- file listing subscriptions and applications.
Kustomize to framework Kustomize can still be used within the framework but it will be driven by Helm. If you have a lot of kustomization.yaml, you may not need to refactor all of it. However, you will need a Helm chart to drive it and you will need to check for names and paths etc. that you may need to parameterize using the Helm templates capabilities.
For example, the original Argo CD subscription YAML from one of the patterns looked like this:
apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: argocd-operator namespace: argocd spec: channel: alpha installPlanApproval: Manual name: argocd-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: argocd-operator.v0.0.11 While we could have continued to use the ArgoCD community operator, we instead transitioned to using OpenShift GitOps, the Red Hat supported product. But this static subscription would not allow updates for continuous integration of new versions. And you&rsquo;ll remember from the Operators section above that we specify channel names as part of the subscription of operators. So we can instead using something like this (understanding the move to openshift-gitops-operator instead of ArgoCD).
apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: openshift-gitops-operator namespace: openshift-operators labels: operators.coreos.com/openshift-gitops-operator.openshift-operators: &#34;&#34; spec: channel: {{ .Values.main.gitops.channel }} installPlanApproval: {{ .Values.main.options.installPlanApproval }} name: openshift-gitops-operator source: redhat-operators sourceNamespace: openshift-marketplace {{- if .Values.main.options.useCSV }} startingCSV: openshift-gitops-operator.{{ .Values.main.gitops.csv }} {{- end }} Size matters If things are taking a long time to deploy, use the OpenShift console to check on memory and other potential capacity issues with the cluster. If running in a cloud you may wish to up the machine size. Check the sizing charts.
`,url:"https://validatedpatterns.io/contribute/creating-a-pattern/",breadcrumb:"/contribute/creating-a-pattern/"},"https://validatedpatterns.io/patterns/multicloud-gitops/mcg-imperative-actions/":{title:"Imperative actions",tags:[],content:` Using kubernetes cronjobs to apply imperative actions There is currently no way within Argo CD to apply an imperative action against a cluster. However, you can declaratively apply changes to a cluster using kubernetes cronjob resources. Customers can use their Ansible playbooks to take action against a cluster if necessary.
The authors of the Ansible playbooks and roles must ensure that the playbooks are idempotent because they get applied through cronjobs which are set to run every 10 minutes.
Adding your playbooks to the pattern requires the following:
Move your Ansible configurations to the appropriate directory under Ansible in your forked repository.
Define your job as a list, for example:
imperative: # NOTE: We *must* use lists and not hashes. As hashes lose ordering once parsed by helm # The default schedule is every 10 minutes: imperative.schedule # Total timeout of all jobs is 1h: imperative.activeDeadlineSeconds # imagePullPolicy is set to always: imperative.imagePullPolicy # For additional overrides that apply to the jobs, please refer to the README located in # the top level of this repository. jobs: - name: regional-ca # Ansible playbook to be run playbook: ansible/playbooks/on-hub-get-regional-ca.yml # per playbook timeout in seconds timeout: 234 # verbosity: &#34;-v&#34; Additional job customizations The pattern includes override parameters to provide further customization. Refer to the list below for the available overrides.
imperative: jobs: [] # This image contains Ansible + kubernetes.core by default and is used to run the jobs image: registry.redhat.io/ansible-automation-platform-22/ee-supported-rhel8:latest namespace: &#34;imperative&#34; # configmap name in the namespace that will contain all helm values valuesConfigMap: &#34;helm-values-configmap&#34; cronJobName: &#34;imperative-cronjob&#34; jobName: &#34;imperative-job&#34; imagePullPolicy: Always # This is the maximum timeout of all the jobs (1h) activeDeadlineSeconds: 3600 # By default we run this every 10minutes schedule: &#34;*/10 * * * *&#34; # Schedule used to trigger the vault unsealing (if explicitely enabled) # Set to run every 9 minutes in order for load-secrets to succeed within # a reasonable amount of time (it waits up to 15 mins) insecureUnsealVaultInsideClusterSchedule: &#34;*/9 * * * *&#34; # Increase Ansible verbosity with &#39;-v&#39; or &#39;-vv..&#39; verbosity: &#34;&#34; serviceAccountCreate: true # service account to be used to run the cron pods serviceAccountName: imperative-sa clusterRoleName: imperative-cluster-role clusterRoleYaml: &#34;&#34; roleName: imperative-role roleYaml: &#34;&#34; `,url:"https://validatedpatterns.io/patterns/multicloud-gitops/mcg-imperative-actions/",breadcrumb:"/patterns/multicloud-gitops/mcg-imperative-actions/"},"https://validatedpatterns.io/patterns/devsecops/secure-supply-chain-demo/":{title:"Multicluster DevSecOps Demo",tags:[],content:`Demonstrating Multicluster DevSecOps Background Up until now the Multicluster DevSecOps validated pattern has focused primarily on successfully deploying the architectural pattern components on the three different clusters. Now it is time to see DevSecOps in action as we step through a number of pipeline demonstrations to see the secure supply chain in action.
Prerequisite preparation Make sure to have hub, development and production environments setup. It is possible to set up a production environment on your hub cluster if you wish to use only two clusters.
Local laptop/workstation Make sure you have git and OpenShift&rsquo;s oc command-line clients.
OpenShift Cluster Make sure you have the kubeadmin administrator login for the data center cluster. Use this or the kubeconfig (export the path) to provide administrator access to your data center cluster. It is not required that you have access to the edge (factory) clusters. GitOps and DevOps will take care of the edge clusters.
GitHub account You will need to login into GitHub and be able to fork two repositories.
hybrid-cloud-patterns/multicluster-devsecops hybrid-cloud-patterns/chat-client Pipeline Demos Pipeline 1: Build &amp; Deploy Running this pipeline shows how a build and deploy can run easily and push an image to production and everything looks fine. There are many development environments that run in this &ldquo;trusted&rdquo; mode today.
However the built image is never scanned. And in the next pipeline we see that the same image that was deployed in Pipeline 1 is actually not secure and should never have been deployed.
TBD - screen shots
Pipeline 2: Build &amp; Scan with Failure Pipeline 2 is the same as Pipeline 1 except that the image is scanned and found to fail the scan. The image is NOT pushed to a registry and therefore not deployed to production.
TBD - screen shots
Pipeline 3: Build &amp; Scan with Success Pipeline 3 builds an image that successfully scans without issue. This shows the steps to add a scan task to the pipeline.
TBD - screen shots
Pipeline 4: Build, Scan, Sign and Push to Prod Pipeline 4 demonstrates a more complete pipeline that builds, scans and also signs the image before pushing.
Pipeline 4 is the preferred DevSecOps approach and can be modified to include more security based tasks. E.g. when using a base image for a build, the signature of that image can be checked before the build step even starts.
TBD - screen shots
`,url:"https://validatedpatterns.io/patterns/devsecops/secure-supply-chain-demo/",breadcrumb:"/patterns/devsecops/secure-supply-chain-demo/"},"https://validatedpatterns.io/patterns/industrial-edge/troubleshooting/":{title:"Troubleshooting",tags:[],content:`Troubleshooting Our Issue Tracker Installation-phase Failures The framework for deploying the applications and their operators has been made easy for the user by using OpenShift GitOps for continuous deployment (Argo CD). It takes time to deploy everything. You may have to go back and forth between the OpenShift cluster console and the OpenShift GitOps console to check on applications and operators being up and in a ready state.
The applications deployment for the main data center are as follows. First OpenShift GitOps operator will deploy. See the OpenShift Console to see that it is running. Then OpenShift GitOps takes over the rest of the deployment. It deploys the following applications
Advanced Cluster Management operator in the application acm. this will manage the edge clusters Open Data Hub in the application odh for the data science components. OpenShift Pipelines is deployed in the application pipelines AMQ Streams is deployed to manage data coming from factories and stored in a data lake. The data lake uses S3 based storage and is deployed in the central-s3 application Testing at the data center is managed by the manuela-test application Make sure that all these applications are Healthy 💚 and Synced ✅ in the OpenShift GitOps console. If in a state other than Healthy (Progressing, Degraded, Missing, Unknown') then it&rsquo;s time to dive deeper into that application and see what has happened.
The applications deployed on the factory (edge) cluster are as follows. After a successful importing [1] a factory cluster to the main ACM hub, you should check in the factory cluster&rsquo;s OpenShift UI to see if the projects open-cluster-manager-agent and open-cluster-manager-agent-addons are running. When these are deployed then OpenShift GitOps operator will be deployed on the cluster. From there OpenShift GitOps deploys the following applications:
datalake application sets streams to the data center. stormshift sets up application and AMQ integration components odh sets up the AI/ML models that have been developed by the data scientists. [1] ACM has different ways of describing this process based on which tool you are using. Attach, Join, Import are terms associated with bringing a cluster under the management of a hub cluster.
Install loop does not complete Symptom: make install does not complete in a timely fashion (~10 minutes from start). Status messages keep scrolling Cause: One of the conditions for installation has not been completed. See below for details.
Resolution: Re-run the failing step outside the loop. See below for how.
It is safe to exit the loop (via Ctrl-C, for example) and run the operations separately.
The industrial edge pattern runs two post-install operations after creating the main ArgoCD applications:
Extracting the secret from the datacenter ArgoCD instance for use in the Pipelines
This depends on the installation of both the cluster-wide GitOps operator, and the installation of an instance in the datacenter namespace. The logic is controlled here (where the parameters are set) and here, which does the interactions with the cluster (to extract the secret and create a resource in manuela-ci).
This task runs first, and if it does not complete, the seed pipeline will not start either. Things to check:
Check to make sure the operators are installing in your cluster correctly. Ensure you have enough capacity in your cluster to run all the needed resources. You can attempt to run the extraction outside of make install. Ensure that you have logged in to the cluster (via oc login or by exporting a suitable KUBECONFIG:
Run make secret in the base directory of your industrial-edge repository fork. Running the &ldquo;seed&rdquo; pipeline to populate the image registries for the manuela-tst-all namespace and the edge/factory namespaces (manuela-stormshift-messaging, manuela-line-dashboard etc.).
It is important that the seed pipeline run and complete because the applications will be &ldquo;degraded&rdquo; until they can deploy the images, and seed is what populates the images in the local cluster registries and instructs the applications to use them.
The seed pipeline depends on the Pipelines operator to be installed, as well as the tkn Task (in the manuela-ci namespace). The script checks for both. (make install calls the sleep-seed target, which checks for the resources before trying to kick off a seed pipeline run.
Run make seed in the base directory of your industrial edge repository fork. This kicks off the pipeline without checking for its dependencies. This target does not ensure that the seed pipeline completes. See below on how to re-run seed if the seed pipeline fails for any reason. It is safe to run the seed pipeline multiple times - each time it runs it will update the image targets for each of the images in both test (manuela-tst-all) and production (manuela-stormshift-messaging etc).
Subscriptions not being installed Symptom: Install seems to &ldquo;freeze&rdquo; at a specific point. Expected operators do not install in the cluster Cause: It is possible an operator was requested to be installed that isn&rsquo;t allowed to be installed on this version of OpenShift.
Resolution: In general, use the project-supplied global.options.UseCSV setting of False. This requests the current, best version of the operator available. If a specific CSV (Cluster Service Version) is requested but unavailable, that operator will not be able to install at all, and when an operator fails to install, that may have a cascading effect on other operators.
Potential (Known) Operational Issues Pipeline Failures Symptom: &ldquo;User not found&rdquo; error in first stage of pipeline run Cause: Despite the message, the error is most likely that you don&rsquo;t have a fork of manuela-dev.
Resolution: Fork manuela-dev into your namespace in GitHub and run make seed.
Symptom: Intermittent failures in Pipeline stages Some sample errors:
level=error msg=&#34;Error while applying layer: ApplyLayer io: read/write on closed pipe stdout: {\\&#34;layerSize\\&#34;:7301}\\n stderr: &#34; error creating build container: Error committing the finished image: error adding layer with blob time=&#34;2021-09-29T18:48:27Z&#34; level=fatal msg=&#34;Error trying to reuse blob sha256:235f9e6f3559c04d5ee09b613dcab06dbc03ceb93b65ce364afe35c03fd53574 at destination: failed to read from destination repository martjack/iot-software-sensor: 500 (Internal Server Error) I1006 22:07:47.908257 14 request.go:645] Throttling request took 1.195150708s, request: GET:https://172.30.0.1:443/apis/autoscaling.openshift.io/v1?timeout=32s PipelineRun started: seed-iot-software-sensor-run-cpzzv Waiting for logs to be available... E1006 22:08:27.106369 14 runtime.go:78] Observed a panic: &#34;send on closed channel&#34; (send on closed channel) goroutine 487 [running]: k8s.io/apimachinery/pkg/util/runtime.logPanic(0x1b40ee0, 0x1fe47b0) /workspace/pkg/mod/k8s.io/apimachinery@v0.19.7/pkg/util/runtime/runtime.go:74 +0x95 k8s.io/apimachinery/pkg/util/runtime.HandleCrash(0x0, 0x0, 0x0) /workspace/pkg/mod/k8s.io/apimachinery@v0.19.7/pkg/util/runtime/runtime.go:48 +0x89 panic(0x1b40ee0, 0x1fe47b0) When this happens, the pipeline may not entirely stop running. It is safe to stop/cancel the pipeline run, and desirable to do so, since multiple pipelines attempting to change the repository at the same time could cause more failures.
Resolution: Run make seed in the root of the repository OR re-run the failed pipeline segment (e.g. seed-iot-frontend or seed-iot-consumer).
We&rsquo;re looking into better long-term fixes for a number of the situations that can cause these situations as #40.
Symptom: Error in &ldquo;push-*&rdquo; pipeline tasks Cause: Multiple processes or people were trying to make changes to the repository at the same time. The state of the repository changed in the middle of the process in such a way that the update was not a &ldquo;fast-forward&rdquo; in git terms.
Resolution: Re-run the failed pipeline segment OR run make seed from the root of your fork of the industrial-edge repository.
It is also possible that multiple pipelines were running at the same time and were making conflicting changes. We recommend running one pipeline at a time.
Symptom: Pipelines application perpetually &ldquo;progressing&rdquo; and not showing green/healthy. May show &ldquo;degraded&rdquo; Cause: Most likely the application is missing the images that are built by the seed pipeline.
Resolution: Run make seed from the root of your forked repository directory, which will build the images and deploy them to both test and production.
Symptom: There is a &ldquo;spinny&rdquo; next to one of the resources in the app that never resolves Cause: Check for a PersistentVolumeClaim that is not in use.
Resolution: Delete the unused PVC
ArgoCD not syncing Symptom: ArgoCD shows an error and &ldquo;Unknown&rdquo; sync status Cause: A change has been made in the repository that renders invalid YAML
Resolution: Fix the issue as identified by the error message, and commit and push the fix OR revert the last one.
Certain changes might invalidate objects in ArgoCD, and this will prevent ArgoCD from deploying the change related to that commit. The error message for that situation might look like this (this particular change removed the Image details from the kustomization.yaml file, and we resolved it by re-adding the image entries:
rpc error: code = Unknown desc = Manifest generation error (cached): \`/bin/bash -c helm template . --name-template \${ARGOCD_APP_NAME:0:52} -f https://github.com/claudiol/industrial-edge/raw/deployment/values-global.yaml -f https://github.com/claudiol/industrial-edge/raw/deployment/values-datacenter.yaml --set global.repoURL=$ARGOCD_APP_SOURCE_REPO_URL --set global.targetRevision=$ARGOCD_APP_SOURCE_TARGET_REVISION --set global.namespace=$ARGOCD_APP_NAMESPACE --set global.pattern=industrial-edge --set global.valuesDirectoryURL=https://github.com/claudiol/industrial-edge/raw/deployment --post-renderer ./kustomize\` failed exit status 1: Error: error while running post render on files: error while running command /tmp/https:__github.com_claudiol_industrial-edge/charts/datacenter/manuela-tst/kustomize. error output: ++ dirname /tmp/https:__github.com_claudiol_industrial-edge/charts/datacenter/manuela-tst/kustomize + BASE=/tmp/https:__github.com_claudiol_industrial-edge/charts/datacenter/manuela-tst + &#39;[&#39; /tmp/https:__github.com_claudiol_industrial-edge/charts/datacenter/manuela-tst = /tmp/https:__github.com_claudiol_industrial-edge/charts/datacenter/manuela-tst &#39;]&#39; + BASE=./ + cat + echo / /tmp/https:__github.com_claudiol_industrial-edge/charts/datacenter/manuela-tst / /tmp/https:__github.com_claudiol_industrial-edge/charts/datacenter/manuela-tst + ls -al total 44 drwxr-xr-x. 3 default root 166 Oct 6 20:59 . drwxr-xr-x. 7 default root 98 Oct 6 20:28 .. -rw-r--r--. 1 default root 1105 Oct 6 20:28 Chart.yaml -rw-r--r--. 1 default root 22393 Oct 6 20:59 helm.yaml -rw-r--r--. 1 default root 98 Oct 6 20:59 kustomization.yaml -rwxr-xr-x. 1 default root 316 Oct 6 20:28 kustomize -rw-r--r--. 1 default root 348 Oct 6 20:28 system-image-builder-role-binding.yaml drwxr-xr-x. 7 default root 115 Oct 6 20:28 templates -rw-r--r--. 1 default root 585 Oct 6 20:28 values.yaml + kubectl kustomize ./ Error: json: cannot unmarshal object into Go struct field Kustomization.images of type []image.Image : exit status 1 Use --debug flag to render out invalid YAML Symptom: Warnings in ArgoCD for the same resource being owned by multiple applications Cause:
This is a byproduct of the way the pattern installs applications at the moment. We are tracking this as #39.
Symptom: Applications show &ldquo;not in sync&rdquo; status in ArgoCD Cause: There is a discrepancy between what the git repository says the application should have, and how that state is realized in ArgoCD.
The installation mechanism currently installs operators as parts of multiple applications when running on the same cluster, so it is a race condition in ArgoCD to see which one &ldquo;wins.&rdquo; This is a problem with the way we are installing the patterns. We are tracking this as #38.
`,url:"https://validatedpatterns.io/patterns/industrial-edge/troubleshooting/",breadcrumb:"/patterns/industrial-edge/troubleshooting/"},"https://validatedpatterns.io/patterns/retail/troubleshooting/":{title:"Troubleshooting",tags:[],content:"Troubleshooting Our Issue Tracker ",url:"https://validatedpatterns.io/patterns/retail/troubleshooting/",breadcrumb:"/patterns/retail/troubleshooting/"},"https://validatedpatterns.io/patterns/medical-diagnosis/troubleshooting/":{title:"Troubleshooting Guide",tags:[],content:` Contents Understanding the Makefile The Makefile is the entrypoint for the pattern. We use the Makefile to bootstrap the pattern to the cluster. After the initial bootstrapping of the pattern, the Makefile isn’t required for ongoing operations but can often be useful when needing to make a change to a config within the pattern by running a make upgrade which allows us to refresh the bootstrap resources without having to tear down the pattern or cluster.
make install / make deploy Executing make install within the pattern application will trigger a make deploy from &lt;pattern_directory&gt;/common. This initializes the common components of the pattern framework and will install a helm chart in the default namespace. At this point cluster services such as Red Hat Advanced Cluster Management and OpenShift Gitops are deployed.
Once common completes, the remaining tasks within the make install target will execute.
make vault-init / make load-secrets This pattern is integrated with HashiCorp Vault and External Secrets services for secrets management within the cluster. These targets install vault from a Helm chart and the load the secret (values-secret.yaml) you created during Getting Started.
If values-secret.yaml does not exist, make will exit with an error saying so. Furthermore, if the values-secret.yaml file does exist but is improperly formatted, ansible will exit with an error about being improperly formatted. If you are not sure how format the secret, please refer to Getting Started.
make bootstrap / make upgrade make bootstrap is the target used for deploying the application specific components of the pattern. It is the final step in the initial make install target. Running make bootstrap directly should typically not be necessary, instead you are encouraged to run make upgrade.
Generally, executing make upgrade should only be required when something goes wrong with the application pattern deployment. For instance, if a value was missed, and the chart wasn’t rendered correctly, executing make upgrade after fixing the value would be necessary.
If you have any further questions, please, feel free to review the Makefile for the common and Medical Diagnosis components. It is located in common/Makefile and ./Makefile respectively.
Troubleshooting the Pattern Deployment Occasionally the pattern will encounter issues during the deployment. This can happen for any number of reasons, but most often it is because of either a change within the operator itself or something has changed/happened to the Operator Lifecycle Manager (OLM) which determines which operators are available in the operator catalog. Generally, when an issue occurs with the OLM, the operator is unavailable for installation. To ensure that the operator is in the catalog:
oc get packagemanifests | grep &lt;operator-name&gt; When an issue occurs with the operator itself you can verify the status of the subscription and make sure that there are no warnings.An additional option is to log into the OpenShift Console, click on Operators, and check the status of the operator.
Other issues encounter could be with a specific application within the pattern misbehaving. Most of the pattern is deployed into the xraylab-1 namespace. Other components like ODF are deployed into openshift-storage and the OpenShift Serverless Operators are deployed into knative-serving, knative-eventing namespaces.
Use the grafana dashboard to assist with debugging and identifying the issue
Problem: No information is being processed in the dashboard
Solution: Most often this is due to the image-generator deploymentConfig needing to be scaled up. The image-generator by design is scaled to 0;
oc scale -n xraylab-1 dc/image-generator --replicas=1 Or open the openshift-console, click on workloads, then click deploymentConfigs, click image-generator, and scale the pod to 1 or more.
Problem: When browsing to the xraylab grafana dashboard and there are no images in the right-pane, only a security warning.
Solution: The certificates for the openshift cluster are untrusted by your system. The easiest way to solve this is to open a browser and go to the s3-rgw route (oc get route -n openshift-storage), then acknowledge and accept the security warning.
Problem: In the dashboard interface, no metrics data is available.
Solution: There is likely something wrong with the Prometheus DataSource for the grafana dashboard. You can check the status of the datasource by executing the following:
oc get grafanadatasources -n xraylab-1 Ensure that the prometheus datasource exists and that the status is available. This could potentially be the token from the service account (grafana-serviceaccount) that is provided to the datasource as a bearer token.
Problem: The dashboard is showing red in the corners of the dashboard panes.
Solution: This is most likely due to the xraylab database not being available or misconfigured. Please check the database and ensure that it is functioning properly.
Step 1: Ensure that the database is populated with the correct tables:
oc exec -it xraylabdb-1-&lt;uuid&gt; bash mysql -u root USE xraylabdb; SHOW tables; The expected output is:
Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 75 Server version: 10.3.32-MariaDB MariaDB Server Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others. Type &#39;help;&#39; or &#39;\\h&#39; for help. Type &#39;\\c&#39; to clear the current input statement. MariaDB [(none)]&gt; USE xraylabdb; Database changed MariaDB [xraylabdb]&gt; show tables; +---------------------+ | Tables_in_xraylabdb | +---------------------+ | images_anonymized | | images_processed | | images_uploaded | +---------------------+ 3 rows in set (0.000 sec) Step 2: Verify the password set in the values-secret.yaml is working
oc exec -it xraylabdb-1-&lt;uuid&gt; bash mysql -u xraylab -D xraylabdb -h xraylabdb -p &lt;provide your password at the prompt&gt; If you are able to successfully login then your password has been configured correctly in vault, the external secrets operator and mounted to the database correctly.
Problem: The image-generator is scaled correctly, but nothing is happening in the dashboard.
Solution: This could be that the serverless eventing function isn’t picking up the notifications from ODF and therefore, not triggering the knative-serving function to scale up. In this situation there are a number of things to check, the first thing is to check the logs of the rook-ceph-rgw-ocs-storagecluster-cephobjectstore-a-&lt;podGUID&gt; pod in the openshift-storage namespace.
oc logs -n openshift-storage -f &lt;pod&gt; -c rgw You should see the PUT statement with a status code of 200
Next ensure that the kafkasource and kafkservice and kafka topic resources have been created:
oc get -n xraylab-1 kafkasource NAME TOPICS BOOTSTRAPSERVERS READY REASON AGE xray-images [&#34;xray-images&#34;] [&#34;xray-cluster-kafka-bootstrap.xraylab-1.svc:9092&#34;] True 23m oc get -n xraylab-1 kservice NAME URL LATESTCREATED LATESTREADY READY REASON risk-assessment https://risk-assessment-xraylab-1.apps.&lt;SUBDOMAIN&gt; risk-assessment-00001 risk-assessment-00001 True oc get -n xraylab-1 kafkatopics NAME CLUSTER PARTITIONS REPLICATION FACTOR READY consumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a xray-cluster 50 1 True strimzi-store-topic---effb8e3e057afce1ecf67c3f5d8e4e3ff177fc55 xray-cluster 1 3 True strimzi-topic-operator-kstreams-topic-store-changelog---b75e702040b99be8a9263134de3507fc0cc4017b xray-cluster 1 1 True xray-images xray-cluster 1 1 True `,url:"https://validatedpatterns.io/patterns/medical-diagnosis/troubleshooting/",breadcrumb:"/patterns/medical-diagnosis/troubleshooting/"},"https://validatedpatterns.io/learn/using-validated-pattern-operator/":{title:"Using the Validated Patterns Operator",tags:[],content:` You can use the Validated Patterns Operator to install and manage Validated Patterns. Use the Red Hat Hybrid Cloud Console to install the Validated Patterns Operator. After installing the Operator, you can create an instance where you can specify the details for your pattern. The Validated Patterns Operator then installs and manages the required assets and artifacts that the pattern requires.
Installing the Validated Patterns Operator Prerequisites Access to an OpenShift Container Platform cluster using an account with cluster-admin permissions.
Procedure Navigate in the Red Hat Hybrid Cloud Console to the Operators → OperatorHub page.
Scroll or type a keyword into the Filter by keyword box to find the Operator you want. For example, type validated patterns to find the Validated Patterns Operator.
Select the Operator to display additional information.
Choosing a Community Operator warns that Red Hat does not certify Community Operators; you must acknowledge the warning before continuing.
Read the information about the Operator and click Install.
On the Install Operator page:
Select an Update channel (if more than one is available).
Select a Version (if more than one is available).
Select an Installation mode:
All namespaces on the cluster (default) installs the Operator in the default openshift-operators namespace to watch and be made available to all namespaces in the cluster. This option is not always available.
A specific namespace on the cluster allows you to choose a specific, single namespace in which to install the Operator. The Operator will only watch and be made available for use in this single namespace.
Select Automatic or Manual approval strategy.
Click Install to make the Operator available to the selected namespaces on this OpenShift Container Platform cluster.
Verification To confirm that the installation is successful:
Navigate to the Operators → Installed Operators page.
Check that the Operator is installed in the selected namespace and its status is Succeeded.
Creating a pattern instance Prerequisites The Validated Patterns Operator is successfully installed in the relevant namespace.
Procedure Navigate to the Operators → Installed Operators page.
Click the installed Validated Patterns Operator.
Under the Details tab, in the Provided APIs section, in the Pattern box, click Create Instance that displays the Create Pattern page.
On the the Create Pattern page, select Form view and enter information in the following fields:
Name - A name for the pattern deployment that is used in the projects that you created.
Labels - Apply any other labels you might need for deploying this pattern.
Cluster Group Name - Select a cluster group name to identify the type of cluster where this pattern is being deployed. For example, if you are deploying the Industrial Edge pattern, the cluster group name is datacenter. If you are deploying the Multicloud GitOps pattern, the cluster group name is hub.
To know the cluster group name for the patterns that you want to deploy, check the relevant pattern-specific requirements.
Expand the Git Config section to reveal the options and enter the required information.
Change the Target Repo URL to your forked repository URL. For example, change https://github.com/validatedpatterns/&lt;pattern_name&gt; to https://github.com/&lt;your-git-username&gt;/&lt;pattern-name&gt;
Optional: You might need to change the Target Revision field. The default value is HEAD. However, you can also provide a value for a branch, tag, or commit that you want to deploy. For example, v2.1, main, or a branch that you created, my-branch.
Ensure that you have made any required changes to your values-*.yaml files locally and pushed them to your forked repository on the correct branch or target that you chose in the previous step.
Click Create.
Verification The Red Hat OpenShift GitOps Operator displays in list of Installed Operators. The Red Hat OpenShift GitOps Operator installs the remaining assets and artifacts for this pattern. To view the installation of these assets and artifacts, such as Red Hat Advanced Cluster Management (RHACM), ensure that you switch to Project:All Projects.
For more information about post-installation instructions for a pattern, see its Getting started page.
`,url:"https://validatedpatterns.io/learn/using-validated-pattern-operator/",breadcrumb:"/learn/using-validated-pattern-operator/"},"https://validatedpatterns.io/learn/workflow/":{title:"Workflow",tags:[],content:`Workflow These patterns are designed to be composed of multiple components, and for those components to be used in gitops workflows by consumers and contributors. To use the first pattern as an example, we maintain the Industrial Edge pattern, which uses a repo with pattern-specific logic and configuration as well as a common repo which has elements common to multiple patterns. The common repository is included in each pattern repository as a subtree.
Consuming a pattern Fork the pattern repository on GitHub to your workspace (GitHub user or organization). It is necessary to fork because your fork will be updated as part of the GitOps and DevOps processes, and the main branch (by default) will be used in the automated workflows.
Clone the forked copy
git clone git@github.com:&lt;your-workspace&gt;/industrial-edge.git
Create a local copy of the Helm values file that can safely include credentials
DO NOT COMMIT THIS FILE You do not want to push personal credentials to GitHub.
cp values-secret.yaml.template ~/values-secret.yaml vi ~/values-secret.yaml Customize the deployment for your cluster
vi values-global.yaml git commit values-global.yaml git push Contributing For contributions, we recommend adding the upstream repository as an additional remote, and making changes on a branch other than main. Changes on this branch can then be merged to the main branch (to be reflected in the GitOps workflows) and will be easier to make upstream, if you wish. Contributions from your forked main branch will contain, by design:
Customizations to values-global.yaml and other files that are particular to your installation Commits made by Tekton and other automated processes that will be particular to your installation To isolate changes for upstreaming (hcp is &ldquo;Hybrid Cloud Patterns&rdquo;, you can use a different remote and/or branch name if you want):
git remote add hcp https://github.com/hybrid-cloud-patterns/industrial-edge git fetch --all git branch -b hcp-main -t hcp/main &lt;make changes on the hcp-main branch&gt; git push origin hcp-main To update branch hcp-main with upstream changes:
git checkout hcp-main git pull --rebase To reflect these changes in your forked repository (such as if you would like to submit a PR later):
git push origin hcp-main If you want to integrate upstream pattern changes into your local GitOps process:
git checkout main git merge hcp-main git push origin main Using this workflow, the hcp-main branch will:
Be isolated from any changes that are being made by your local GitOps processes Be merge-able (or cherry-pick-able) into your local main branch to be used by your local GitOps processes (this is especially useful for tracking when any submodules, like common, update) Be a good basis for submitting Pull Requests to be integrated upstream, since it will not contain your local configuration differences or your local GitOps commits Changing subtrees Our patterns use the git subtree feature as a mechanism to promote modularity, so that multiple patterns can use the same common basis. Over time we will move more functionality into common, to isolate the components that are particular to each pattern, and standard usage conventions emerge. This will make the tools in common more powerful and featureful, and make it easier to develop new patterns. Normally, we will maintain the common subtree in the normal course of updates, and pulling changes from upstream will include any changes from common.
You only need to change subtrees if you want to test changes in the common/ area of the pattern repositories, or if you wish to contribute to the common/ repository itself in conjunction with one of the patterns. Using the pattern by itself does not require changing subtrees.
For the common cases (use and consumption of the pattern), users do not need to be aware that the pattern uses a subtree at all.
git clone https://github.com/&lt;your-workspace&gt;/industrial-edge If you want to change and track your own version of common, you should fork and clone our common repository separately:
git clone https://github.com/&lt;your-workspace&gt;/common Now, you can make changes in your fork&rsquo;s main branch, or else make a new branch and make changes there.
If you want to track these changes in your fork of the pattern repository (industrial-edge in this case), you will need to swap out the subtree in industrial-edge for the version of common you forked. We have provided a script to make this a bit easier:
common/scripts/make_common_subtree.sh &lt;subtree_repo&gt; &lt;subtree_branch&gt; &lt;subtree_remote_name&gt; This script will set up a new remote in your local working directory with the repository you specify. It will replace the common directory with a new common from the fork and branch you specify, and commit it. The script will not push the result.
For example:
common/scripts/make_common_subtree.sh https://github.com/mhjacks/common.git wip-main common-subtree This will replace common in the current repository with the wip-main branch from the common in mhjacks&rsquo;s common repository, and call the remote common-subtree.
From that point, changes from mhjacks&rsquo;s wip-main branch on mhjacks&rsquo;s fork of common can be pulled in this way:
git subtree pull --prefix common common-subtree wip-main When run without arguments, the script will run as if it had been given the following arguments:
common/scripts/make_common_subtree.sh https://github.com/hybrid-cloud-patterns/common.git main common-subtree Which are the defaults the repository is normally configured with.
Subtree vs. Submodule It has always been important to us to be have a substrate for patterns that is as easy as possible to share amongst multiple patterns. While it is possible to share changes between multiple unrelated git repositories, it is an almost entirely manual process, prone to error. We feel it is important to be able to provide a &ldquo;pull&rdquo; experience (i.e. one git &ldquo;pull&rdquo; type action) to update the shared components of a pattern. Two strategies exist for repository sharing in this way: submodule and subtree. We started with submodules but have since moved to subtree.
Atlassian has some good documentation on what subtree is here and here. In short, a subtree integrates another repository&rsquo;s history into a parent repository, which allows for most of the benefits of a submodule workflow, without most of the caveats.
Earlier versions of this document described the usage of patterns with submodules instead of subtrees. In the earliest stages of pattern development, we used submodules because the developers of the project were familiar with submodules and had used them previously, but we had not used subtrees. User feedback, as well as some of the unavoidable complexities of submodules, convinced us to try subtrees and we believe we will stick with that strategy. Some of the unavoidable complexities of submodules include:
Having to remember to checkout repositories with --recurse-submdules, or else doing git submodule init &amp;&amp; git submodule sync. Experienced developers asked in several of our support channels early on why common was empty. Hoping that other tools that are interacting with the repository are compatible with the submodule approach. (To be fair, tools like ArgoCD and Tekton Pipelines did this very well; their support of submodules was one of the key reasons we started with submodules) When changing branches on a submoduled repository, if the branch you were changing to was pointed to a different revision of the submoduled repository, the repository would show out of sync. While this behavior is correct, it can be surprising and difficult to navigate. In disconnected environments, submodules require mirroring more repositories. Developing with a fork of the submoduled repository means maintaining two forked repositories and multiple branches in both. Subtrees have some pitfalls as well. In the subtree strategy, it is easier to diverge from the upstream version of the subtree repository, and in fact with a typical git clone, the user may not be aware that a subtree is in use at all. This can be considered a feature, but could become problematic if the user/consumer later wants to update to a newer version of the subtree but local changes might conflict. Additionally, since subtrees are not as well understood generally, there can be some surprising effects. In practice, we have run into the following:
Cherry picking from a subtree commit into the parent puts the change in the parent location, not the subtree Contributing to Patterns using Common Subtrees Once you have forked common and changed your subtree for testing, changes from your fork can then be proposed to [https://github.com/hybrid-cloud-patterns/common.git] and can then be integrated into other patterns. A change to upstream common for a particular upstream pattern would have to be done in two stages:
PR the change into upstream&rsquo;s common PR the updated common into the pattern repository `,url:"https://validatedpatterns.io/learn/workflow/",breadcrumb:"/learn/workflow/"},"https://validatedpatterns.io/learn/implementation/":{title:"Implementation Requirements",tags:[],content:`Technical Requirements Additional requirements specific to the implementation for all Community, and Validated patterns
Must Patterns MUST include one or more Git repositories, in a publicly accessible location, containing configuration elements that can be consumed by the OpenShift GitOps operator (ArgoCD) without supplying custom ArgoCD images.
Patterns MUST be useful without all content stored in private git repos
Patterns MUST include a list of names and versions of all the products and projects being consumed by the pattern
Patterns MUST be useful without any sample applications that are private or lack public sources.
Patterns must not become useless due to bit rot or opaque incompatibilities in closed source “applications”.
Patterns MUST NOT store sensitive data elements, including but not limited to passwords, in Git
Patterns MUST be possible to deploy on any IPI-based OpenShift cluster (BYO)
We distinguish between the provisioning and configuration requirements of the initial cluster (“Patterns”), and of clusters/machines managed by the initial cluster (see “Managed clusters”)
Patterns MUST use a standardized clustergroup Helm chart, as the initial OpenShift GitOps application that describes all namespaces, subscriptions, and any other GitOps applications which contain the configuration elements that make up the solution.
Managed clusters MUST operate on the premise of “eventual consistency” (automatic retries, and an expectation of idempotence), which is one of the essential benefits of the GitOps model.
Imperative elements MUST be implemented as idempotent code stored in Git
Should Patterns SHOULD include sample application(s) to demonstrate the business problem(s) addressed by the pattern.
Patterns SHOULD try to indicate which parts are foundational as opposed to being for demonstration purposes.
Patterns SHOULD use the VP operator to deploy patterns. However anything that creates the OpenShift GitOps subscription and initial clustergroup application could be acceptable.
Patterns SHOULD embody the “open hybrid cloud model” unless there is a compelling reason to limit the availability of functionality to a specific platform or topology.
Patterns SHOULD use industry standards and Red Hat products for all required tooling
Patterns prefer current best practices at the time of pattern development. Solutions that do not conform to best practices should expect to justify non-conformance and/or expend engineering effort to conform.
Patterns SHOULD NOT make use of upstream/community operators and images except, depending on the market segment, where critical to the overall solution.
Such operators are forbidden to be deployed into an increasing number of customer environments, which limits reuse. Alternatives include productizing the operator, and building it in-cluster from trusted sources as part of the pattern.
Patterns SHOULD be decomposed into modules that perform a specific function, so that they can be reused in other patterns.
For example, Bucket Notification is a capability in the Medical Diagnosis pattern that could be used for other solutions.
Patterns SHOULD use Ansible Automation Platform to drive the declarative provisioning and management of managed hosts (e.g. RHEL). See also “Imperative elements”.
Patterns SHOULD use RHACM to manage policy and compliance on any managed clusters.
Patterns SHOULD use RHACM and a standardized acm chart to deploy and configure OpenShift GitOps to managed clusters.
Managed clusters SHOULD be loosely coupled to their hub, and use OpenShift GitOps to consume applications and configuration directly from Git as opposed to having hard dependencies on a centralized cluster.
Managed clusters SHOULD use the “pull” deployment model for obtaining their configuration.
Imperative elements SHOULD be implemented as Ansible playbooks
Imperative elements SHOULD be driven declaratively – by which we mean that the playbooks should be triggered by Jobs and/or CronJobs stored in Git and delivered by OpenShift GitOps.
Can Patterns CAN include additional configuration and/or demo elements located in one or more additional private git repos. Patterns CAN include automation that deploys a known set of clusters and/or machines in a specific topology Patterns CAN limit functionality/testing claims to specific platforms, topologies, and cluster/node sizes Patterns CAN consume operators from established partners (e.g. Hashicorp Vault, and Seldon) Patterns CAN include managed clusters Patterns CAN include details or automation for provisioning managed clusters, or rely on the admin to pre-provision them out-of-band. Patterns CAN also choose to model multi-cluster solutions as an uncoordinated collection of “initial hub clusters” Imperative elements CAN interact with cluster state and/or external influences `,url:"https://validatedpatterns.io/learn/implementation/",breadcrumb:"/learn/implementation/"},"https://validatedpatterns.io/learn/community/":{title:"Community Patterns",tags:[],content:`Community Pattern Requirements tl;dr What are they: Best practice implementations conforming to the Validated Patterns implementation practices Purpose: Codify best practices and promote collaboration between different groups inside, and external to, Red Hat Creator: Customers, Partners, GSIs, Services/Consultants, SAs, and other Red Hat teams Requirements General requirements for all Community, and Validated patterns
Base Patterns MUST include a top-level README highlighting the business problem and how the pattern solves it
Patterns MUST include an architecture drawing. The specific tool/format is flexible as long as the meaning is clear.
Patterns MUST undergo an informal architecture review by a community leader to ensure that the solution has the right products, and they are generally being used as intended.
For example: not using a database as a message bus. As community leaders, contributions from within Red Hat may be subject to a higher level of scrutiny While we strive to be inclusive, the community will have quality standards and generally using the framework does not automatically imply a solution is suitable for the community to endorse/publish.
Patterns MUST undergo an informal technical review by a community leader to ensure that it conforms to the technical requirements and meets basic reuse standards
Patterns MUST document their support policy
It is anticipated that most community patterns will be supported by the community on a best-effort basis, but this should be stated explicitly. The validated patterns team commits to maintaining the framework but will also accept help.
Patterns SHOULD include a recorded demo highlighting the business problem and how the pattern solves it
`,url:"https://validatedpatterns.io/learn/community/",breadcrumb:"/learn/community/"},"https://validatedpatterns.io/learn/validated/":{title:"Validated Patterns",tags:[],content:`Validated Pattern Requirements tl;dr What are they: Technical foundations, backed by CI, that have succeeded in the market and Red Hat expects to be repeatable across customers and segments. Purpose: Reduce risk, accelerate sales cycles, and allow consulting organizations to be more effective. Creator: The Validated Patterns team in conjunction with: Partners, GSIs, Services/Consultants, SAs, and other Red Hat teams Onboarding Existing Implementations The Validated Patterns team has a preference for empowering other, and not taking credit for their work.
Where there is an existing application/demo, there is also a strong preference for the originating team to own any changes that are needed for the implementation to become a validated pattern. Alternatively, if the Validated Patterns team drives the conversion, then in order to prevent confusion and duplicated efforts, we are likely to ask for a commitment to phase out use of the previous implementation for future engagements such as demos, presentations, and workshops.
The goal is to avoid bringing a parallel implementation into existence which divides Red Hat resources, and creates confusion internally and with customers as the implementations drift apart.
In both scenarios the originating team can choose where to host the primary repository, will be given admin permissions to any fork in https://github.com/hybrid-cloud-patterns, and will receive on-going assistance from the Validated Patterns team.
Nominating a Community Pattern to become Validated If there is a community pattern that you believe would be a good candidate for becoming validated, please email hybrid-cloud-patterns@googlegroups.com at least 4 weeks prior to the end of a given quarter in order for the necessary work to be considered as part of the following quarter’s planning process.
Please be aware that each Validated Pattern represents an ongoing maintenance, support, and CI effort. Finite team capacity means we must critically balance this cost against the potential customer opportunity. A “no” or “not yet” result is not intended as an insult against the pattern or its author.
Requirements Validated Patterns have deliverable and requirements in addition to those specified for Community-level patterns
Must Validated Patterns MUST contain more than two RH products. Alternative: Engage with the BU
Validated Patterns, or the solution on which they are based, MUST have been deployed and approved for use in at least one customer environment.
Alternative: Community Pattern
Validated Patterns MUST be meaningful without specialized hardware, including flavors of architectures not explicitly supported. Alternative: Engage with DCI
Qualification is a Validated Patterns engineering decision with input from the pattern owner.
Validated Patterns MUST be broadly applicable. Alternative: Engage with Phased Gate and/or TAMs
Qualification is a Validated Patterns PM decision with input from the pattern owner.
Validated Patterns MUST only make use of Red Hat products that are already fully supported by their product team(s).
Validated Patterns MUST NOT rely on functionality in tech-preview, or hidden behind feature gates.
Validated Patterns MUST conform to the same Community-level implementation requirements
Validated Patterns MUST have their architectures reviewed by the PM, TPM, or TMM of each Red Hat product they consume to ensure consistency with the product teams’ intentions and roadmaps
Validated Patterns MUST have their implementation reviewed by the patterns team to ensure that it is sufficiently flexible to function across a variety of platforms, customer environments, and any relevant verticals.
Validated Patterns MUST include a standardized architecture drawing, created with (or at least conforming to) the PAC tooling
Validated Patterns MUST include a presentation deck oriented around the business problem being solved and intended for use by the field to sell and promote the solution
Validated Patterns MUST include a recorded demo highlighting the business problem and how the pattern solves it
Validated Patterns MUST include a test plan covering all features or attributes being highlighted by the demo that also spans multiple products. Negative flow tests (such as resiliency or data retention in the presence of network outages) are limited to scenarios covered by the demonstration script.
Validated Patterns MUST include automated CI testing that runs on every change to the pattern, or a schedule no less frequently than once per week
Validated Patterns MUST create a new point release of the validation-level deliverables when minor versions (e.g. “12” in OpenShift 4.12) of consumed products change
Validated Patterns MUST document their support policy
The individual products used in a Validated Pattern are backed by the full Red Hat support experience conditional on the customer’s subscription to those products, and the individual products’ support policy. Additional components in a Validated Pattern that are not supported by Red Hat (e.g. Hashicorp Vault, and Seldon Core) will require a customer to obtain support from that vendor directly. The validated patterns team is very motivated to address any problems in the VP Operator, as well as problems in the common helm charts, but cannot not offer any SLAs at this time. See also our standard disclaimer
Validated Patterns DO NOT imply an obligation of support for partner or community operators by Red Hat.
Should Validated Patterns SHOULD focus on functionality not performance.
Validated Patterns SHOULD trigger CI runs for new versions of consumed products
Validated Patterns SHOULD provide an RHPDS lab environment
A bare bones environment into which the solution can be deployed, and a list of instructions for doing so (e.g. installing and configuring OpenShift GitOps)
Validated Patterns SHOULD provide pre-built demo environment using RHPDS
Having an automated demo within the RHPDS system, that will be built based on the current stable version that is run against the CI testing system
Validated Patterns SHOULD track deployments of each validation-level deliverable
For lifecycle decisions like discontinuing support of a version For notification if problems are found in our CI
Can Teams creating Validated Patterns CAN provide their own SLA
A document for QE that defines, at a technical level, how to validate if the pattern has been successfully deployed and is functionally operational. Example: Validating an Industrial Edge Deployment
`,url:"https://validatedpatterns.io/learn/validated/",breadcrumb:"/learn/validated/"},"https://validatedpatterns.io/patterns/devsecops/cluster-sizing/":{title:"Cluster Sizing",tags:[],content:`OpenShift Cluster Sizing for the Multicluster DevSecOps Pattern Tested Platforms The Multicluster DevSecOps pattern has been tested in the following Certified Cloud Providers. Due to changes in Advanced Cluster Management 2.5, this pattern does not work, &ldquo;out-of-the-box&rdquo;, with earlier versions of OCP than 4.10. While it&rsquo;s possible that it could work with some changes, we do not recommend using a version less than 4.10.
| Certified Cloud Providers | 4.10 | 4.11 | 4.x | :&mdash;- | :&mdash;- | :&mdash;- | Amazon Web Services | Tested | Untested | | Google Compute | Untested | Untested | | Microsoft Azure | Untested | Untested |
Multicluster DevSecOps Pattern Components Here&rsquo;s an inventory of what gets deployed by default the Secure Supply Chain pattern on the Hub OpenShift cluster:
Name Kind Namespace Description Red Hat Advanced Cluster Management Operator open-cluster-management Advance cluster management Red Hat OpenShift GitOps Operator openshift-operators ArgoCD GitOps Red Hat Advanced Cluster Security Operator stackrox Advanced cluster security, central and secured Red Hat Quay Operator quay-enterprise Secure container registry Red Hat Open Data Foundation Operator openshift-storage Highly available software-defined storage Hashicorp Vault Community version Operator vault Secrets Management The hub can be modified to deploy OpenShift Pipelines if needed. See Development cluster pattern components.
Multicluster DevSecOps Pattern OpenShift Datacenter HUB Cluster Size The Secure Supply Chain pattern has been tested with a defined set of specifically tested configurations that represent the most common combinations that Red Hat OpenShift Container Platform (OCP) customers are using or deploying for the x86_64 architecture.
The Hub OpenShift Cluster is made up of the the following on the AWS deployment tested:
Node Type Number of nodes Cloud Provider Instance Type Control Plane 3 Amazon Web Services m5.xlarge Worker 3 Amazon Web Services m5.4xlarge The Hub OpenShift cluster needs to be a larger than the managed clusters for this demo because it deploys critical pattern infrastructure components like Red Hat Quay which requires Red Hat Open Data Foundation (ODF). The above cluster sizing is close to a minimum size for a Hub cluster. In the next few sections we take some snapshots of the cluster utilization while the Multicluster DevSecOps pattern is running. Keep in mind that resources will have to be added as more images and image versions are added to the Quay registry.
Hub Cluster utilization Below is a snapshot of the OpenShift cluster utilization while running the Multicluster DevSecOps pattern:
TBD
CPU Memory File System Network Pod Count 38 66 GiB 226 MiB 13 MB/s 441 Secure Supply Chain Pattern OpenShift Development (devel) Cluster Size Here&rsquo;s an inventory of what gets deployed by default the Secure Supply Chain pattern on the Development (devel) OpenShift cluster:
Name Kind Namespace Description Red Hat Advanced Cluster Management agent open-cluster-management Advance cluster management agent only Red Hat OpenShift GitOps Operator openshift-operators ArgoCD GitOps Red Hat Advanced Cluster Security Operator stackrox Advanced cluster security, secured Red Hat OpenShift Pipelines Operator openshift-operators Tekton pipelines for CI Red Hat Quay Bridge Operator openshift-operators Quay registry integration The OpenShift cluster is a standard deployment of 3 control plane nodes and 3 or more worker nodes.
Node Type Number of nodes Cloud Provider Instance Type Control Plane/Worker 6 Google Cloud n1-standard-8 Control Plane/Worker 6 Amazon Cloud Services m5.2xlarge Control Plane/Worker 6 Microsoft Azure Standard_D8s_v3 Multicluster DevSecOps Pattern OpenShift Production (prod) Cluster Size Here&rsquo;s an inventory of what gets deployed by default the Multicluster DevSecOps pattern on the Production (prod) OpenShift cluster:
Name Kind Namespace Description Red Hat Advanced Cluster Management agent open-cluster-management Advance cluster management agent only Red Hat OpenShift GitOps Operator openshift-operators ArgoCD GitOps Red Hat Advanced Cluster Security Operator stackrox Advanced cluster security, secured Red Hat Quay Bridge Operator openshift-operators Quay registry integration The OpenShift cluster is a standard datacenter deployment of 3 control plane nodes and 3 or more worker nodes.
Node Type Number of nodes Cloud Provider Instance Type Control Plane/Worker 6 Google Cloud n1-standard-8 Control Plane/Worker 6 Amazon Cloud Services m5.2xlarge Control Plane/Worker 6 Microsoft Azure Standard_D8s_v3 Managed Datacenter Cluster Utilization GCP
This is a snapshot of a Google Cloud managed data center cluster running the production Multicluster DevSecOps pattern.
CPU Memory File System Network Pod Count AWS
This is a snapshot of a Amazon Web Services managed data center cluster running the production Multicluster DevSecOps pattern.
CPU Memory File System Network Pod Count Azure
This is a snapshot of an Azure managed data center cluster running the production Multicluster DevSecOps pattern.
CPU Memory File System Network Pod Count AWS Instance Types The Multicluster DevSecOps pattern was tested with the highlighted AWS instances in bold. The OpenShift installer will let you know if the instance type meets the minimum requirements for a cluster.
The message that the openshift installer will give you will be similar to this message
INFO Credentials loaded from default AWS environment variables FATAL failed to fetch Metadata: failed to load asset &#34;Install Config&#34;: [controlPlane.platform.aws.type: Invalid value: &#34;m4.large&#34;: instance type does not meet minimum resource requirements of 4 vCPUs, controlPlane.platform.aws.type: Invalid value: &#34;m4.large&#34;: instance type does not meet minimum resource requirements of 16384 MiB Memory] Below you can find a list of the AWS instance types that can be used to deploy the Multicluster DevSecOps pattern.
Instance type Default vCPUs Memory (GiB) Datacenter Factory/Edge 3x3 OCP Cluster 3 Node OCP Cluster m4.xlarge 4 16 N N m4.2xlarge 8 32 Y Y m4.4xlarge 16 64 Y Y m4.10xlarge 40 160 Y Y m4.16xlarge 64 256 Y Y m5.xlarge 4 16 Y N m5.2xlarge 8 32 Y Y m5.4xlarge 16 64 Y Y m5.8xlarge 32 128 Y Y m5.12xlarge 48 192 Y Y m5.16xlarge 64 256 Y Y m5.24xlarge 96 384 Y Y The OpenShift cluster is made of 4 Control Plane nodes and 3 Workers for the Datacenter and the Edge/managed data center cluster are made of 3 Control Plane and 3 Worker nodes. For the node sizes we used the m5.xlarge on AWS and this instance type met the minimum requirements to deploy the Multicluster DevSecOps pattern successfully on the Datacenter hub. On the managed data center cluster we used the m5.xlarge since the minimum cluster was comprised of 3 nodes. .
To understand better what types of nodes you can use on other Cloud Providers we provide some of the details below.
Azure Instance Types The Multicluster DevSecOps pattern was also deployed on Azure using the Standard_D8s_v3 VM size. Below is a table of different VM sizes available for Azure. Keep in mind that due to limited access to Azure we only used the Standard_D8s_v3 VM size.
The OpenShift cluster is made of 3 Control Plane nodes and 3 Workers for the Datacenter cluster.
The OpenShift cluster is made of 3 Control Plane nodes and 3 or more workers for each of the managed data center clusters.
Type Sizes Description General purpose B, Dsv3, Dv3, Dasv4, Dav4, DSv2, Dv2, Av2, DC, DCv2, Dv4, Dsv4, Ddv4, Ddsv4 Balanced CPU-to-memory ratio. Ideal for testing and development, small to medium databases, and low to medium traffic web servers. Compute optimized F, Fs, Fsv2, FX High CPU-to-memory ratio. Good for medium traffic web servers, network appliances, batch processes, and application servers. Memory optimized Esv3, Ev3, Easv4, Eav4, Ev4, Esv4, Edv4, Edsv4, Mv2, M, DSv2, Dv2 High memory-to-CPU ratio. Great for relational database servers, medium to large caches, and in-memory analytics. Storage optimized Lsv2 High disk throughput and IO ideal for Big Data, SQL, NoSQL databases, data warehousing and large transactional databases. GPU NC, NCv2, NCv3, NCasT4_v3, ND, NDv2, NV, NVv3, NVv4 Specialized virtual machines targeted for heavy graphic rendering and video editing, as well as model training and inferencing (ND) with deep learning. Available with single or multiple GPUs. High performance compute HB, HBv2, HBv3, HC, H Our fastest and most powerful CPU virtual machines with optional high-throughput network interfaces (RDMA). For more information please refer to the Azure VM Size Page.
Google Cloud (GCP) Instance Types The Multicluster DevSecOps pattern was also deployed on GCP using the n1-standard-8 VM size. Below is a table of different VM sizes available for GCP. Keep in mind that due to limited access to GCP we only used the n1-standard-8 VM size.
The OpenShift cluster is made of 3 Control Plane and 3 Workers for the Datacenter cluster.
The OpenShift cluster is made of 3 Nodes combining Control Plane/Workers for the Edge/managed data center cluster.
The following table provides VM recommendations for different workloads.
| General purpose | Workload optimized
Cost-optimized Balanced Scale-out optimized Memory-optimized Compute-optimized Accelerator-optimized E2 N2, N2D, N1 T2D M2, M1 C2 A2 Day-to-day computing at a lower cost Balanced price/performance across a wide range of VM shapes Best performance/cost for scale-out workloads Ultra high-memory workloads Ultra high performance for compute-intensive workloads Optimized for high performance computing workloads For more information please refer to the GCP VM Size Page.
`,url:"https://validatedpatterns.io/patterns/devsecops/cluster-sizing/",breadcrumb:"/patterns/devsecops/cluster-sizing/"},"https://validatedpatterns.io/patterns/industrial-edge/cluster-sizing/":{title:"Cluster Sizing",tags:[],content:`OpenShift Cluster Sizing for the Industrial Edge Pattern Tested Platforms The Industrial-Edge pattern has been tested in the following Certified Cloud Providers.
Certified Cloud Providers 4.8 4.9 4.10 Amazon Web Services :heavy_check_mark: Microsoft Azure :heavy_check_mark: Google Cloud Platform :heavy_check_mark: General OpenShift Minimum Requirements OpenShift 4 has the following minimum requirements for sizing of nodes:
Minimum 4 vCPU (additional are strongly recommended). Minimum 16 GB RAM (additional memory is strongly recommended, especially if etcd is colocated on masters). Minimum 40 GB hard disk space for the file system containing /var/. Minimum 1 GB hard disk space for the file system containing /usr/local/bin/. There are several applications that comprise the industrial-edge pattern. In addition, the industrial-edge pattern also includes a number of supporting operators that are installed by OpenShift GitOps using ArgoCD.
Industrial-Edge Pattern Components Here&rsquo;s an inventory of what gets deployed by the Industrial-Edge pattern on the Datacenter/Hub OpenShift cluster:
Name Kind Namespace Description line-dashboard Application manuela-tst-all Frontend application machine-sensor-1 Application manuela-tst-all Data publisher machine-sensor-2 Application manuela-tst-all Data publisher messaging Application manuela-tst-all Data subscriber mqtt2kafka-integration Application manuela-tst-all Kafka Integration anomaly-detection-predictor-0-anomaly-detection Application manuela-tst-all Anomaly detection application manuela-kafka-cluster-entity-operator Operator manuela-tst-all Kafka Red Hat Advanced Cluster Management Operator open-cluster-management Advance Cluster Management Red Hat Integration - AMQ Broker Operator manuela-tst-all AMQ Broker Red Hat Integration - AMQ Streams Operator manuela-tst-all AMQ Streams Open Data Hub Operator openshift-operators Open Data Hub Red Hat OpenShift GitOps Operator openshift-operators OpenShift GitOps Red Hat Integration - Camel K Operator manuela-tst-all Integration Platform, Kamelet Binding, Kamelet Red Hat OpenShift Pipelines Operator All Namespaces Tekton Config, Pipelines, Triggers, Addons Seldon Operator Operator manuela-tst-all Seldon Deployment Industrial-Edge Pattern OpenShift Datacenter HUB Cluster Size The Industrial-Edge pattern has been tested with a defined set of specifically tested configurations that represent the most common combinations that Red Hat OpenShift Container Platform (OCP) customers are using or deploying for the x86_64 architecture.
The Datacenter HUB OpenShift Cluster is made up of the the following on the AWS deployment tested:
Node Type Number of nodes Cloud Provider Instance Type Master 3 Amazon Web Services m5.xlarge Worker 4 Amazon Web Services m5.xlarge The Datacenter HUB OpenShift cluster needs to be a bit bigger than the Factory/Edge clusters because this is where the developers will be running pipelines to build and deploy the Industrial Edge pattern on the cluster. The above cluster sizing is close to a minimum size for a Datacenter HUB cluster. In the next few sections we take some snapshots of the cluster utilization while the Industrial Edge pattern is running. Keep in mind that resources will have to be added as more developers are working building their applications.
Datacenter Cluster utilization Below is a snapshot of the OpenShift cluster utilization while running the Industrial-Edge pattern:
CPU Memory File System Network Pod Count 13.84 Used 42.16 available of 56 73.5 GiB 146.3 GiB available of 219.8 GiB 106 GiB 732.9 GiB available of 838.9 GiB 20.65 MBps in 22.84 MBps out 354 pods Industrial-Edge Pattern OpenShift Factory Edge Cluster Size The OpenShift cluster is made of 3 Nodes combining Master/Workers for the Edge/Factory cluster.
Node Type Number of nodes Cloud Provider Instance Type Master/Worker 3 Google Cloud n1-standard-8 Master/Worker 3 Amazon Cloud Services m5.2xlarge Master/Worker 3 Microsoft Azure Standard_D8s_v3 Factory/Edge Cluster Utilization GCP
This is a snapshot of a Google Cloud Factory Edge cluster running the production Industrial-Edge pattern.
CPU Memory File System Network Pod Count 6.55 17.45 available of 24 43.19 GiB usage 45.09 GiB available of 88.28 GiB 48.45 GiB usage 334 GiB available of 382.5 GiB 9.64 MBps in15.79 MBps out 187 pods AWS
This is a snapshot of a Amazon Web Services Factory Edge cluster running the production Industrial-Edge pattern.
CPU Memory File System Network Pod Count 5.1 18.9 available of 24 42.91 GiB 49.27 GiB available of 92.18 GiB 51.54 GiB 308 GiB available of 359.5 GiB 9.41 MBps in 10.38 MBps out 194 pods Azure
This is a snapshot of an Azure Factory Edge cluster running the production Industrial-Edge pattern.
CPU Memory File System Network Pod Count 7.86 15.65 available of 24 42.76 Gib used 51.15 GiB available of 94.2 GiB 71.29 GiB used 2.93 TiB available of 3 TiB 8.98 MBps in 9.64 MBps out 192 *pods AWS Instance Types The industrial-edge pattern was tested with the highlighted AWS instances in bold. The OpenShift installer will let you know if the instance type meets the minimum requirements for a cluster.
The message that the openshift installer will give you will be similar to this message
INFO Credentials loaded from default AWS environment variables FATAL failed to fetch Metadata: failed to load asset &#34;Install Config&#34;: [controlPlane.platform.aws.type: Invalid value: &#34;m4.large&#34;: instance type does not meet minimum resource requirements of 4 vCPUs, controlPlane.platform.aws.type: Invalid value: &#34;m4.large&#34;: instance type does not meet minimum resource requirements of 16384 MiB Memory] Below you can find a list of the AWS instance types that can be used to deploy the industrial-edge pattern.
Instance type Default vCPUs Memory (GiB) Datacenter Factory/Edge 3x3 OCP Cluster 3 Node OCP Cluster m4.xlarge 4 16 N N m4.2xlarge 8 32 Y Y m4.4xlarge 16 64 Y Y m4.10xlarge 40 160 Y Y m4.16xlarge 64 256 Y Y m5.xlarge 4 16 Y N m5.2xlarge 8 32 Y Y m5.4xlarge 16 64 Y Y m5.8xlarge 32 128 Y Y m5.12xlarge 48 192 Y Y m5.16xlarge 64 256 Y Y m5.24xlarge 96 384 Y Y The OpenShift cluster is made of 4 Masters and 3 Workers for the Datacenter and the Edge/Factory cluster are made of 3 Master/Worker nodes. For the node sizes we used the m5.xlarge on AWS and this instance type met the minimum requirements to deploy the industrial-edge pattern successfully on the Datacenter hub. On the Factory/Edge cluster we used the m5.2xlarge since the minimum cluster was comprised of 3 nodes. .
To understand better what types of nodes you can use on other Cloud Providers we provide some of the details below.
Azure Instance Types The industrial-edge pattern was also deployed on Azure using the Standard_D8s_v3 VM size. Below is a table of different VM sizes available for Azure. Keep in mind that due to limited access to Azure we only used the Standard_D8s_v3 VM size.
The OpenShift cluster is made of 3 Master and 3 Workers for the Datacenter cluster.
The OpenShift cluster is made of 3 Nodes combining Master/Workers for the Edge/Factory cluster.
Type Sizes Description General purpose B, Dsv3, Dv3, Dasv4, Dav4, DSv2, Dv2, Av2, DC, DCv2, Dv4, Dsv4, Ddv4, Ddsv4 Balanced CPU-to-memory ratio. Ideal for testing and development, small to medium databases, and low to medium traffic web servers. Compute optimized F, Fs, Fsv2, FX High CPU-to-memory ratio. Good for medium traffic web servers, network appliances, batch processes, and application servers. Memory optimized Esv3, Ev3, Easv4, Eav4, Ev4, Esv4, Edv4, Edsv4, Mv2, M, DSv2, Dv2 High memory-to-CPU ratio. Great for relational database servers, medium to large caches, and in-memory analytics. Storage optimized Lsv2 High disk throughput and IO ideal for Big Data, SQL, NoSQL databases, data warehousing and large transactional databases. GPU NC, NCv2, NCv3, NCasT4_v3, ND, NDv2, NV, NVv3, NVv4 Specialized virtual machines targeted for heavy graphic rendering and video editing, as well as model training and inferencing (ND) with deep learning. Available with single or multiple GPUs. High performance compute HB, HBv2, HBv3, HC, H Our fastest and most powerful CPU virtual machines with optional high-throughput network interfaces (RDMA). For more information please refer to the Azure VM Size Page.
Google Cloud (GCP) Instance Types The industrial-edge pattern was also deployed on GCP using the n1-standard-8 VM size. Below is a table of different VM sizes available for GCP. Keep in mind that due to limited access to GCP we only used the n1-standard-8 VM size.
The OpenShift cluster is made of 3 Master and 3 Workers for the Datacenter cluster.
The OpenShift cluster is made of 3 Nodes combining Master/Workers for the Edge/Factory cluster.
The following table provides VM recommendations for different workloads.
| General purpose | Workload optimized
Cost-optimized Balanced Scale-out optimized Memory-optimized Compute-optimized Accelerator-optimized E2 N2, N2D, N1 T2D M2, M1 C2 A2 Day-to-day computing at a lower cost Balanced price/performance across a wide range of VM shapes Best performance/cost for scale-out workloads Ultra high-memory workloads Ultra high performance for compute-intensive workloads Optimized for high performance computing workloads For more information please refer to the GCP VM Size Page.
`,url:"https://validatedpatterns.io/patterns/industrial-edge/cluster-sizing/",breadcrumb:"/patterns/industrial-edge/cluster-sizing/"},"https://validatedpatterns.io/patterns/retail/cluster-sizing/":{title:"Cluster Sizing",tags:[],content:`OpenShift Cluster Sizing for the Retail Pattern Tested Platforms The retail pattern has been tested in the following Certified Cloud Providers.
Certified Cloud Providers 4.10 Amazon Web Services :heavy_check_mark: Microsoft Azure Google Cloud Platform General OpenShift Minimum Requirements OpenShift 4 has the following minimum requirements for sizing of nodes:
Minimum 4 vCPU (additional are strongly recommended). Minimum 16 GB RAM (additional memory is strongly recommended, especially if etcd is colocated on masters). Minimum 40 GB hard disk space for the file system containing /var/. Minimum 1 GB hard disk space for the file system containing /usr/local/bin/. There are several applications that comprise the retail pattern. In addition, the retail pattern also includes a number of supporting operators that are installed by OpenShift GitOps using ArgoCD.
Retail Pattern OpenShift Datacenter HUB Cluster Size The retail pattern has been tested with a defined set of specifically tested configurations that represent the most common combinations that Red Hat OpenShift Container Platform (OCP) customers are using or deploying for the x86_64 architecture.
The Datacenter HUB OpenShift Cluster is made up of the the following on the AWS deployment tested:
Node Type Number of nodes Cloud Provider Instance Type Master 3 Amazon Web Services m5.xlarge Worker 3 Amazon Web Services m5.xlarge The Datacenter HUB OpenShift cluster needs to be a bit bigger than the Factory/Edge clusters because this is where the developers will be running pipelines to build and deploy the Industrial Edge pattern on the cluster. The above cluster sizing is close to a minimum size for a Datacenter HUB cluster. In the next few sections we take some snapshots of the cluster utilization while the Industrial Edge pattern is running. Keep in mind that resources will have to be added as more developers are working building their applications.
Retail Pattern OpenShift Store Edge Cluster Size The OpenShift cluster is made of 3 Nodes combining Master/Workers for the Edge/Factory cluster.
Node Type Number of nodes Cloud Provider Instance Type Master/Worker 3 Google Cloud n1-standard-8 Master/Worker 3 Amazon Cloud Services m5.2xlarge Master/Worker 3 Microsoft Azure Standard_D8s_v3 AWS Instance Types The retail pattern was tested with the highlighted AWS instances in bold. The OpenShift installer will let you know if the instance type meets the minimum requirements for a cluster.
The message that the openshift installer will give you will be similar to this message
INFO Credentials loaded from default AWS environment variables FATAL failed to fetch Metadata: failed to load asset &#34;Install Config&#34;: [controlPlane.platform.aws.type: Invalid value: &#34;m4.large&#34;: instance type does not meet minimum resource requirements of 4 vCPUs, controlPlane.platform.aws.type: Invalid value: &#34;m4.large&#34;: instance type does not meet minimum resource requirements of 16384 MiB Memory] Below you can find a list of the AWS instance types that can be used to deploy the retail pattern.
Instance type Default vCPUs Memory (GiB) Datacenter Factory/Edge 3x3 OCP Cluster 3 Node OCP Cluster m4.xlarge 4 16 N N m4.2xlarge 8 32 Y Y m4.4xlarge 16 64 Y Y m4.10xlarge 40 160 Y Y m4.16xlarge 64 256 Y Y m5.xlarge 4 16 Y N m5.2xlarge 8 32 Y Y m5.4xlarge 16 64 Y Y m5.8xlarge 32 128 Y Y m5.12xlarge 48 192 Y Y m5.16xlarge 64 256 Y Y m5.24xlarge 96 384 Y Y The OpenShift cluster is made of 3 Masters and 3 Workers for the Datacenter and the Edge/Factory cluster are made of 3 Master/Worker nodes. For the node sizes we used the m5.xlarge on AWS and this instance type met the minimum requirements to deploy the retail pattern successfully on the Datacenter hub. On the Factory/Edge cluster we used the m5.2xlarge since the minimum cluster was comprised of 3 nodes. .
To understand better what types of nodes you can use on other Cloud Providers we provide some of the details below.
Azure Instance Types The retail pattern was also deployed on Azure using the Standard_D8s_v3 VM size. Below is a table of different VM sizes available for Azure. Keep in mind that due to limited access to Azure we only used the Standard_D8s_v3 VM size.
The OpenShift cluster is made of 3 Master and 3 Workers for the Datacenter cluster.
The OpenShift cluster is made of 3 Nodes combining Master/Workers for the Edge/Factory cluster.
Type Sizes Description General purpose B, Dsv3, Dv3, Dasv4, Dav4, DSv2, Dv2, Av2, DC, DCv2, Dv4, Dsv4, Ddv4, Ddsv4 Balanced CPU-to-memory ratio. Ideal for testing and development, small to medium databases, and low to medium traffic web servers. Compute optimized F, Fs, Fsv2, FX High CPU-to-memory ratio. Good for medium traffic web servers, network appliances, batch processes, and application servers. Memory optimized Esv3, Ev3, Easv4, Eav4, Ev4, Esv4, Edv4, Edsv4, Mv2, M, DSv2, Dv2 High memory-to-CPU ratio. Great for relational database servers, medium to large caches, and in-memory analytics. Storage optimized Lsv2 High disk throughput and IO ideal for Big Data, SQL, NoSQL databases, data warehousing and large transactional databases. GPU NC, NCv2, NCv3, NCasT4_v3, ND, NDv2, NV, NVv3, NVv4 Specialized virtual machines targeted for heavy graphic rendering and video editing, as well as model training and inferencing (ND) with deep learning. Available with single or multiple GPUs. High performance compute HB, HBv2, HBv3, HC, H Our fastest and most powerful CPU virtual machines with optional high-throughput network interfaces (RDMA). For more information please refer to the Azure VM Size Page.
Google Cloud (GCP) Instance Types The retail pattern was also deployed on GCP using the n1-standard-8 VM size. Below is a table of different VM sizes available for GCP. Keep in mind that due to limited access to GCP we only used the n1-standard-8 VM size.
The OpenShift cluster is made of 3 Master and 3 Workers for the Datacenter cluster.
The OpenShift cluster is made of 3 Nodes combining Master/Workers for the Edge/Factory cluster.
The following table provides VM recommendations for different workloads.
| General purpose | Workload optimized
Cost-optimized Balanced Scale-out optimized Memory-optimized Compute-optimized Accelerator-optimized E2 N2, N2D, N1 T2D M2, M1 C2 A2 Day-to-day computing at a lower cost Balanced price/performance across a wide range of VM shapes Best performance/cost for scale-out workloads Ultra high-memory workloads Ultra high performance for compute-intensive workloads Optimized for high performance computing workloads For more information please refer to the GCP VM Size Page.
`,url:"https://validatedpatterns.io/patterns/retail/cluster-sizing/",breadcrumb:"/patterns/retail/cluster-sizing/"},"https://validatedpatterns.io/patterns/medical-diagnosis/ideas-for-customization/":{title:"Ideas for Customization",tags:[],content:` One of the major goals of the Red Hat patterns development process is to create modular, customizable demos. The medical diagnosis pattern is just an example of how AI/ML workloads built for object detection and classification can be run on top of OpenShift clusters. Consider your workloads for a moment - how would your workload best consume the pattern framework? Do your consumers require on-demand or near real time responses when using your application? Is your application processing images or data that is protected by either Government Privacy Laws or HIPAA? The medical diagnosis pattern has the ability to answer the call to either of these requirements via OpenShift Serverless and OpenShift Data Foundations.
What are some different ways that I could use this pattern? The medical-diagnosis pattern is scanning X-Ray images to determine the probability that a patient may or may not have Pneumonia. Continuing with the medical path, the pattern could be used for other early detection scenarios that utilize object detection and classification. For example, the pattern could be used to scan C/T images for anomalies in the body such Sepsis, Cancer or even benign tumors. Additionally, the pattern could be used for detecting blood clots, some heart disease as well as bowel disorders like Crohn’s disease.
The Transportation Security Agency (TSA) could use the medical-diagnosis pattern in a way that enhances their existing scanning capabilities to detect with a higher probability restricted items carried on a person or hidden away in a piece of luggage. With MLOps the model is constantly training and learning to better detect those items that are dangerous but aren’t necessarily metallic such as a firearm or knife. The model is also training to dismiss those items that are authorized ultimately saving us from being stopped and searched at security checkpoints!
Militaries could use images collected from drones, satellites or other platforms to identify objects and determine with probability what that object is. For example, the model could be trained to determine a type of ship, potentially its country of origin and other identifying characteristics.
Manufacturing companies could use the pattern to inspect finished products as they roll off a production line. An image of the item, including using different types of light, could be analyzed to help expose defects before packaging and distributing. The item could be routed to a defect area.
Summary These are just a few ideas to help get the creative juices flowing for how you could use the medical-diagnosis pattern as a framework for your application.
Help &amp; Feedback Report Bugs
`,url:"https://validatedpatterns.io/patterns/medical-diagnosis/ideas-for-customization/",breadcrumb:"/patterns/medical-diagnosis/ideas-for-customization/"},"https://validatedpatterns.io/patterns/ansible-edge-gitops/openshift-virtualization/":{title:"OpenShift Virtualization",tags:[],content:`OpenShift Virtualization Understanding the Edge GitOps VMs Helm Chart The heart of the Edge GitOps VMs helm chart is a template file that was designed with a fair amount of flexibility in mind. Specifically, it allows you to specify:
One or more &ldquo;groups&rdquo; of VMs (such as &ldquo;kiosk&rdquo; in our example) with an arbitrary number of instances per group Different sizing parameters (cores, threads, memory, disk size) for each group Different SSH keypair credentials for each group Different OS&rsquo;s for each group Different sets of TCP and/or UDP ports open for each group This is to allow you to set up, for example, 4 VMs of one type, 3 VMs of another, and 2 VMs of a third type. This will hopefully abstract the details of VM creation through OpenShift Virtualization and allow you to focus on what kinds and how many of the different sorts of VMs you might need to set up. (Note that AWS&rsquo;s smallest metal node is 72 cores and 192 GB of RAM at initial release, so there is plenty of room for different combinations/configurations.)
How we got here - Default OpenShift Virtualization templates OpenShift virtualization expects to install virtual machines from image templates by default, and provides a number of OpenShift templates to facilitate this. The default templates are installed in the openshift namespace; the OpenShift console also provides a wizard for creating VMs that use the same templates.
As of OpenShift Virtualization 4.10.1, the following templates were available on installation:
$ oc get template NAME DESCRIPTION PARAMETERS OBJECTS 3scale-gateway 3scale&#39;s APIcast is an NGINX based API gateway used to integrate your interna... 17 (8 blank) 3 amq63-basic Application template for JBoss A-MQ brokers. These can be deployed as standal... 11 (4 blank) 6 amq63-persistent An example JBoss A-MQ application. For more information about using this temp... 13 (4 blank) 8 amq63-persistent-ssl An example JBoss A-MQ application. For more information about using this temp... 18 (6 blank) 12 amq63-ssl An example JBoss A-MQ application. For more information about using this temp... 16 (6 blank) 10 apicurito Design beautiful, functional APIs with zero coding, using a visual designer f... 7 (1 blank) 7 cache-service Red Hat Data Grid is an in-memory, distributed key/value store. 8 (1 blank) 4 cakephp-mysql-example An example CakePHP application with a MySQL database. For more information ab... 21 (4 blank) 8 cakephp-mysql-persistent An example CakePHP application with a MySQL database. For more information ab... 22 (4 blank) 9 centos-stream8-desktop-large Template for CentOS Stream 8 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream8-desktop-medium Template for CentOS Stream 8 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream8-desktop-small Template for CentOS Stream 8 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream8-desktop-tiny Template for CentOS Stream 8 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream8-server-large Template for CentOS Stream 8 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream8-server-medium Template for CentOS Stream 8 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream8-server-small Template for CentOS Stream 8 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream8-server-tiny Template for CentOS Stream 8 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream9-desktop-large Template for CentOS Stream 9 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream9-desktop-medium Template for CentOS Stream 9 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream9-desktop-small Template for CentOS Stream 9 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream9-desktop-tiny Template for CentOS Stream 9 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream9-server-large Template for CentOS Stream 9 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream9-server-medium Template for CentOS Stream 9 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream9-server-small Template for CentOS Stream 9 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos-stream9-server-tiny Template for CentOS Stream 9 VM or newer. A PVC with the CentOS Stream disk i... 4 (2 generated) 1 centos7-desktop-large Template for CentOS 7 VM or newer. A PVC with the CentOS disk image must be a... 4 (2 generated) 1 centos7-desktop-medium Template for CentOS 7 VM or newer. A PVC with the CentOS disk image must be a... 4 (2 generated) 1 centos7-desktop-small Template for CentOS 7 VM or newer. A PVC with the CentOS disk image must be a... 4 (2 generated) 1 centos7-desktop-tiny Template for CentOS 7 VM or newer. A PVC with the CentOS disk image must be a... 4 (2 generated) 1 centos7-server-large Template for CentOS 7 VM or newer. A PVC with the CentOS disk image must be a... 4 (2 generated) 1 centos7-server-medium Template for CentOS 7 VM or newer. A PVC with the CentOS disk image must be a... 4 (2 generated) 1 centos7-server-small Template for CentOS 7 VM or newer. A PVC with the CentOS disk image must be a... 4 (2 generated) 1 centos7-server-tiny Template for CentOS 7 VM or newer. A PVC with the CentOS disk image must be a... 4 (2 generated) 1 dancer-mysql-example An example Dancer application with a MySQL database. For more information abo... 18 (5 blank) 8 dancer-mysql-persistent An example Dancer application with a MySQL database. For more information abo... 19 (5 blank) 9 datagrid-service Red Hat Data Grid is an in-memory, distributed key/value store. 7 (1 blank) 4 datavirt64-basic-s2i Application template for JBoss Data Virtualization 6.4 services built using S2I. 20 (6 blank) 6 datavirt64-extensions-support-s2i An example JBoss Data Virtualization application. For more information about... 35 (9 blank) 10 datavirt64-ldap-s2i Application template for JBoss Data Virtualization 6.4 services that configur... 21 (6 blank) 6 datavirt64-secure-s2i An example JBoss Data Virtualization application. For more information about... 51 (22 blank) 8 decisionserver64-amq-s2i An example BRMS decision server A-MQ application. For more information about... 30 (5 blank) 10 decisionserver64-basic-s2i Application template for Red Hat JBoss BRMS 6.4 decision server applications... 17 (5 blank) 5 django-psql-example An example Django application with a PostgreSQL database. For more informatio... 19 (5 blank) 8 django-psql-persistent An example Django application with a PostgreSQL database. For more informatio... 20 (5 blank) 9 eap-xp3-basic-s2i Example of an application based on JBoss EAP XP. For more information about u... 20 (5 blank) 8 eap74-basic-s2i An example JBoss Enterprise Application Platform application. For more inform... 20 (5 blank) 8 eap74-https-s2i An example JBoss Enterprise Application Platform application configured with... 30 (11 blank) 10 eap74-sso-s2i An example JBoss Enterprise Application Platform application Single Sign-On a... 50 (21 blank) 10 fedora-desktop-large Template for Fedora 34 VM or newer. A PVC with the Fedora disk image must be... 4 (2 generated) 1 fedora-desktop-medium Template for Fedora 34 VM or newer. A PVC with the Fedora disk image must be... 4 (2 generated) 1 fedora-desktop-small Template for Fedora 34 VM or newer. A PVC with the Fedora disk image must be... 4 (2 generated) 1 fedora-desktop-tiny Template for Fedora 34 VM or newer. A PVC with the Fedora disk image must be... 4 (2 generated) 1 fedora-highperformance-large Template for Fedora 34 VM or newer. A PVC with the Fedora disk image must be... 4 (2 generated) 1 fedora-highperformance-medium Template for Fedora 34 VM or newer. A PVC with the Fedora disk image must be... 4 (2 generated) 1 fedora-highperformance-small Template for Fedora 34 VM or newer. A PVC with the Fedora disk image must be... 4 (2 generated) 1 fedora-highperformance-tiny Template for Fedora 34 VM or newer. A PVC with the Fedora disk image must be... 4 (2 generated) 1 fedora-server-large Template for Fedora 34 VM or newer. A PVC with the Fedora disk image must be... 4 (2 generated) 1 fedora-server-medium Template for Fedora 34 VM or newer. A PVC with the Fedora disk image must be... 4 (2 generated) 1 fedora-server-small Template for Fedora 34 VM or newer. A PVC with the Fedora disk image must be... 4 (2 generated) 1 fedora-server-tiny Template for Fedora 34 VM or newer. A PVC with the Fedora disk image must be... 4 (2 generated) 1 fuse710-console The Red Hat Fuse Console eases the discovery and management of Fuse applicati... 8 (1 blank) 5 httpd-example An example Apache HTTP Server (httpd) application that serves static content.... 9 (3 blank) 5 jenkins-ephemeral Jenkins service, without persistent storage.... 11 (all set) 7 jenkins-ephemeral-monitored Jenkins service, without persistent storage. ... 12 (all set) 8 jenkins-persistent Jenkins service, with persistent storage.... 13 (all set) 8 jenkins-persistent-monitored Jenkins service, with persistent storage. ... 14 (all set) 9 jws31-tomcat7-basic-s2i Application template for JWS applications built using S2I. 12 (3 blank) 5 jws31-tomcat7-https-s2i An example JBoss Web Server application configured for use with https. For mo... 17 (5 blank) 7 jws31-tomcat8-basic-s2i An example JBoss Web Server application. For more information about using thi... 12 (3 blank) 5 jws31-tomcat8-https-s2i An example JBoss Web Server application. For more information about using thi... 17 (5 blank) 7 jws56-openjdk11-tomcat9-ubi8-basic-s2i An example JBoss Web Server application. For more information about using thi... 10 (3 blank) 5 jws56-openjdk11-tomcat9-ubi8-https-s2i An example JBoss Web Server application. For more information about using thi... 15 (5 blank) 7 jws56-openjdk8-tomcat9-ubi8-basic-s2i An example JBoss Web Server application. For more information about using thi... 10 (3 blank) 5 jws56-openjdk8-tomcat9-ubi8-https-s2i An example JBoss Web Server application. For more information about using thi... 15 (5 blank) 7 mariadb-ephemeral MariaDB database service, without persistent storage. For more information ab... 8 (3 generated) 3 mariadb-persistent MariaDB database service, with persistent storage. For more information about... 9 (3 generated) 4 mysql-ephemeral MySQL database service, without persistent storage. For more information abou... 8 (3 generated) 3 mysql-persistent MySQL database service, with persistent storage. For more information about u... 9 (3 generated) 4 nginx-example An example Nginx HTTP server and a reverse proxy (nginx) application that ser... 10 (3 blank) 5 nodejs-postgresql-example An example Node.js application with a PostgreSQL database. For more informati... 18 (4 blank) 8 nodejs-postgresql-persistent An example Node.js application with a PostgreSQL database. For more informati... 19 (4 blank) 9 openjdk-web-basic-s2i An example Java application using OpenJDK. For more information about using t... 9 (1 blank) 5 postgresql-ephemeral PostgreSQL database service, without persistent storage. For more information... 7 (2 generated) 3 postgresql-persistent PostgreSQL database service, with persistent storage. For more information ab... 8 (2 generated) 4 processserver64-amq-mysql-persistent-s2i An example BPM Suite application with A-MQ and a MySQL database. For more inf... 49 (13 blank) 14 processserver64-amq-mysql-s2i An example BPM Suite application with A-MQ and a MySQL database. For more inf... 47 (13 blank) 12 processserver64-amq-postgresql-persistent-s2i An example BPM Suite application with A-MQ and a PostgreSQL database. For mor... 46 (10 blank) 14 processserver64-amq-postgresql-s2i An example BPM Suite application with A-MQ and a PostgreSQL database. For mor... 44 (10 blank) 12 processserver64-basic-s2i An example BPM Suite application. For more information about using this templ... 17 (5 blank) 5 processserver64-externaldb-s2i An example BPM Suite application with a external database. For more informati... 47 (22 blank) 7 processserver64-mysql-persistent-s2i An example BPM Suite application with a MySQL database. For more information... 40 (14 blank) 10 processserver64-mysql-s2i An example BPM Suite application with a MySQL database. For more information... 39 (14 blank) 9 processserver64-postgresql-persistent-s2i An example BPM Suite application with a PostgreSQL database. For more informa... 37 (11 blank) 10 rails-pgsql-persistent An example Rails application with a PostgreSQL database. For more information... 21 (4 blank) 9 rails-postgresql-example An example Rails application with a PostgreSQL database. For more information... 20 (4 blank) 8 redis-ephemeral Redis in-memory data structure store, without persistent storage. For more in... 5 (1 generated) 3 redis-persistent Redis in-memory data structure store, with persistent storage. For more infor... 6 (1 generated) 4 rhdm711-authoring Application template for a non-HA persistent authoring environment, for Red H... 76 (46 blank) 11 rhdm711-authoring-ha Application template for a HA persistent authoring environment, for Red Hat D... 92 (47 blank) 17 rhdm711-kieserver Application template for a managed KIE Server, for Red Hat Decision Manager 7... 61 (42 blank) 6 rhdm711-prod-immutable-kieserver Application template for an immutable KIE Server in a production environment,... 66 (45 blank) 8 rhdm711-prod-immutable-kieserver-amq Application template for an immutable KIE Server in a production environment... 80 (54 blank) 20 rhdm711-trial-ephemeral Application template for an ephemeral authoring and testing environment, for... 63 (40 blank) 8 rhel6-desktop-large Template for Red Hat Enterprise Linux 6 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel6-desktop-medium Template for Red Hat Enterprise Linux 6 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel6-desktop-small Template for Red Hat Enterprise Linux 6 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel6-desktop-tiny Template for Red Hat Enterprise Linux 6 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel6-server-large Template for Red Hat Enterprise Linux 6 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel6-server-medium Template for Red Hat Enterprise Linux 6 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel6-server-small Template for Red Hat Enterprise Linux 6 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel6-server-tiny Template for Red Hat Enterprise Linux 6 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel7-desktop-large Template for Red Hat Enterprise Linux 7 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel7-desktop-medium Template for Red Hat Enterprise Linux 7 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel7-desktop-small Template for Red Hat Enterprise Linux 7 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel7-desktop-tiny Template for Red Hat Enterprise Linux 7 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel7-highperformance-large Template for Red Hat Enterprise Linux 7 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel7-highperformance-medium Template for Red Hat Enterprise Linux 7 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel7-highperformance-small Template for Red Hat Enterprise Linux 7 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel7-highperformance-tiny Template for Red Hat Enterprise Linux 7 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel7-server-large Template for Red Hat Enterprise Linux 7 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel7-server-medium Template for Red Hat Enterprise Linux 7 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel7-server-small Template for Red Hat Enterprise Linux 7 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel7-server-tiny Template for Red Hat Enterprise Linux 7 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel8-desktop-large Template for Red Hat Enterprise Linux 8 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel8-desktop-medium Template for Red Hat Enterprise Linux 8 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel8-desktop-small Template for Red Hat Enterprise Linux 8 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel8-desktop-tiny Template for Red Hat Enterprise Linux 8 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel8-highperformance-large Template for Red Hat Enterprise Linux 8 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel8-highperformance-medium Template for Red Hat Enterprise Linux 8 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel8-highperformance-small Template for Red Hat Enterprise Linux 8 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel8-highperformance-tiny Template for Red Hat Enterprise Linux 8 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel8-server-large Template for Red Hat Enterprise Linux 8 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel8-server-medium Template for Red Hat Enterprise Linux 8 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel8-server-small Template for Red Hat Enterprise Linux 8 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel8-server-tiny Template for Red Hat Enterprise Linux 8 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel9-desktop-large Template for Red Hat Enterprise Linux 9 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel9-desktop-medium Template for Red Hat Enterprise Linux 9 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel9-desktop-small Template for Red Hat Enterprise Linux 9 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel9-desktop-tiny Template for Red Hat Enterprise Linux 9 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel9-highperformance-large Template for Red Hat Enterprise Linux 9 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel9-highperformance-medium Template for Red Hat Enterprise Linux 9 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel9-highperformance-small Template for Red Hat Enterprise Linux 9 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel9-highperformance-tiny Template for Red Hat Enterprise Linux 9 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel9-server-large Template for Red Hat Enterprise Linux 9 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel9-server-medium Template for Red Hat Enterprise Linux 9 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel9-server-small Template for Red Hat Enterprise Linux 9 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhel9-server-tiny Template for Red Hat Enterprise Linux 9 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 rhpam711-authoring Application template for a non-HA persistent authoring environment, for Red H... 80 (46 blank) 12 rhpam711-authoring-ha Application template for a HA persistent authoring environment, for Red Hat P... 101 (47 blank) 20 rhpam711-kieserver-externaldb Application template for a managed KIE Server with an external database, for... 83 (59 blank) 8 rhpam711-kieserver-mysql Application template for a managed KIE Server with a MySQL database, for Red... 70 (42 blank) 9 rhpam711-kieserver-postgresql Application template for a managed KIE Server with a PostgreSQL database, for... 71 (42 blank) 9 rhpam711-managed Application template for a managed HA production runtime environment, for Red... 87 (46 blank) 14 rhpam711-prod Application template for a managed HA production runtime environment, for Red... 102 (55 blank) 28 rhpam711-prod-immutable-kieserver Application template for an immutable KIE Server in a production environment,... 76 (45 blank) 11 rhpam711-prod-immutable-kieserver-amq Application template for an immutable KIE Server in a production environment... 97 (58 blank) 23 rhpam711-prod-immutable-monitor Application template for a router and monitoring console in a production envi... 66 (44 blank) 14 rhpam711-trial-ephemeral Application template for an ephemeral authoring and testing environment, for... 63 (40 blank) 8 s2i-fuse710-spring-boot-2-camel Spring Boot 2 and Camel QuickStart. This example demonstrates how you can use... 18 (3 blank) 3 s2i-fuse710-spring-boot-2-camel-rest-3scale Spring Boot 2, Camel REST DSL and 3Scale QuickStart. This example demonstrate... 19 (3 blank) 5 s2i-fuse710-spring-boot-2-camel-xml Spring Boot 2 and Camel Xml QuickStart. This example demonstrates how you can... 18 (3 blank) 3 sso72-https An example RH-SSO 7 application. For more information about using this templa... 26 (15 blank) 6 sso72-mysql An example RH-SSO 7 application with a MySQL database. For more information a... 36 (20 blank) 8 sso72-mysql-persistent An example RH-SSO 7 application with a MySQL database. For more information a... 37 (20 blank) 9 sso72-postgresql An example RH-SSO 7 application with a PostgreSQL database. For more informat... 33 (17 blank) 8 sso72-postgresql-persistent An example RH-SSO 7 application with a PostgreSQL database. For more informat... 34 (17 blank) 9 sso73-https An example application based on RH-SSO 7.3 image. For more information about... 27 (16 blank) 6 sso73-mysql An example application based on RH-SSO 7.3 image. For more information about... 37 (21 blank) 8 sso73-mysql-persistent An example application based on RH-SSO 7.3 image. For more information about... 38 (21 blank) 9 sso73-ocp4-x509-https An example application based on RH-SSO 7.3 image. For more information about... 13 (7 blank) 5 sso73-ocp4-x509-mysql-persistent An example application based on RH-SSO 7.3 image. For more information about... 24 (12 blank) 8 sso73-ocp4-x509-postgresql-persistent An example application based on RH-SSO 7.3 image. For more information about... 21 (9 blank) 8 sso73-postgresql An example application based on RH-SSO 7.3 image. For more information about... 34 (18 blank) 8 sso73-postgresql-persistent An example application based on RH-SSO 7.3 image. For more information about... 35 (18 blank) 9 sso74-https An example application based on RH-SSO 7.4 on OpenJDK image. For more informa... 27 (16 blank) 6 sso74-ocp4-x509-https An example application based on RH-SSO 7.4 on OpenJDK image. For more informa... 13 (7 blank) 5 sso74-ocp4-x509-postgresql-persistent An example application based on RH-SSO 7.4 on OpenJDK image. For more informa... 21 (9 blank) 8 sso74-postgresql An example application based on RH-SSO 7.4 on OpenJDK image. For more informa... 34 (18 blank) 8 sso74-postgresql-persistent An example application based on RH-SSO 7.4 on OpenJDK image. For more informa... 35 (18 blank) 9 sso75-https An example application based on RH-SSO 7.5 on OpenJDK image. For more informa... 27 (16 blank) 6 sso75-ocp4-x509-https An example application based on RH-SSO 7.5 on OpenJDK image. For more informa... 13 (7 blank) 5 sso75-ocp4-x509-postgresql-persistent An example application based on RH-SSO 7.5 on OpenJDK image. For more informa... 21 (9 blank) 8 sso75-postgresql An example application based on RH-SSO 7.5 on OpenJDK image. For more informa... 34 (18 blank) 8 sso75-postgresql-persistent An example application based on RH-SSO 7.5 on OpenJDK image. For more informa... 35 (18 blank) 9 windows10-desktop-large Template for Microsoft Windows 10 VM. A PVC with the Windows disk image must... 3 (1 generated) 1 windows10-desktop-medium Template for Microsoft Windows 10 VM. A PVC with the Windows disk image must... 3 (1 generated) 1 windows10-highperformance-large Template for Microsoft Windows 10 VM. A PVC with the Windows disk image must... 3 (1 generated) 1 windows10-highperformance-medium Template for Microsoft Windows 10 VM. A PVC with the Windows disk image must... 3 (1 generated) 1 windows2k12r2-highperformance-large Template for Microsoft Windows Server 2012 R2 VM. A PVC with the Windows disk... 3 (1 generated) 1 windows2k12r2-highperformance-medium Template for Microsoft Windows Server 2012 R2 VM. A PVC with the Windows disk... 3 (1 generated) 1 windows2k12r2-server-large Template for Microsoft Windows Server 2012 R2 VM. A PVC with the Windows disk... 3 (1 generated) 1 windows2k12r2-server-medium Template for Microsoft Windows Server 2012 R2 VM. A PVC with the Windows disk... 3 (1 generated) 1 windows2k16-highperformance-large Template for Microsoft Windows Server 2016 VM. A PVC with the Windows disk im... 3 (1 generated) 1 windows2k16-highperformance-medium Template for Microsoft Windows Server 2016 VM. A PVC with the Windows disk im... 3 (1 generated) 1 windows2k16-server-large Template for Microsoft Windows Server 2016 VM. A PVC with the Windows disk im... 3 (1 generated) 1 windows2k16-server-medium Template for Microsoft Windows Server 2016 VM. A PVC with the Windows disk im... 3 (1 generated) 1 windows2k19-highperformance-large Template for Microsoft Windows Server 2019 VM. A PVC with the Windows disk im... 3 (1 generated) 1 windows2k19-highperformance-medium Template for Microsoft Windows Server 2019 VM. A PVC with the Windows disk im... 3 (1 generated) 1 windows2k19-server-large Template for Microsoft Windows Server 2019 VM. A PVC with the Windows disk im... 3 (1 generated) 1 windows2k19-server-medium Template for Microsoft Windows Server 2019 VM. A PVC with the Windows disk im... 3 (1 generated) 1 Additionally, you may copy and customize these templates if you wish. The template file is an example of a customized template that was used to help develop this pattern.
Creating a VM from the Console via Template These templates can be run through the OpenShift Console from the Virtualization tab. Note the &ldquo;Create VM&rdquo; buttons on the right side of this picture:
Clicking on the &ldquo;Create VM&rdquo; button will bring up a wizard that looks like this:
Accepting the defaults from this wizard will give a success screen:
Until it is deleted, you can monitor the machine&rsquo;s lifecycle from the VirtualMachines tab:
This is a great way to gain familiarity with how the system works, but we might possibly want an interface we can use more programmatically.
Creating a VM from the command line via oc process This is a useful way to understand what kinds of objects OpenShift Virtualization creates and manages:
$ oc process -n openshift rhel8-desktop-medium | oc apply -f - virtualmachine.kubevirt.io/rhel8-q63yuvxpjdvy18l7 created You could also use the &ldquo;Create VM Wizard&rdquo; in the OpenShift console.
Another option - capturing template output and converting it into a Helm Chart See details here.
Components of the virtual-machines template Setup - the mechanism for creating identifiers declaratively The first part of the template file sets up some variables that we will use later as the template is expanded. We use a sequential numbering scheme for VM name creation because that is an easy way to make each item in the set declarative - it ensures that if you ask for 5 VMs of a particular type, they will have predictable names, and if one is deleted, it will be replaced by a VM with the same name.
We use explicit &ldquo;range&rdquo; variables for the Go templating. This is because the implicit range variable is easily &ldquo;trampled&rdquo;, and we have at least two different dimensions to iterate on - vm &ldquo;role&rdquo; and &ldquo;index&rdquo; within that role.
The External Secret - SSH pubkey The first item we define as part of this structure is an external secret to hold an SSH pubkey. This pubkey will be mounted in the VM under an unprivileged user&rsquo;s home directory - and generally that unprivileged user is expected to be able to sudo root without password. By default, RHEL images are configured to only allow SSH access via pubkey. In this pattern, the private key and public key for the SSH connections are loaded into both Vault (which we inherited from previous patterns) and Ansible Automation Platform.
Since the keys are defined per VM &ldquo;group&rdquo;, it is possible and expected that you could have different keypairs for different groups of VMs. Nothing would prevent you from using the same keypair for all machines if you have different groups, though.
While the pubkey is not truly a &ldquo;secret&rdquo;, the availability of the External Secrets Operator made for a nice opportunity to allow for variance in configuration without necessarily requiring local customization of the pattern. The OpenShift Virtualization model has no way of knowing that multiple servers may have the same SSH credentials, and in fact cannot depend on this. So it creates a pubkey object by default for each VM, and we imitate this behavior in the pattern.
The VirtualMachine definition The VirtualMachine definition is the biggest part of the template. All of it is derived from customization of the default templates that OpenShift Virtualization installs in the openshift namespace - especially most of the labels and annotations, with the following exceptions:
labels app This is set to $identifier to match a general pattern with other applications.
edge-gitops-role This is set explicitly and used elsewhere in this pattern to help identify resources by role. The intention is to be able to use the edge-gitops-role as a selector for targeting various kind of queries, including (especially) Ansible inventories. Though please note - because of the way Kubernetes (and OpenShift) work, when you connect to a VM with Ansible you are connecting to the Service object directly, not to the VM. (Another way to look at it is that the Service object is providing network abstraction over the VM object.)
Other resources in the rest of the VirtualMachine definition are copied from the default template, with appropriate Helm variables included.
Initial user access Note that the initial user (default: cloud-user) and initial password are customizable via values overrides. The kiosk type shows an example of how to either use a user/password specific to the type or a default for the chart using the coalesce function.
The Service definition The Service definition is potentially complex. The purpose of this Service object is to expose all of the needed TCP and UDP network ports within the cluster. (Providing access to them from outside the cluster would require Route or Ingress objects, and would have some significant security implications; access to these entities from outside the cluster is not the focus of this pattern, so we do not provide it at this time.)
A given VM may expose one port (for Ansible access, you need at least TCP/22), or it may expose many ports. You are free to define a service per port if you like, but it seems more convenient to define them all as a single service.
One aspect of the templating you may find interesting is the use of the toPrettyJson filter in Go. Since YAML is a proper superset of JSON, this is a neat trick that allows to include a nested data structure without having to worry about how to indent it. (As toPrettyJson uses the square bracket ([]) and curly bracket ({}) notation for arrays and hashes, YAML can interpret it without worrying about its indentation.
Accessing the VMs There are three mechanisms for access to these VMs:
Ansible - keypair authentication The ssh keypairs from your values-secret.yaml are loaded into both Vault and AAP for use later. The pattern currently defines one such keypair, kiosk-ssh, but could support more, such as iot-ssh, gateway-ssh, etc. more details on how to expand on this pattern are described below.
AAP only needs the private key and the username as a machine credential. The public key is not truly a secret, but it seemed interesting and useful to use the external secret operator to associate the public key with VM instances this way and prevent having to diverge from the upstream pattern to include local ssh pubkey specifications.
Note that the default SSH setting for RHEL does not allow password-based logins via SSH, and it&rsquo;s at the very least inconvenient to copy the SSH private key into a VM inside the cluster, so the typical way the keypair will be used is through Ansible.
Virtual Machine Console Access via OpenShift Console Navigate to Virtualization -&gt; VirtualMachines and make sure Project: All Projects or edge-gitops-vms is selected:
Click on the &ldquo;three dots&rdquo; menu on the right, which will open a dialog like the following:
Note: In OpenShift Virtualization 4.11, the &ldquo;Open Console&rdquo; option appears when you click on the virtual machine name in openshift console. The dialog looks like this:
The virtual machine console view will either show a standard RHEL console login screen, or if the demo is working as designed, it will show the Ignition application running in kiosk mode. If the console shows a standard RHEL login, it can be accessed using the the initial user name (cloud-user by default) and password (which is what is specified in the Helm chart Values as either the password specific to that machine group, the default cloudInit, or a hardcoded default which can be seen in the template here. On a VM created through the wizard or via oc process from a template, the password will be set on the VirtualMachine object in the volumes section.
Initial User login (cloud-user) In general, and before the VMs have been configured by the Ansible Jobs, you can log in to the VMs on the console using the user and password you specified in the Helm chart, or else you can look at the VirtualMachine object and see what the username and password setting are. The pattern, by design, replaces the typical console view with Firefox running in kiosk mode. But this mechanism can still be used if you change the console from &ldquo;VNC Console&rdquo; to &ldquo;Serial Console&rdquo;.
The &ldquo;extra&rdquo; VM Template Also included in the edge-gitops-vms chart is a separate template that will allow the creation of VMs with similar (though not identical characteristics) to the ones defined in the chart.
The rhel8-kiosk-with-svc template is preserved as an intermediate step to creating your own VM types, to see how the pipeline from default VM template -&gt; customized template -&gt; Helm-variable chart can work.
Next Steps Help &amp; Feedback Report Bugs `,url:"https://validatedpatterns.io/patterns/ansible-edge-gitops/openshift-virtualization/",breadcrumb:"/patterns/ansible-edge-gitops/openshift-virtualization/"},"https://validatedpatterns.io/patterns/multicloud-gitops/mcg-ideas-for-customization/":{title:"Ideas for customization",tags:[],content:` About customizing a pattern One of the major goals of the Validated Patterns development process is to create modular, customizable demos. The Multicloud Gitops is just an example of a pattern managing multiple clusters in a GitOps fashion. It contains a very simple config-demo application, which prints out a secret that was injected into the vault through an out-of-band mechanism.
You can customize this demo in different ways.
Split the config-demo across hub and regional clusters Currently hub and regional clusters are reusing the exact same helm chart found at charts/all/config-demo. The first customization step could be to split the demo app in two separate charts: one in charts/hub/config-demo and one in charts/region/config-demo. Once charts/all/config-demo has been copied to charts/hub/config-demo and charts/region/config-demo, you need to include them in the respective values-hub.yaml and values-region-one.yaml, respectively.
After completing this configuration, you can start customizing the two apps and make them output a different web page entirely depending if the pod is running on the hub or on the cluster.
Rest API addition After splitting the charts, you could implement a small REST API server on the hub. This API could serve read-only values out to anyone and could provide some update write-APIs only if the client provides a secret, for example, by using the X-API-KEY mechanism. You can tweak the config-demo application to communicate to the hub and use a vault-injected secret as the X-API-KEY. So the hub would possess the key through the External-Secrets generated kubernetes secret and the regional app would possess that same secret via the RHACM policy pushing out secrets via the {{hub fromSecret}} mechanism.
`,url:"https://validatedpatterns.io/patterns/multicloud-gitops/mcg-ideas-for-customization/",breadcrumb:"/patterns/multicloud-gitops/mcg-ideas-for-customization/"},"https://validatedpatterns.io/patterns/ansible-edge-gitops/ideas-for-customization/":{title:"Ideas for Customization",tags:[],content:`Ideas for Customization Why change it? One of the major goals of the Red Hat patterns development process is to create modular, customizable demos. Maybe you are not interested in Ignition as an application, or you do not have kiosks&hellip;but you do have other use cases that involve running containers on edge devices. Maybe you want to experiment with different releases of RHEL, or you want to do something different with Ansible Automation Platform.
This demo in particular can be customized in a number of ways that might be very interesting - and here are some starter ideas with some instructions on exactly what and where changes would need to be made in the pattern to accommodate those changes.
HOWTO define your own VM sets using the chart Either fork the repo or copy the edge-gitops-vms chart out of it.
Customize the values.yaml file
The vms data structure is designed to support multiple groups and types of VMs. The kiosk example defines all of the variables currently supported by the chart, including references to the Vault instance and port definitions. If, for example, you wanted to replace kiosk with new iotsensor and iotgateway types, the whole file might look like this:
--- secretStore: name: vault-backend kind: ClusterSecretStore cloudInit: defaultUser: &#39;cloud-user&#39; defaultPassword: &#39;6toh-n1d5-9xpq&#39; vms: iotsensor: count: 4 flavor: small workload: server os: rhel8 role: iotgateway storage: 20Gi memory: 2Gi cores: 1 sockets: 1 threads: 1 cloudInitUser: cloud-user cloudInitPassword: 6toh-n1d5-9xpq template: rhel8-server-small sshsecret: secret/data/hub/iotsensor-ssh sshpubkeyfield: publickey ports: - name: ssh port: 22 protocol: TCP targetPort: 22 iotgateway: count: 1 flavor: medium workload: server os: rhel8 role: iotgateway storage: 30Gi memory: 4Gi cores: 1 sockets: 1 threads: 1 cloudInitUser: cloud-user cloudInitPassword: 6toh-n1d5-9xpq template: rhel8-server-medium sshsecret: secret/data/hub/iotgateway-ssh sshpubkeyfield: publickey ports: - name: ssh port: 22 protocol: TCP targetPort: 22 - name: mqtt port: 1883 protocol: TCP targetPort: 1883 This would create 1 iotgateway VM and 4 iotsensor VMs. Adjustments would also need to be made in values-secret and ansible-load-controller to add the iotgateway-ssh and iotsensor-ssh data structures.
HOWTO define your own VM sets &ldquo;from scratch&rdquo; Pick a default template from the standard OpenShift Virtualization template library in the openshift namespace. For this pattern, we used rhel8-desktop-medium: $ oc get template -n openshift rhel8-desktop-medium NAME DESCRIPTION PARAMETERS OBJECTS rhel8-desktop-medium Template for Red Hat Enterprise Linux 8 VM or newer. A PVC with the RHEL disk... 4 (2 generated) 1 It might help to create a VM through the command line template process, and see what objects OpenShift Virtualization creates to bring that VM up: To see the actual JSON that the template converts into:
$ oc process -n openshift rhel8-desktop-medium { &#34;kind&#34;: &#34;List&#34;, &#34;apiVersion&#34;: &#34;v1&#34;, &#34;metadata&#34;: {}, &#34;items&#34;: [ { &#34;apiVersion&#34;: &#34;kubevirt.io/v1&#34;, &#34;kind&#34;: &#34;VirtualMachine&#34;, &#34;metadata&#34;: { &#34;annotations&#34;: { &#34;vm.kubevirt.io/validations&#34;: &#34;[\\n {\\n \\&#34;name\\&#34;: \\&#34;minimal-required-memory\\&#34;,\\n \\&#34;path\\&#34;: \\&#34;jsonpath::.spec.domain.resources.requests.memory\\&#34;,\\n \\&#34;rule\\&#34;: \\&#34;integer\\&#34;,\\n \\&#34;message\\&#34;: \\&#34;This VM requires more memory.\\&#34;,\\n \\&#34;min\\&#34;: 1610612736\\n }\\n]\\n&#34; }, &#34;labels&#34;: { &#34;app&#34;: &#34;rhel8-yywa22lijw8hl017&#34;, &#34;vm.kubevirt.io/template&#34;: &#34;rhel8-desktop-medium&#34;, &#34;vm.kubevirt.io/template.revision&#34;: &#34;1&#34;, &#34;vm.kubevirt.io/template.version&#34;: &#34;v0.19.5&#34; }, &#34;name&#34;: &#34;rhel8-yywa22lijw8hl017&#34; }, &#34;spec&#34;: { &#34;dataVolumeTemplates&#34;: [ { &#34;apiVersion&#34;: &#34;cdi.kubevirt.io/v1beta1&#34;, &#34;kind&#34;: &#34;DataVolume&#34;, &#34;metadata&#34;: { &#34;name&#34;: &#34;rhel8-yywa22lijw8hl017&#34; }, &#34;spec&#34;: { &#34;sourceRef&#34;: { &#34;kind&#34;: &#34;DataSource&#34;, &#34;name&#34;: &#34;rhel8&#34;, &#34;namespace&#34;: &#34;openshift-virtualization-os-images&#34; }, &#34;storage&#34;: { &#34;resources&#34;: { &#34;requests&#34;: { &#34;storage&#34;: &#34;30Gi&#34; } } } } } ], &#34;running&#34;: false, &#34;template&#34;: { &#34;metadata&#34;: { &#34;annotations&#34;: { &#34;vm.kubevirt.io/flavor&#34;: &#34;medium&#34;, &#34;vm.kubevirt.io/os&#34;: &#34;rhel8&#34;, &#34;vm.kubevirt.io/workload&#34;: &#34;desktop&#34; }, &#34;labels&#34;: { &#34;kubevirt.io/domain&#34;: &#34;rhel8-yywa22lijw8hl017&#34;, &#34;kubevirt.io/size&#34;: &#34;medium&#34; } }, &#34;spec&#34;: { &#34;domain&#34;: { &#34;cpu&#34;: { &#34;cores&#34;: 1, &#34;sockets&#34;: 1, &#34;threads&#34;: 1 }, &#34;devices&#34;: { &#34;disks&#34;: [ { &#34;disk&#34;: { &#34;bus&#34;: &#34;virtio&#34; }, &#34;name&#34;: &#34;rhel8-yywa22lijw8hl017&#34; }, { &#34;disk&#34;: { &#34;bus&#34;: &#34;virtio&#34; }, &#34;name&#34;: &#34;cloudinitdisk&#34; } ], &#34;inputs&#34;: [ { &#34;bus&#34;: &#34;virtio&#34;, &#34;name&#34;: &#34;tablet&#34;, &#34;type&#34;: &#34;tablet&#34; } ], &#34;interfaces&#34;: [ { &#34;masquerade&#34;: {}, &#34;name&#34;: &#34;default&#34; } ], &#34;networkInterfaceMultiqueue&#34;: true, &#34;rng&#34;: {} }, &#34;machine&#34;: { &#34;type&#34;: &#34;pc-q35-rhel8.4.0&#34; }, &#34;resources&#34;: { &#34;requests&#34;: { &#34;memory&#34;: &#34;4Gi&#34; } } }, &#34;evictionStrategy&#34;: &#34;LiveMigrate&#34;, &#34;networks&#34;: [ { &#34;name&#34;: &#34;default&#34;, &#34;pod&#34;: {} } ], &#34;terminationGracePeriodSeconds&#34;: 180, &#34;volumes&#34;: [ { &#34;dataVolume&#34;: { &#34;name&#34;: &#34;rhel8-yywa22lijw8hl017&#34; }, &#34;name&#34;: &#34;rhel8-yywa22lijw8hl017&#34; }, { &#34;cloudInitNoCloud&#34;: { &#34;userData&#34;: &#34;#cloud-config\\nuser: cloud-user\\npassword: nnpa-12td-e0r7\\nchpasswd: { expire: False }&#34; }, &#34;name&#34;: &#34;cloudinitdisk&#34; } ] } } } } ] } And to use the template to create a VM:
oc process -n openshift rhel8-desktop-medium | oc apply -f - virtualmachine.kubevirt.io/rhel8-q63yuvxpjdvy18l7 created In just a few minutes, you will have a blank rhel8 VM running, which you can then login to (via console) and customize.
Get the details of this template as a local YAML file: oc get template -n openshift rhel8-desktop-medium -o yaml &gt; my-template.yaml Once you have this local template, you can view the elements you want to customize, possibly using this as an example.
HOWTO Define your own Ansible Controller Configuration The ansible_load_controller.sh is designed to be relatively easy to customize with a new controller configuration. Structurally, it is principally based on configure_controller.yml from the Red Hat Community of Practice controller_configuration collection. The order and specific list of roles invoked is taken from there.
To customize it, the main thing would be to replace the different variables in the role tasks with the your own. The script includes the roles for variable types that this pattern does not manage in order to make that part straightforward. Feel free to add your own roles and playbooks (and add them to the controller configuration script).
The reason this pattern ships with a script as it does instead of invoking the referenced playbook directly is that several of the configuration elements depend on each other, and there was not a super-convenient place to put things like the controller credentials as the playbook suggests.
HOWTO substitute your own container application (instead of ignition) Adjust the query in the inventory_preplay.yml either by overriding the vars for the play, or forking the repo and replacing the vars with your own query terms. (That is, use your own label(s) and namespace to discover the services you want to connect to.
Adjust or override the vars in the provision_kiosk.yml playbook to suitable values for your own container application. The roles it calls are fairly generic, so changing the vars is all you should need to do.
Next Steps Help &amp; Feedback Report Bugs `,url:"https://validatedpatterns.io/patterns/ansible-edge-gitops/ideas-for-customization/",breadcrumb:"/patterns/ansible-edge-gitops/ideas-for-customization/"},"https://validatedpatterns.io/patterns/devsecops/ideas-for-customization/":{title:"Ideas for Customization",tags:[],content:`Ideas for Customization More desirable tools in the development pipeline One of the major goals of the Red Hat patterns development process is to create modular, customizable demos. Maybe there is a tool in the CI/CD pipeline that you&rsquo;d like to substitute for a preferred tool. E.g. using Clair in Quay to do image scanning instead of RH ACS. Or Quay Enterprise may be removed for another image repository.
How to add a new tool to the pipeline In the region directory make a new directory that will house the Helm chart for your tool.
Follow the guide on how to extend a pattern
Deploying development and hub components to one cluster In some environments the organization may require a single cluster with different namespaces for development environments and hub environments. To achieve this you could combine the components in the development cluster group values-development.yaml file into the values-hub.yaml file.
Things to consider. While OpenShift Pipelines needs to move to the values-hub.yaml file, the Quay bridge and ACS integration for pipeline scanning is not required. I.e. some of the plumbing needed to connect pipelines to various artifacts can be removed. Policies that are used by ACM move secrets to the development cluster would not be needed.
Different production environments While this can be done with any of the patterns the Multicluster DevSecOps pattern is about building and deploying developed code. There maybe a variety of places where a deployment could land in production. Consider a smart city application. Various types of cluster groups could be used in production - a cluster group for traffic light applications, a cluster group for electric tram cars, a cluster group for smart road signs.
values-traffic-lights.yaml
values-tram-cars.yaml
values-smart-signs.yaml
GitOps and DevSecOps would be used to make sure that applications would be deployed on the correct clusters. Some of the &ldquo;clusters&rdquo; might be light single-node clusters. Some applications be be deployed to several cluster groups. E.g. the application to place information on a smart sign might also be deployed to the tram cars that also have smart signs in passenger compartments or the engineers compartment.
`,url:"https://validatedpatterns.io/patterns/devsecops/ideas-for-customization/",breadcrumb:"/patterns/devsecops/ideas-for-customization/"},"https://validatedpatterns.io/patterns/industrial-edge/ideas-for-customization/":{title:"Ideas for Customization",tags:[],content:`Ideas for Customization Why change it? One of the major goals of the Red Hat patterns development process is to create modular, customizable demos. The Industrial Edge demonstration includes multiple, simulated, IoT devices publishing their temperature and vibration telemetry to our data center and ultimately persisting the data into an AWS S3 storage service bucket which we call the Data Lake. All of this is done using our Red Hat certified products running on OpenShift.
This demo in particular can be customized in a number of ways that might be very interesting - and here are some starter ideas with some instructions on exactly what and where changes would need to be made in the pattern to accommodate those changes.
HOWTO Forking the Industrial Edge repository to your github account Hopefully we are all familiar with GitHub. If you are not GitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere. Our Industrial Edge GitOps repository is available in our Hybrid Cloud Patterns GitHub organization.
To fork this repository, and deploy the Industrial Edge pattern, follow the steps found in our Getting Started section. This will allow you to follow the next few HOWTO guides in this section.
Our sensors have been configured to send data relating to the vibration of the devices. To show the power of GitOps, and keeping state in a git repository, we can make a change to the config map of one of the sensors to detect and report data on temperature. This is done via a variable called SENSOR_TEMPERATURE_ENABLED that is initially set to false. Setting this variable to true will trigger the GitOps engine to synchronize the application, restart the machine sensor and apply the change.
There are two environments in the Industrial Edge demonstration:
The staging environment that lives in the manuela-tst-all namespace The production environment which lives in the stormshift namespaces As an operator you would first make changes to the staging first. Here are the steps to see how the GitOps engine does it&rsquo;s magic. These changes will be reflected in the staging environment Line Dashboard UI in the manuela-tst-all namespace.
The config maps in question live in the charts/datacenter/manuela-tst/templates/machine-sensor directory There are two config maps that we can change: machine-sensor-1-configmap.yaml machine-sensor-2-configmap.yaml Change the following variable in machine-sensor-1-configmap.yaml SENSOR_TEMPERATURE_ENABLED: &ldquo;true&rdquo; Make sure you commit the changes to git git add machine-sensor-1-configmap.yaml git commit -m &ldquo;Changed SENSOR_TEMPERATURE_ENABLED to true&rdquo; git push Now you can go to the Line Dashboard application and see how the UI shows the temperature for that device. You can find the route link by: Change the Project context to manuela-tst-all Navigate to Networking-&gt;Routes Press on the Location link to see navigate to the UI. HOWTO Applying the pattern to a new use case There are a lot of IoT devices that we could add to this pattern. In today&rsquo;s world we have IoT devices that perform different functions and these devices are connected to a network where they have the ability of sending telemetry data to other devices or a central data center. In this particular use case we address an Industrial sector but what about applying this use case to other sectors such as Automotive or Delivery service companies?
If we take the Deliver Service use case, and apply it to this pattern, we would have to take into account the following aspects:
The main components in the pattern architecture can be used as is. The broker and kafka components are the vehicles for the streaming data coming from the devices. The IoT sensor software would have to be developed. The IoT devices will now be mobile so that presents a few challenges tracking the devices in part due to spotty connectivity to send the data stream. The number of IoT devices to be tracked will increase depending on the fleet of delivery trucks out in the field. Scalability will be an important aspect for the pattern to be able to handle. A new AI/ML model would have to be developed to &ldquo;learn&rdquo; through the analysis of the data stream from the IoT devices. The idea is that this pattern can be used for other use cases keeping the main components in place. The components that would be new to the pattern are: IoT device code, AI/ML models, and specific kafka/broker topics to keep track of.
Next Steps What ideas for customization do you have? Can you use this pattern for other use cases? Let us know through our feedback link below.
Help &amp; Feedback{: .btn .fs-5 .mb-4 .mb-md-0 .mr-2 } Report Bugs{: .btn .btn-red .fs-5 .mb-4 .mb-md-0 .mr-2 }
`,url:"https://validatedpatterns.io/patterns/industrial-edge/ideas-for-customization/",breadcrumb:"/patterns/industrial-edge/ideas-for-customization/"},"https://validatedpatterns.io/patterns/multicloud-gitops-portworx/ideas-for-customization/":{title:"Ideas for Customization",tags:[],content:`Ideas for customization Why change it? One of the major goals of the Red Hat patterns development process is to create modular, customizable demos. The Multicloud Gitops is just an example of a pattern managing multiple clusters in a gitops fashion. It contains a very simple &lsquo;config-demo&rsquo; application which prints out a secret that was injected into the vault via an out-of-band mechanism.
It could be an interesting exercise to customize this demo in different ways.
Split the config-demo across hub and regional clusters Currently hub and regional clusters are reusing the exact same helm chart found at charts/all/config-demo. The first customization step could be to split the demo app in two separate charts: one in charts/hub/config-demo and one in charts/region/config-demo. Once charts/all/config-demo has been copied to charts/hub/config-demo and charts/region/config-demo, we need to include them in the respective values-hub.yaml and values-region-one.yaml, respectively.
Once this is done we can start customizing the two apps and make them output a different web page entirely depending if the pod is running on the hub or on the cluster.
Rest API addition Another idea, after splitting the charts, could be to implement a small Rest API server on the hub. This API could serve read-only values out to anyone and could provide some update write-APIs only if the client provides a secret (for example via the X-API-KEY mechanism). The config-demo application could be tweaked to talk to the hub and use a vault-injected secret as the X-API-KEY. So the hub would possess the key via the External-Secrets generated K8s secret and the regional app would possess that same secret via the ACM policy pushing out secrets via the {{hub fromSecret}} mechanism.
In the end the possibilities to tweak this pattern are endless. Do let us know if you have an awesome idea that you&rsquo;d like to add
Contribute to this pattern: Help &amp; Feedback{: .btn .fs-5 .mb-4 .mb-md-0 .mr-2 } Report Bugs{: .btn .btn-red .fs-5 .mb-4 .mb-md-0 .mr-2 }
`,url:"https://validatedpatterns.io/patterns/multicloud-gitops-portworx/ideas-for-customization/",breadcrumb:"/patterns/multicloud-gitops-portworx/ideas-for-customization/"},"https://validatedpatterns.io/patterns/retail/ideas-for-customization/":{title:"Ideas for Customization",tags:[],content:"Ideas for Customization ",url:"https://validatedpatterns.io/patterns/retail/ideas-for-customization/",breadcrumb:"/patterns/retail/ideas-for-customization/"},"https://validatedpatterns.io/learn/secrets/":{title:"Secrets",tags:[],content:`Infrastructure Background Enterprise applications require security, especially in multi-cluster and multi-site environments. Applications require trust and use certificates and other secrets in order to establish and maintain trust. In this section we will look at various ways of managing secrets.
When you start developing distributed enterprise applications there is a strong temptation to ignore security during development and add it at the end. This is proven to be a very bad practice that accumulates technical debt that sometimes never gets resolved.
While the DevOps model of development strongly encourages shifting security to the left many developers didn&rsquo;t really take notice and so the more explicit term DevSecOps was created. Essentially, &ldquo;pay attention and consider and implement security as early as possible in the lifecycle&rdquo;. (i.e. shift left on the time line).
Secret Management One area that has been impacted by a more automated approach to security is in the secret management. DevOps (and DevSecOps) environments require the use of many different services:
Code repositories GitOps tools Image repositories Build pipelines All of these services require credentials. (Or should do!) And keeping those credentials secret is very important. E.g. pushing your credentials to your personal GitHub/GitLab repository is not a secure solution.
While using a file based secret management can work if done correctly, most organizations opt for a more enterprise solution using a secret management product or project. The Cloud Native Computing Foundation (CNCF) has many such projects. The Hybrid Cloud Patterns project has started with Hashicorp Vault secret management product but we look forward to other project contributions.
What&rsquo;s next? Getting started with Vault
`,url:"https://validatedpatterns.io/learn/secrets/",breadcrumb:"/learn/secrets/"},"https://validatedpatterns.io/learn/vault/":{title:"HashiCorp Vault",tags:[],content:`Deploying HashiCorp Vault in a validated pattern Prerequisites You have deployed/installed a validated pattern using the instructions provided for that pattern. This should include setting having logged into the cluster using oc login or setting you KUBECONFIG environment variable and running a make install.
Setting up HashiCorp Vault Any validated pattern that uses HashiCorp Vault already has deployed Vault as part of the make install. To verify that Vault is installed you can first see that the vault project exists and then select the Workloads/Pods:
In order to setup HashiCorp Vault there are two different ways, both of which happen automatically as part of the make install command:
Inside the cluster directly when the helm value clusterGroup.insecureUnsealVaultInsideCluster is set to true. With this method a cronjob will run every five minutes inside the imperative namespace and unseal, initialize and configure the vault. The vault&rsquo;s unseal keys and root token will be stored inside a secret called vaultkeys in the imperative namespace. It is considered best practice to copy the content of that secret offline, store it securely and then delete it. On the user&rsquo;s computer when the helm value clusterGroup.insecureUnsealVaultInsideCluster is set to false. This will store the json containing containing both vault root token and unseal keys inside a file called common/pattern-vault.init. It is recommended to encrypt this file or store it securely. An example output is the following:
{ &#34;recovery_keys_b64&#34;: [], &#34;recovery_keys_hex&#34;: [], &#34;recovery_keys_shares&#34;: 0, &#34;recovery_keys_threshold&#34;: 0, &#34;root_token&#34;: &#34;hvs.VNFq7yPuZljq2VDJTkgAMs2Z&#34;, &#34;unseal_keys_b64&#34;: [ &#34;+JJjKgZyEB1rbKlXs1aTuC+PBivukIlnpoe7bH4qc7TL&#34;, &#34;X2ib6LNZw+kOQH1WYR9t3RE2SgB5WbEf2FfD40OybNXf&#34;, &#34;A4DIhv9atLIQsqqyDAYkmfEJPYhFVuKGSGYwV7WCtGcL&#34;, &#34;ZWkQ7+qtgmClKdlNKWcdpvyxArm07P9eArHZB4/CMZWn&#34;, &#34;HXakF073+Kk7oOpAFbGlKIWYApzUhC/F1LDfowF/M1LK&#34; ], &#34;unseal_keys_hex&#34;: [ &#34;f892632a0672101d6b6ca957b35693b82f8f062bee908967a687bb6c7e2a73b4cb&#34;, &#34;5f689be8b359c3e90e407d56611f6ddd11364a007959b11fd857c3e343b26cd5df&#34;, &#34;0380c886ff5ab4b210b2aab20c062499f1093d884556e28648663057b582b4670b&#34;, &#34;656910efeaad8260a529d94d29671da6fcb102b9b4ecff5e02b1d9078fc23195a7&#34;, &#34;1d76a4174ef7f8a93ba0ea4015b1a5288598029cd4842fc5d4b0dfa3017f3352ca&#34; ], &#34;unseal_shares&#34;: 5, &#34;unseal_threshold&#34;: 3 } The vault&rsquo;s root token is needed to log into the vault&rsquo;s UI and the unseal keys are needed whenever the vault pods are restarted. In the OpenShift console click on the nine box at the top and click on the vault line: []
Copy the root_token field which in the example above has the value hvs.VNFq7yPuZljq2VDJTkgAMs2Z and paste it in the sign-in page:
After signing in you will see the secrets that have been created.
Unseal If you don&rsquo;t see the sign in page but instead see an unseal page, something may have happened the cluster and you need to unseal it again. Instead of using make vault-init you should run make vault-unseal. You can also unseal it manually by running vault operator unseal inside the vault-0 pod in the vault namespace.
What&rsquo;s next? Check with the validated pattern instructions to see if there are further steps you need to perform. Sometimes this might be deploying a pattern on an edge cluster and checking to see if the correct Vault handshaking and updating occurs.
`,url:"https://validatedpatterns.io/learn/vault/",breadcrumb:"/learn/vault/"},"https://validatedpatterns.io/patterns/ansible-edge-gitops/troubleshooting/":{title:"Troubleshooting",tags:[],content:`Troubleshooting Our Issue Tracker Please file an issue if you see a problem!
`,url:"https://validatedpatterns.io/patterns/ansible-edge-gitops/troubleshooting/",breadcrumb:"/patterns/ansible-edge-gitops/troubleshooting/"},"https://validatedpatterns.io/learn/faq/":{title:"FAQ",tags:[],content:`FAQ What is a Hybrid Cloud Pattern? Hybrid Cloud Patterns are collections of applications (in the ArgoCD sense) that demonstrate aspects of hub/edge computing that seem interesting and useful. Hybrid Cloud Patterns will generally have a hub or centralized component, and an edge component. These will interact in different ways.
Many things have changed in the IT landscape in the last few years - containers and kubernetes have taken the industry by storm, but they introduce many technologies and concepts. It is not always clear how these technologies and concepts play together - and Hybrid Cloud Patterns is our effort to show these technologies working together on non-trivial applications in ways that make sense for real customers and partners to use.
The first Hybrid Cloud Pattern is based on MANUela, an application developed by Red Hat field associates. This application highlights some interesting aspects of the industrial edge in a cloud-native world - the hub component features pipelines to build the application, a &ldquo;twin&rdquo; for testing purposes, a central data lake, an s3 component to gather data from the edge installations (which are factories in this case). The edge component has machine sensors, which are responsible for only gathering data from instrumented line devices and shares them via MQTT messaging. The edge also features Seldon, an AI/ML framework for making predictions, a custom Node.js application to show data in real time, and messaging components supporting both MQTT and Kafka protocols. The local applications use MQTT to retrieve data for display, and the Kafka components move the data to the central hub for storage and analysis.
We are actively developing new Hybrid Cloud Patterns. Watch this space for updates!
How are they different from XYZ? Many technology demos can be very minimal - such demos have an important place in the ecosystem to demonstrate the intent of an individual technology. Hybrid Cloud Patterns are meant to demonstrate groups of technologies working together in a cloud native way. And yet, we hope to make these patterns general enough to allow for swapping application components out &ndash; for example, if you want to swap out ActiveMQ for RabbitMQ to support MQTT - or use a different messaging technology altogether, that should be possible. The other components will require reconfiguration.
What technologies are used? Key technologies in the stack for Industrial Edge include:
Red Hat OpenShift Container Platform Red Hat Advanced Cluster Management Red Hat OpenShift GitOps (based on ArgoCD) Red Hat OpenShift Pipelines (based on tekton) Red Hat Integration - AMQ Broker (ActiveMQ Artemis MQTT) Red Hat Integration - AMQ Streams (Kafka) Red Hat Integration - Camel K Seldon Operator In the future, we expect to further use Red Hat OpenShift, and expand the integrations with other elements of the ecosystem. How can the concept of GitOps integrate with a fleet of devices that are not running Kubernetes? What about integrations with baremetal or VM servers? Sounds like a job for Ansible! We expect to tackle some of these problems in future patterns.
How are they structured? Hybrid Cloud Patterns come in parts - we have a common repository with logic that will apply to multiple patterns. Layered on top of that is our first pattern - industrial edge. This layout allows for individual applications within a pattern to be swapped out by pointing to different repositories or branches for those individual components by customizing the values files in the root of the repository to point to different branches or forks or even different repositories entirely. (At present, the repositories all have to be on github.com and accessible with the same token.)
The common repository is primarily concerned with how to deploy the GitOps operator, and to create the namespaces that will be necessary to manage the pattern applications.
The pattern repository has the application-specific layout, and determines which components are installed in which places - hub or edge. The pattern repository also defines the hub and edge locations. Both the hub and edge are expected to have multiple components each - the hub will have pipelines and the CI/CD framework, as well as any centralization components or data analysis components. Edge components are designed to be smaller as we do not need to deploy Pipelines or the test and staging areas to the Edge.
Each application is described as a series of resources that are rendered into GitOps (ArgoCD) via Helm and Kustomize. The values for these charts are set by values files that need to be &ldquo;personalized&rdquo; (with your local cluster values) as the first step of installation. Subsequent pushes to the gitops repository will be reflected in the clusters running the applications.
Who is behind this? Today, a team of Red Hat engineers including Andrew Beekhof (@beekhof), Lester Claudio (@claudiol), Martin Jackson (@mhjacks), William Henry (@ipbabble), Michele Baldessari (@mbaldessari), Jonny Rickard (@day0hero) and others.
Excited or intrigued by what you see here? We&rsquo;d love to hear your thoughts and ideas! Try the patterns contained here and see below for links to our repositories and issue trackers.
How can I get involved? Try out what we&rsquo;ve done and submit issues to our issue trackers.
We will review pull requests to our pattern repositories.
`,url:"https://validatedpatterns.io/learn/faq/",breadcrumb:"/learn/faq/"},"https://validatedpatterns.io/patterns/":{title:"Patterns",tags:[],content:`Browse through available patterns and their respective documentation for deployment and operation. Filter patterns by type, industry, and product.
`,url:"https://validatedpatterns.io/patterns/",breadcrumb:"/patterns/"},"https://validatedpatterns.io/patterns/retail/":{title:"Retail",tags:[],content:`Retail Pattern Background This pattern demonstrates a pattern that models the store side of a retail application.
It is derived from the Quarkus Coffeeshop Demo done by Red Hat Solution Architects. The demo models the use of multiple application microservices which use Kafka messaging to interact and a Postgres database to persist data. (There is a homeoffice analytics suite in the demo that we hope to include in a later version of the pattern.
This demo pulls together several different strands of the demo and allows for multiple stores to be installed on remote clusters via ACM if the user desires.
The demo allows users to go to the store&rsquo;s web page, order drinks and food items, and see those items &ldquo;made&rdquo; and served by the microservices in real time.
The pattern includes build pipelines and a demo space, so that changes to the applications can be tested prior to &ldquo;production&rdquo; deployments.
Solution elements How to use a GitOps approach to keep in control of configuration and operations How to centrally manage multiple clusters, including workloads How to build and deploy workloads across clusters using modern CI/CD How to architect a modern application using microservices and Kafka in Java Red Hat Technologies Red Hat OpenShift Container Platform (Kubernetes) Red Hat Advanced Cluster Management (Open Cluster Management) Red Hat OpenShift GitOps (ArgoCD) Red Hat OpenShift Pipelines (Tekton) Red Hat AMQ Streams (Apache Kafka Event Broker) Architecture The following diagram shows the relationship between the microservices, messaging, and database components:
The hub. This cluster hosts the CI/CD pipelines, a test instance of the applications and messaging/database services for testing purposes, and a single functional store. Optional remote clusters. Each remote site can support a complete store environment. The default one modelled is a &ldquo;RALEIGH&rdquo; store location. Demo Scenario The Retail Validated Pattern / Demo Scenario is focused in the Quarkus Coffeeshop retail experience. In a full retail environment, it would be easy to be overwhelmed by things like item files, tax tables, item movement/placement within the store and so on, so the demo does not attempt to model all those elements - instead offering a subset of services to give a sense of how data can flow in such a system, how microservices should interact (via API calls and message passing), and where data can be persisted.
In the future we hope to expand this pattern with the homeoffice components, to further demonstrate how data can flow from leaf nodes to centralized data analytics services, which are crucial in retail IT environments.
Web Service - the point of sale within the store. Shows the menu, and allows the user to order food and drinks, and shows when orders are ready. Counter service - the &ldquo;heart&rdquo; of the store operation - receives orders and dispatches them to the barista and kitchen services, as appropriate. Users may order as many food and drink items in one order as they wish. Barista - the service responsible for providing items from the &ldquo;drinks&rdquo; side of the menu. Kitchen - the service responsible for providing items from the &ldquo;food&rdquo; side of the menu. Further documentation on the individual services is available at the upstream Quarkus Coffeeshop documentation site.
`,url:"https://validatedpatterns.io/patterns/retail/",breadcrumb:"/patterns/retail/"},"https://validatedpatterns.io/":{title:"Validated Patterns",tags:[],content:`
Reference architectures with added value Validated Patterns are an evolution of how you deploy applications in a hybrid cloud. With a pattern, you can automatically deploy a full application stack through a GitOps-based framework. With this framework, you can create business-centric solutions while maintaining a level of Continuous Integration (CI) over your application.
`,url:"https://validatedpatterns.io/",breadcrumb:"/"},"https://validatedpatterns.io/blog/":{title:"Blog",tags:[],content:`Find out the latest news about Validated Patterns.
`,url:"https://validatedpatterns.io/blog/",breadcrumb:"/blog/"},"https://validatedpatterns.io/blog/2022-12-01-multicluster-devsecops/":{title:"Multicluster DevSecOps",tags:[],content:`Multicluster DevSecOps Software supply chain security: The why and what Today more and more organizations are turning to agile development models and DevOps. With this approach, development organizations can deliver more enhancements and bug fixes in a timely manner, providing more value to their customers. While DevOps can include security earlier in the software lifecycle, in practice this has not always been the case. DevSecOps explicitly calls on organizations to pay attention to security best practices and to automate them or “Shift Left” as much as possible.
DevSecOps means baking in application and infrastructure security from the start. In order to be successful, organizations must look both upstream where their dependencies come from, and also how their components integrate together in the production environment. It also means automating security gates to keep the DevOps workflow from slowing down. As we learn from experience, we codify that into the automation process.
A successful DevSecOps based supply chain must consider four areas of concern:
Secure developer dependencies Secure code development Secure deployment of resources into a secure environment Software Bill of Materials (SBOM) Within each of these areas there are also many best practices to be applied particularly in Cloud Native development using container technology.
Scanning new development code for potential vulnerabilities Scanning dependent images that new code will be layered upon Attesting to the veracity of images using image signing Scanning images for know CVEs Scanning the environment for potential networking vulnerabilities Scanning for misconfiguration of images and other assets Ensuring consistent automated deployment of secure configuration using GitOps Continuous upgrading of security policies from both trusted third parties and experience This pattern deploys several Red Hat Products:
Red Hat OpenShift Container Platform (Kubernetes platform) Red Hat OpenShift GitOps (ArgoCD) Red Hat Advanced Cluster Management (Open Cluster Management) Red Hat OpenShift Pipelines (Tekton) Red Hat Quay (OCI image registry with security features enabled) Red Hat Open Data Foundation (highly available storage) Red Hat Advanced Cluster Security (scanning and monitoring) Highlight: Multicluster While all of the components can be deployed on a single cluster, which makes for a simple demo, this pattern deploys a real world architecture where the central management, development environments, and production are all deployed on different clusters. This ensures that the pattern is structured for real-world deployments, with all the functionality needed to make such an architecture work already built-in, so that pattern consumers can concentrate on what is being delivered, rather than how.
The heavy lifting in the pattern includes a great deal of integration between components, especially those spanning across clusters:
Deployment of Quay Enterprise with OpenShift Data Foundations as a storage backend Deployment of Quay Bridge operator configured to connect with Quay Enterprise on hub cluster Deployment of ACS on managed nodes with integration back to ACS central on the hub Deployment of a secure pipeline with scanning and signing tools, including ACS Highlight: DevSecOps with Pipelines &ldquo;OpenShift Pipelines makes CI/CD concepts such as a &lsquo;pipeline&rsquo;, a &rsquo;task&rsquo;, a &lsquo;step&rsquo; natively instantiatable [sic] so it can use the scalability, security, ease of deployment capabilities of Kubernetes.&rdquo; (Introducing OpenShift Pipelines). The pattern consumes many of the OpenShift Pipelines out of the box tasks but also defines new tasks for scanning and signing and includes them in enhanced DevSecOps pipelines.
While these pipelines are included in the pattern, the pattern also implements the use of Pipelines-as-Code feature where the pipeline can be part of the application code repository. &ldquo;This allows developers to ship their CI/CD pipelines within the same git repository as their application, making it easier to keep both of them in sync in terms of release updates.&rdquo;
Highlight: Using the CI Pipeline to provide supply chain security This pattern includes some other technologies in the development CI pipeline, including cosign, a SIGSTORE project, implemented with Tekton Chains. Cosign supports container signing, verification, and storage in an OCI registry. It enables consumers to sign their pipeline resources and images and share the attestation files providing downstream consumers assurances that they are consuming a trusted artifact.
We also implement open source tools like Sonarqube for static code analysis, nexus for securely storing build artifacts in-cluster, and an open source reports application that is used to upload and present the reports from the security pipeline.
Not using these tools in your environment? That’s not a problem. The pattern framework is flexible. Organizations using different services can swap out what’s in the pattern with their software of choice to fit their environment.
Where do we go from here? This pattern provides a complete deployment solution for Multicluster DevSecOps that can be used as part of a supply chain deployment pattern across different industries.
Documentation for how to install the pattern is here, where there are detailed installation instructions and more technical details on the different components in the pattern.
`,url:"https://validatedpatterns.io/blog/2022-12-01-multicluster-devsecops/",breadcrumb:"/blog/2022-12-01-multicluster-devsecops/"},"https://validatedpatterns.io/blog/2022-11-20-argo-rollouts/":{title:"Progressive Delivery with Argo Rollouts",tags:[],content:`Progressive Delivery with Argo Rollouts Introduction Progressive delivery is about the controlled deployment of an application. It is by no means a new concept when it comes to software deployment and delivery. The concept (maybe not in name) has been around for quite a while, and if you’ve done application or system updates prior to kubernetes some of these concepts will sound familiar. The rollout resource is a direct replacement of the kubernetes deployment resource. This allows for very easy conversion of an existing deployment into a rollout resource.
Additionally, Argo Rollouts can use metrics from a number of providers (we&rsquo;ll be using the default - prometheus) which can be used to abort a rollout if there are issues with the application deployment such as failed health checks, pod restarts ..etc. Using metrics to control the rollout is beyond the scope of this blog, but will be part of a future blog.
OpenShift and OpenShift-GitOps do not officially support argo-rollouts to date, but support should be expected in 2023
Blue/Green is the concept of having two versions of the same application running at the same time so that we can verify that the application updates are behaving the way they are supposed to. The blue (or production) application doesn’t change at all and the green (preview/updated application) is deployed beside it. While this is a tried and true approach it does have some drawbacks. One significant drawback to this strategy is that you will need to have enough capacity to support both applications running simultaneously. This can be a major hurdle in resource-constrained environments, or with applications that require a license to operate.
Credits: argoproj.github.io
Canary is the more modern, more advanced approach to blue/green. With a canary deployment we deploy the new version of the application to a subset of our users, while the rest will continue with the original version. If there’s an issue with the new version, only that subset of users will be affected. With canary rollouts we can specify the percentage of traffic that gets allocated to the new application release as well as a timer for how long we want in between steps. This is ideal when you are trying to test a new feature and you want to gather metrics / data from live traffic.
Credits: argoproj.github.io
In this blog, we’re going to use OpenShift Gitops to deploy the Argo Rollouts progressive delivery controller and we’re going to walk through a blue/green deployment as well as a canary deployment.
Preparation Let’s start by deploying the argo-rollouts pattern from the argo-rollouts. For this demo, I have deployed a 3-Node compact cluster using m5.2xlarge machine types in AWS. This demo will only use rollouts to deploy onto a single cluster.
oc get nodes NAME STATUS ROLES AGE VERSION ip-10-0-137-28.us-east-2.compute.internal Ready master,worker 13h v1.24.6+5157800 ip-10-0-165-204.us-east-2.compute.internal Ready master,worker 13h v1.24.6+5157800 ip-10-0-206-142.us-east-2.compute.internal Ready master,worker 13h v1.24.6+5157800 oc get machines -n openshift-machine-api NAME PHASE TYPE REGION ZONE AGE argo-rollouts-7d9dd-master-0 Running m5.2xlarge us-east-2 us-east-2a 13h argo-rollouts-7d9dd-master-1 Running m5.2xlarge us-east-2 us-east-2b 13h argo-rollouts-7d9dd-master-2 Running m5.2xlarge us-east-2 us-east-2c 13h If you&rsquo;ve never deployed OpenShift before, you could try ROSA the pay-as-you-go OpenShift managed service.
Next, you&rsquo;ll need to create a fork of the argo-rollouts repo. Go there in a browser, make sure you’re logged in to GitHub, click the “Fork” button, and confirm the destination by clicking the big green &ldquo;Create fork&rdquo; button.
Next, install the Validated Patterns operator from Operator Hub.
And finally, click through to the installed operator, and select the Create instance button and fill out the Create a Pattern form. Most of the defaults are fine, but make sure you update the GitSpec URL to point to your fork of argo-rollouts, rather than https://github.com/hybrid-cloud-patterns/argo-rollouts.
To see what’s going on, click on “Installed Operators” and then change the Project to “All Projects”. After a bit, you will see the following operators installed: Advanced Cluster Manager for Kubernetes multicluster engine for kubernetes Red Hat OpenShift GitOps Package Server Validated Patterns Operator
Once everything is installed we need to clone our fork of the repository to our local machine. Go to your account in github and click the big green “code” button, and then click the “copy” icon to copy the url of the repository.
Switch over to your cli and type: git clone &lt;paste_the_url_just_copied&gt; ; next, change directories into the repository.
Optionally, the argo project provides a plugin for the kubectl which can be used to manage rollouts in our cluster. This is totally optional and not required, however it does make it easy to track progress of the rollout. To install follow the official install procedures.
Argo Rollouts In your copy of the repository, we can find the manifests that make up argo rollouts in charts/all/argo-rollouts/templates. Now we COULD use oc/kubectl create -f but that defeats the purpose of gitops! So we&rsquo;re going to use openshift-gitops and the validated pattern framework to deploy the argo rollouts controller for us.
If you are interested in understanding what each of the manifests are for, I encourage you to visit the argo rollouts architecture page which details each resource.
Let&rsquo;s review how the framework is deploying argo-rollouts for us. Take a look at values-hub.yaml to see how argo rollouts is declared:
First, we tell argocd to create the argo-rollouts namespace
namespaces: - open-cluster-management - vault - golang-external-secrets - argo-rollouts Next, we define a project, a project is an argocd resource that groups application resources together
projects: - hub - argo-rollouts Finally, we add a map for argo-rollouts where we define our application
applications: &lt;...omitted...&gt; argo-rollouts: name: argo-rollouts namespace: argo-rollouts project: argo-rollouts path: charts/all/argo-rollouts To watch the deployment in action, log in to your cluster console, and then select the “squared” drop down box and select “Hub ArgoCD”. After accepting the security warnings for self-signed certificates, in the ArgoCD login screen click “Login with OpenShift”, when prompted select “Allow user permissions”.
You are now in the ArgoCD console and can see the applications deployed (or being deployed). If you don’t see the rollouts application right away don’t fret, by default, ArgoCD’s reconciliation loop runs every 3 minutes.
After a few, you should see the following in your ArgoCD console.
With Argo Rollouts deployed, we can start using progressive delivery! Let’s start with blue/green!
Blue/Green When you use a blue/green deployment strategy you will have two instances of the application running simultaneously. The “blue” or production instance will continue to receive connections and run without change, the “green” or updated application will start and be available using a different service. You can create a route (or ingress) if you’d like, and then when satisfied promote the “green” application to production.
Once promoted the rollout will update the &ldquo;blue&rdquo; replicaset which will then scale the &ldquo;blue&rdquo; version of the pods down to zero. After the rollout promotion is completed we can check the rollout status using the argo rollouts plugin.
Let&rsquo;s add the bluegreen application to our pattern. The first thing we need to do is update the values-hub.yaml file.
Add bluegreen namespace to the list of namespaces to be created by openshift-gitops namespaces: - open-cluster-management - vault - golang-external-secrets - argo-rollouts - bluegreen We&rsquo;re going to create this application in the argo-rollouts project - this is just for simplicity in the demo.
Add &lsquo;bluegreen&rsquo; application stanza under Applications
applications: &lt;... omitted ...&gt; bluegreen: name: bluegreen namespace: bluegreen project: argo-rollouts path: charts/all/bluegreen When finished editing make sure that you commit your changes to git!
git commit -am &#34;Added blue-green application to the pattern&#34; git push -u origin main Our demo application is an example application that the argo project provides. We declare this image in the values-global.yaml file, and we will modify the tag to trigger the rollout.
values-global.yaml
rollout: image: argoproj/rollouts-demo:blue Review the snippet below to add bluegreen to the pattern!
Check that the application deployed successfully in the argocd user interface. If everything went well you should see something like this:
With our demo application deployed, let&rsquo;s a rollout by changing the image tag in values-global.yaml to green.
Once we&rsquo;ve made the change we need to commit and push our changes to git
git add values-global.yaml git commit -m &#34;triggering rollout with image update&#34; git push -u origin main Once argocd recognizes the update, the rollout controller will create a replicaset for the green application and will start the desired number of pods. The green application is using its own service and that service is exposed via a route. Once the replicaset has created the pods, the rollout will pause waiting for an action to either promote or abort the rollout.
We have two ways of viewing the rollout. The first is through UI in the argocd interface, and the other is using the argocd rollouts plugin. The UI doesn&rsquo;t provide as much detail as the plugin, so I&rsquo;ll show you what both look like for reference.
This is what the plugin shows us before the image tag has been detected:
oc argo rollouts get rollout bluegreen Name: bluegreen Namespace: bluegreen Status: ✔ Healthy Strategy: BlueGreen Images: argoproj/rollouts-demo:blue (stable, active) Replicas: Desired: 2 Current: 2 Updated: 2 Ready: 2 Available: 2 NAME KIND STATUS AGE INFO ⟳ bluegreen Rollout ✔ Healthy 11m └──# revision:1 └──⧉ bluegreen-5f5746dc47 ReplicaSet ✔ Healthy 11m stable,active ├──□ bluegreen-5f5746dc47-5wfnt Pod ✔ Running 11m ready:1/1 └──□ bluegreen-5f5746dc47-q52cl Pod ✔ Running 11m ready:1/1 Once the image tag has been detected and the rollout executed, this is what we see:
oc argo rollouts get rollout bluegreen Name: bluegreen Namespace: bluegreen Status: ॥ Paused Message: BlueGreenPause Strategy: BlueGreen Images: argoproj/rollouts-demo:blue (stable, active) argoproj/rollouts-demo:green (preview) Replicas: Desired: 2 Current: 4 Updated: 2 Ready: 2 Available: 2 NAME KIND STATUS AGE INFO ⟳ bluegreen Rollout ॥ Paused 13m ├──# revision:2 │ └──⧉ bluegreen-69d5bcb78 ReplicaSet ✔ Healthy 66s preview │ ├──□ bluegreen-69d5bcb78-d5bjp Pod ✔ Running 66s ready:1/1 │ └──□ bluegreen-69d5bcb78-vnw4q Pod ✔ Running 66s ready:1/1 └──# revision:1 └──⧉ bluegreen-5f5746dc47 ReplicaSet ✔ Healthy 13m stable,active ├──□ bluegreen-5f5746dc47-5wfnt Pod ✔ Running 13m ready:1/1 └──□ bluegreen-5f5746dc47-q52cl Pod ✔ Running 13m ready:1/1 In the argoCD interface this what the rollout looks like:
If you look at the rollout resource you can see that it is paused. This is because it&rsquo;s waiting on an administrator to approve or cancel the deployment. Both applications are running side by side. Let&rsquo;s take a look at the routes!
The active/blue/production route:
The preview/update/new route:
Now if we promote the application, the green application will become the primary.
In the argoCD interface, click on the bluegreen application, in the application context click on the vertical ellipsis next to the bluegreen rollout, then click promote-full or click abortto back out.
Now if we take a look at our active route we can see that the color changed to green!
You could do the same with the argo rollouts plugin: kubectl argo rollouts promote bluegreen to back out of the rollout: kubectl argo rollouts abort bluegreen
You can verify that the application has been promoted correctly by using the argo rollouts plugin, checking the route, or by checking the image tag in the rollout resource.
oc argo rollouts get rollout bluegreen Name: bluegreen Namespace: bluegreen Status: ✔ Healthy Strategy: BlueGreen Images: argoproj/rollouts-demo:green (stable, active) Replicas: Desired: 2 Current: 2 Updated: 2 Ready: 2 Available: 2 NAME KIND STATUS AGE INFO ⟳ bluegreen Rollout ✔ Healthy 26m ├──# revision:2 │ └──⧉ bluegreen-69d5bcb78 ReplicaSet ✔ Healthy 13m stable,active │ ├──□ bluegreen-69d5bcb78-d5bjp Pod ✔ Running 13m ready:1/1 │ └──□ bluegreen-69d5bcb78-vnw4q Pod ✔ Running 13m ready:1/1 └──# revision:1 └──⧉ bluegreen-5f5746dc47 ReplicaSet • ScaledDown 26m That&rsquo;s it for the blue-green deployment! Now let&rsquo;s take a look at a canary deployment.
Canary Rollout Canary deployments give us a lot of control on how our application is deployed. We can define what percentage of ingress traffic gets the canary or updated application and for how long. Rollouts can use metrics to determine the health of a rollout and make a decision to continue or abort the rollout based on those metrics.
This gives us insight into the health of our application as it is deployed, it gives insights into whether features are being used, and if they&rsquo;re working correctly. It really does open up all kinds of opportunities to learn a lot more about our applications and how they&rsquo;re used! Canary deployments are powerful and add flexibility to our application deployments. Either through a full application deployment or just testing a feature.
Let&rsquo;s get on with the demo!
The canary demo is located in charts/all/canary-demo and similar to how we deployed the bluegreen demo, we need to add the canary-demo application to our pattern for argocd to deploy it.
Add canary-demo namespace to the list of namespaces to be created by openshift-gitops namespaces: - open-cluster-management - vault - golang-external-secrets - argo-rollouts - canary-demo We&rsquo;re going to create this application in the argo-rollouts project - this is just for simplicity in the demo.
Add &lsquo;canary-demo&rsquo; application stanza under Applications
applications: &lt;... omitted ...&gt; bluegreen: name: canary-demo namespace: canary-demo project: argo-rollouts path: charts/all/canary-demo When finished editing make sure that you commit your changes to git!
git commit -am &#34;Added canary application to the pattern&#34; git push -u origin main We can monitor the argocd interface for the application deployment. When the application has successfully deployed you should see something similar to the image below in the argocd interface:
Let’s take a look at the rollout resource for the canary-demo application.
oc get rollout -o yaml canary-demo -n canary-demo
strategy: canary: steps: - setWeight: 20 - pause: {} - setWeight: 40 - pause: duration: 10 - setWeight: 60 - pause: duration: 10 - setWeight: 80 - pause: duration: 10 In the above snippet, we’re telling the rollout controller that we want 20% of the traffic setWeight: 20 to go to the canary for an indefinite amount of time pause: {}, in the next step we want 40% of the traffic to go to the canary for 10 seconds, then 60% for 10 seconds, then 80% for 10 seconds until 100% of the traffic is using the canary service. These values can be modified to whatever makes sense for our deployment, maybe 10 seconds isn’t long enough to collect performance data on our feature canary and we need to run it for a bit longer.
Our active service before the rollout looks like this:
So let&rsquo;s trigger a rollout by changing the image tag! Edit the values-global.yaml and change the tag from blue to red
Make sure you push your changes to git!
git commit -am &#34;Change canary image tag&#34; git push -u origin main When the canary rollout occurs it removes one pod from the existing replicaset to support the canary replicaset. Let&rsquo;s use the argo rollout plugin to see what this looks like:
oc argo rollouts get rollout canary-demo Name: canary-demo Namespace: canary-demo Status: ॥ Paused Message: CanaryPauseStep Strategy: Canary Step: 1/8 SetWeight: 20 ActualWeight: 20 Images: argoproj/rollouts-demo:blue (stable) argoproj/rollouts-demo:red (canary) Replicas: Desired: 5 Current: 5 Updated: 1 Ready: 5 Available: 5 NAME KIND STATUS AGE INFO ⟳ canary-demo Rollout ॥ Paused 8m45s ├──# revision:2 │ └──⧉ canary-demo-6ffd7b9658 ReplicaSet ✔ Healthy 39s canary │ └──□ canary-demo-6ffd7b9658-dhhph Pod ✔ Running 38s ready:1/1 └──# revision:1 └──⧉ canary-demo-7d984ffb4c ReplicaSet ✔ Healthy 8m45s stable ├──□ canary-demo-7d984ffb4c-5hdsr Pod ✔ Running 8m45s ready:1/1 ├──□ canary-demo-7d984ffb4c-6wtjq Pod ✔ Running 8m45s ready:1/1 ├──□ canary-demo-7d984ffb4c-9vnq9 Pod ✔ Running 8m45s ready:1/1 └──□ canary-demo-7d984ffb4c-zh2bj Pod ✔ Running 8m45s ready:1/1 oc get replicasets NAME DESIRED CURRENT READY AGE canary-demo-6ffd7b9658 1 1 1 47s canary-demo-7d984ffb4c 4 4 4 8m53s In the argoCD interface we see that the rollout is paused just like with blue-green
But what about our application - we said we only want 20% of the traffic to go to the new app:
That is awesome! So now let&rsquo;s promote the application and see what happens - the expectation is that it will incrementally update the percentage of connections to the new application until completely promoted.
Let&rsquo;s take a look at what it looks like using the argo rollouts plugin
Conclusion Argo Rollouts makes progressive delivery of our applications super easy. Whether you want to deploy using blue-green or the more advanced canary rollout is up to you. The canary rollout is very powerful and as we saw gives us the ultimate control, with insights and flexibility to deploy applications. There is so much more that argo rollouts can do - this demo barely scratches the surface! Keep an eye out for argo rollouts as part of openshift-gitops in &lsquo;23.
`,url:"https://validatedpatterns.io/blog/2022-11-20-argo-rollouts/",breadcrumb:"/blog/2022-11-20-argo-rollouts/"},"https://validatedpatterns.io/blog/2022-10-12-acm-provisioning/":{title:"Multi-cluster GitOps with Provisioning",tags:[],content:`Multi-cluster GitOps with Provisioning Introduction Validated Patterns are an opinionated GitOps environment that lowers the bar for those creating repeatable and declarative deployments. It’s value is most apparent when delivering demos and solutions that span multiple areas of our portfolio. Our skeleton allows folks to focus on what needs to be delivered while we take care of how to do so using best practices. This is further illustrated in the simplified way patterns use ACM to provision additional clusters.
Not only do patterns allow a cluster to completely configure itself - including elements traditionally handled with scripting and extending beyond the cluster, but we can now also declaratively teach it about a set of clusters it should provision and subsequently configure.
Let’s walk through an example using the Multi-Cloud GitOps pattern as an example…
Preparation If you&rsquo;ve never deployed OpenShift before, you could try ROSA the pay-as-you-go OpenShift managed service.
Installing a validated pattern Start by deploying the Multi-cloud GitOps pattern on AWS.
Next, you&rsquo;ll need to create a fork of the multicloud-gitops repo. Go there in a browser, make sure you’re logged in to GitHub, click the “Fork” button, and confirm the destination by clicking the big green &ldquo;Create fork&rdquo; button.
Now you have a copy of the pattern that you can make changes to. You can read more about the Multi-cloud GitOps pattern on our community site
Next, install the Validated Patterns operator from Operator Hub.
And finally, click through to the installed operator, and select the Create instance button and fill out the Create a Pattern form. Most of the defaults are fine, but make sure you update the GitSpec URL to point to your fork of multicloud-gitops, rather than https://github.com/hybrid-cloud-patterns/multicloud-gitops.
Providing your Cloud Credentials Secrets must never be stored in Git. Even in encrypted form, you likely also publish metadata that may be exploited to launch spear phishing, and waterholing attacks.
The Multi-Cloud GitOps pattern uses HashiCorp&rsquo;s Vault for secure secret storage.
In order to provision additional clusters, the hub will need your cloud credentials. To do this you can either manually load the secrets into the vault via the UI, or make use of the following process for loading them from a machine you control.
Loading provisioning secrets First clone your fork of the repository onto your local machine, and copy the template to a location not controlled by Git (to avoid accidentally committing the contents)
git clone git@github.com:{yourfork}/multicloud-gitops.git cp values-secret.yaml.template ~/values-secret.yaml You will need to uncomment and provide values for the following keys in order to make use of the provisioning functionality:
secrets: aws: [1] aws_access_key_id: AKIAIOSFODNN7EXAMPLE aws_secret_access_key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY files: publickey: ~/.ssh/id_rsa.pub [2] privatekey: ~/.ssh/id_rsa openshiftPullSecret: ~/.dockerconfigjson [3] [1] A guide to finding the relevant AWS values can be found here You might even have them in a ~/.aws/credentials file.
[2] The public/private key-pair is used to allow access to OpenShift nodes for triage purposes.
[3] The openshiftPullSecret is how Red Hat knows you’ve got a licence to install OpenShift. To obtain one, go here, save the contents, and provide that path in the secrets file. The contents should start with something like: {&quot;auths&quot;:{&quot;cloud.openshift.com&quot;:{&quot;auth&quot;:&quot;....
Obtain the login command for your cluster and run it locally. Ensure podman is installed, and load the secrets with:
./common/scripts/pattern-util.sh make load-secrets These values will be used to create a number of secrets that ACM expects in order to provision clusters.
Loading Secrets into the Cluster Define a Managed Cluster Group Managed cluster groups are sets of clusters, grouped by function, that share a common configuration set. There is no limitation on the number of groups, or the number of clusters within each group, however IIUC there is a scaling limit of approximately 1000 clusters in total.
The following is the example we will use today:
managedClusterGroups: myFirstGroup: name: group-one labels: - name: clusterGroup value: group-one .name is significant here and defines which site file (values-{name}.yaml) is used as the cluster&rsquo;s bill-of-materials. In the example above, you would need to make sure that values-group-one.yaml existed at the top of the Git repo and contained a list of all the namespaces, subscriptions, and applications that should be delivered to the cluster.
.labels tells ACM how to decide which clusters get this site configuration. If you were building and importing clusters yourself, these are the labels you would need to specify during the import process. You can specify different and/or additional labels, but the default is to use clusterGroup={name of the group}
Create a Cluster Pool Validated Patterns use cluster pools to automatically provision and configure sets of spoke clusters in cloud environments (so far with a focus on testing AWS). You can even configure the pools to maintain a number of spare (hibernating) clusters, to provide rapid and cost-effective access to clusters on-demand and at scale.
You can read more about cluster pools in the ACM documentation
Defining the cluster pool Defining clusters Each managed cluster group can have multiple pools, here is an example:
clusterPools: myFirstPool: name: aws-ap openshiftVersion: 4.10.18 baseDomain: blueprints.rhecoeng.com platform: aws: region: ap-southeast-2 size: 1 clusters: - tokyo - sydney - jakarta The most important thing to change is .baseDomain, which will need to correspond to a route53 domain associated with your account. We allow multiple pools per group so that the same cluster configuration can be delivered to multiple regions.
You can specify as many clusters as your AWS limits will support. Feel free to choose something different than tokyo, sydney, and jakarta.
If .size is omitted, the pool will automatically resize based on the number of clusters specified. Specifying no clusters will define the pool, but not provision any clusters.
Delivering Applications and Configuration to Clusters Delivering Configuration Changes Deprovisioning Clusters As the provisioning data only exists on the ACM hub cluster, it is important to ensure any managed clusters are deprovisioned before the hub itself is destroyed. In general this involves scaling down the pool(s), and removing the entries in the clusters: list.
You can see the process in action below:
Deprovisioning clusters Conclusion Once entrusted with your cloud credentials, all patterns can drive the creation and subsequent configuration of complex cluster topologies (including hub-of-hubs!).
`,url:"https://validatedpatterns.io/blog/2022-10-12-acm-provisioning/",breadcrumb:"/blog/2022-10-12-acm-provisioning/"},"https://validatedpatterns.io/blog/2022-09-02-route/":{title:"Using subdomain in your OpenShift route definitions",tags:[],content:`What Is a route in OpenShift? When creating Community or Validated Patterns using the Validated Patterns framework we often include application workloads, such as a UI, that will need to be accessed externally. A route allows developers to expose services through an HTTP(S) aware load balancing and proxy layer via a public DNS entry.
How is the kind: Route used in OpenShift? If your application is meant to be used externally then you will need to define a route so that users can access the service using the URL you specify in the route definition.
Here&rsquo;s a simple example of a route definition for a hello-openshift application:
apiVersion: route.openshift.io/v1 kind: Route metadata: name: hello-openshift spec: host: hello-openshift-hello-openshift.&lt;Ingress_Domain&gt; port: targetPort: 8080 to: kind: Service name: hello-openshift As you can see the spec describes the a host: or path to the route, the target port (8080), and the target, or to: that resolves into endpoints.
If we focus on the host: value you see that we need to provide the Ingress_Domain to the host. You might ask yourself: why is this a problem?
If you manage just one cluster, and your application just runs on that cluster, you can just hard code the ingress domain and be on your merry way. But what happens when you are deploying this application to multiple clusters and their domains are different? Whoever is doing the Ops to deploy your application will have to change the Ingress_Domain to match the the cluster domain manually before deploying the application.
Let&rsquo;s go a step further and say you are using GitOps, and this definition lives in a git repository, what happens then? In our humble opinion it becomes a bit more complicated to make sure the ingress domain is set correctly.
The kind: Route spec to the rescue The route specification for openshift can be found here. If you look at the .spec section you can see the properties that can be used. The one that we will focus in the subdomain property. From our docs here&rsquo;s the definition of the subdomain property:
subdomain is a DNS subdomain that is requested within the ingress controller’s domain (as a subdomain). If host is set this field is ignored. An ingress controller may choose to ignore this suggested name, in which case the controller will report the assigned name in the status.ingress array or refuse to admit the route. If this value is set and the server does not support this field host will be populated automatically. Otherwise host is left empty. The field may have multiple parts separated by a dot, but not all ingress controllers may honor the request. This field may not be changed after creation except by a user with the update routes/custom-host permission.
Example: subdomain frontend automatically receives the router subdomain apps.mycluster.com to have a full hostname frontend.apps.mycluster.com.
If you have access to an OpenShift cluster you can test the above route manifest to see the results. Let&rsquo;s create a quick route-example.yaml file and use the subdomain property. From the command line type the following:
$ cat &lt;&lt;EOF &gt; /tmp/route-example.yaml apiVersion: route.openshift.io/v1 kind: Route metadata: name: hello-openshift namespace: hello-openshift spec: subdomain: hello-openshift-hello-openshift port: targetPort: 8080 to: kind: Service name: hello-openshift EOF Now let&rsquo;s create a namespace where we will test create our Route.
$ oc create ns hello-openshift namespace/hello-openshift created Now let&rsquo;s change our context to that namespace.
$ oc project hello-openshift Now using project &#34;hello-openshift&#34; on server &#34;https://api.magic-mirror-2.blueprints.rhecoeng.com:6443&#34;. Last but not least now let&rsquo;s apply that example route definition we just created.
$ oc create -f /tmp/route-example.yaml route.route.openshift.io/hello-openshift created If we inspect what was applied to the OpenShift cluster you should be able to see the following:
$ oc get route/hello-openshift -o yaml apiVersion: route.openshift.io/v1 kind: Route metadata: annotations: openshift.io/host.generated: &#34;true&#34; creationTimestamp: &#34;2022-09-02T21:55:58Z&#34; name: hello-openshift namespace: hello-openshift resourceVersion: &#34;1349531&#34; uid: c0cddbb2-271e-48fb-b7fd-bcd8c5075cdf spec: host: hello-openshift-hello-openshift.apps.magic-mirror-2.blueprints.rhecoeng.com port: targetPort: 8080 subdomain: hello-openshift-hello-openshift to: kind: Service name: hello-openshift weight: 100 wildcardPolicy: None status: ingress: - conditions: - lastTransitionTime: &#34;2022-09-02T21:55:58Z&#34; status: &#34;True&#34; type: Admitted host: hello-openshift-hello-openshift.apps.magic-mirror-2.blueprints.rhecoeng.com routerCanonicalHostname: router-default.apps.magic-mirror-2.blueprints.rhecoeng.com routerName: default wildcardPolicy: None As you can see the subdomain property was replaced with the host property but it includes the ingress domain for the cluster. Why is this important? I can now apply this to any OpenShift cluster without worrying it&rsquo;s ingress domain since it will get appended automatically.
There is currently a known issue when using the subdomain property in which the name given is not published with the correct template, instead of that they are published with the default &lsquo;\${name}-\${namespace}.subdomain.local&rsquo;. This has been fixed and it&rsquo;s available in OpenShift 4.11.
Conclusion Using the subdomain property when defining route is super useful if you are deploying your application to different clusters and it will allow you to not have to hard code the ingress domain for every cluster.
If you have any questions or want to see what we are working on please feel free to visit our Hybrid Cloud Patterns site. If you are excited or intrigued by what you see here we’d love to hear your thoughts and ideas! Try the patterns contained in our Hybrid Cloud Patterns Repo. We will review your pull requests to our pattern repositories.
`,url:"https://validatedpatterns.io/blog/2022-09-02-route/",breadcrumb:"/blog/2022-09-02-route/"},"https://validatedpatterns.io/blog/2022-08-24-clustergroups/":{title:"Using clusterGroups in the Validated Patterns Framework",tags:[],content:`What Is a clusterGroup? The Validated Patterns framework defines itself in terms of clusterGroups. A clusterGroup is a set of one or more Kubernetes clusters that are managed to have the same deployments (that is, subscriptions and applications, namespaces and projects) applied to each member of the same clustergroup. Two different members of the same clusterGroup will differ in cluster-name and in other cluster-specific details (such as PKI and tokens) but will have the same subscriptions and applications as other members of the same clusterGroup.
Essentially, a clusterGroup is an abstract definition of what the pattern will install on each respective cluster in the pattern. It is possible to install multiple clusterGroups on the same cluster, as we do (for example) in Industrial Edge where we have both datacenter and factory clusterGroups, and the factory clusterGroup is installed on the datacenter cluster by default.
Single-Cluster and Multi-Cluster Patterns Because Validated Patterns started as an Edge initiative, we designed the notion of multi-cluster patterns into the framework from the beginning. The first Validated Pattern, Industrial Edge models a central data-lake and an optional remote factory cluster. The factory cluster does not need the CI or test system, nor the central data lake. Since we have different configuration needs on the two types of cluster, we define them as different cluster groups.
Other patterns only (so far) require a single cluster, since they model their Edge requirements in different ways. Medical Diagnosis brings a pre-provided set of data (which would ordinarily come in from Edge clusters). Ansible Edge GitOps has RHEL instances as its Edge environment, not OpenShift clusters - and it runs its VMs through OpenShift Virtualization so it only uses the one cluster. In these cases, it is simplest to designate the single cluster as a Hub cluster.
Uses for a Hub Cluster in a Multi-Cluster Deployment But in multi-cluster deployments, it is often helpful to define a hierarchy. Even if there are exactly two clusters as part of a multi-cluster pattern, it may be helpful to define one as the &ldquo;hub&rdquo;, in case the pattern grows to more clusters later. Modern architectures can scale to many instances, in some cases thousands, and there are certain responsibilities that are convenient to define as &ldquo;central&rdquo; or &ldquo;hub&rdquo; functions. Some examples that exist in the Framework and its applications so far:
Configuration Management (via Advanced Cluster Management, or by Ansible Automation Platform) Data Aggregation (via AMQ Streams) Continuous Integration/Continuous Delivery Pipelines (via OpenShift Pipelines) Data Visualization (via Grafana) Secrets storage and maintenance (via Vault and the External Secrets Operator) Some other potential uses for the &ldquo;hub&rdquo; role or function include:
General &ldquo;control plane&rdquo; functions Metrics aggregation The hub role defines a place in the architecture to run these vital functions, while reserving capacity on the Edge for data gathering and pre-process functions.
But this naturally brings up a question - what are the different roles we have considered in the Validated Patterns framework, and how should they be used?
Types of clusterGroup While we describe the Validated Patterns framework as &ldquo;opinionated&rdquo;, we do not want to make it overly constricting. The framework currently considered two types of clusterGroups: Hub and non-hub. We consider &ldquo;Hub&rdquo; as a role, primarily, but it is also used as the default name for the Hub clusterGroup. There are two important aspects of a Hub clusterGroup:
It is expected to be a singleton (there is expected to be a single hub cluster in the hub clusterGroup) There is expected to be a single Hub clusterGroup in a given pattern Generally, we expect that non-Hub clusterGroups will be Edge clusters, but this is not essential. A clusterGroup should have at least one cluster that it will apply to (otherwise, why define one?) and can have multiple clusters. The framework sets ArgoCD&rsquo;s ignoreMissingValueFiles setting to true unconditionally, and the framework also provides an extraValueFiles variable, which can define multiple optional additional values files on a per-clusterGroup basis.
How the Hierarchy Works - the clustergroup Chart The clustergroup chart begins by looking in the pattern&rsquo;s values-global.yaml for the main.clusterGroupName value. This value is then used to compute the next values file to process - if the value of that variable is hub then the application(s) that are created will use both the values-global.yaml and values-hub.yaml files in the root of the pattern repository. The clusterGroup structure will then be parsed for name and isHubCluster values; namespaces, subscriptions, projects and applications will be applied. Any managedClusterGroups (on the hubCluster) will be defined in terms for Advanced Cluster Management (as is done in Industrial Edge). Because the factory match expression is very general (vendor In OpenShift) this will match the hub cluster and any cluster that is joined to the hub cluster&rsquo;s ACM instance.
Frequently Asked Questions (FAQs) I have a single-cluster Pattern. Do I need to call my hub cluster &ldquo;hub&rdquo;? You can, but you do not have to. If you want to call it something other than hub:
Define a different main.clusterGroupName value in values-global.yaml Create the values-yourhubgroupname.yaml in the root of your pattern repository. Make sure clusterGroup.name variable in values-yourhubgroupname.yaml matches your intended hub group. I have a multi-cluster Pattern. All of my Edge clusters will have the same configuration. Do I need multiple clusterGroups to model it? No! This is the exact scenario we had in mind when we designed clusterGroups. Just add multiple clusters with the same criteria you defined to the ACM instance on your hub cluster, and each cluster you add will get the same subscriptions and applications.
I have a multi-cluster Pattern. How do I decide if I need multiple clusterGroups to model it? Do you have different subscriptions or applications that you plan to use on your different clusterGroups? If so, you should define different clusterGroups and define them as managedClusterGroups for your hub cluster.
`,url:"https://validatedpatterns.io/blog/2022-08-24-clustergroups/",breadcrumb:"/blog/2022-08-24-clustergroups/"},"https://validatedpatterns.io/blog/2022-07-15-push-vs-pull/":{title:"Push or Pull?",tags:[],content:`Push or Pull? Strategies for Large Scale Technology Change Management on the Edge What is Technology Change Management? There is a segment of the technology industry dedicated to keeping track of what is changing in an IT environment, when and how. These include systems like Service Now, Remedy, JIRA, and others. This is definitely a kind of change management, and these systems are important - but the focus of this blog post is not how the work of change management is tracked, but the actual means and strategy of doing those changes.
Edge technology solutions involve hardware and software, and they all require some kind of technology maintenance. Our focus here is on software maintenance - these task can involve updating applications, patching underlying operating systems, performing remote administration - restarting applications and services. Coordinating change for a complex application can be daunting for a centralized datacenter application - but on the Edge, where we have hundreds, thousands, maybe millions of devices and application instances to keep track of, it is harder.
What Do You Mean, Push or Pull? In this article, we are going to discuss two primary strategies for systems that are responsible for making and recording changes on other systems. We are making the assumption here that the system in question is making changes and also recording the results of those changes for later review or troubleshooting. Highly regulated organizations often have audit requirements to show that their financial statements are accurate, and this means demonstrating that there are business processes in place to authorize and schedule changes.
In this context, when we say &ldquo;Push&rdquo;, we mean that a hub or centralized system originates and makes changes on other systems. The key differentiator is that the &ldquo;Push&rdquo; system stays in contact with the managed system throughout the process of the change. The &ldquo;Push&rdquo; system may also keep a record of changes made for its own purposes.
In a &ldquo;Pull&rdquo; system, on the other hand, the centralized system waits for managed systems to connect to it to get their configuration instructions. &ldquo;Pull&rdquo; systems often have agents that use a dedicated protocol to define changes. There may be several steps in a &ldquo;Pull&rdquo; conversation, as defined by the system. A &ldquo;Pull&rdquo; system might also be able to cache and apply a previous configuration. The key differentiator of a &ldquo;Pull&rdquo; system is that it does not need to maintain constant contact with the central system to do its work.
Push and Pull, in this context represent &ldquo;strategies&rdquo; for managing change. A given system can have both &ldquo;push&rdquo; and &ldquo;pull&rdquo; aspects; for example, ansible has an ansible-pull command which clearly works in a &ldquo;pull&rdquo; mode, even though most people recognize ansible as being primarily a push based system. While specific systems or products may be mentioned, the goal is not to evaluate systems themselves, but to talk about the differences and the relative merits and pitfalls of push and pull as strategies for managing change.
What do you mean by &ldquo;Large&rdquo;? The term &ldquo;Large&rdquo; is quoted because it can mean different things in different contexts. For change management systems, it can include, but is not necessarily limited to:
The count of individual systems managed As an arbitrary number, a system that manages 10,000 systems could probably be considered &ldquo;large&rdquo; regardless of other considerations. But sheer numbers of managed systems are only one aspect in play here. But you may still have a &ldquo;large&rdquo; problem if you do not have that many instances. Sheer volume of managed systems impose many constraints on systems that have to change and track other systems - records of changes have to be stored and indexed; there has to be a way to represent the different desired configurations.
The complexity of different configurations across those systems The number of configurations represented across your fleet might be a better predictor for how &ldquo;large&rdquo; the problem is. It is easier to manage 10 configurations with 1,000 instances each than to manage 50 configurations with 100 instances each, for example.
The organizational involvement in managing configurations Fleets have a certain team overhead in managing them as they grow. Who decides what hardware gets deployed, and when? Who decides when new Operating System versions are rolled out? Who does testing? If there are several teams involved in these activities, the problem is almost certainly a &ldquo;large&rdquo; one.
The geographic distribution of systems managed Another aspect of complexity is how widely dispersed the fleet is. It is easier to manage 10,000 instances in one or even two locations than it is to manage 5 instances in each of 2,000 locations. Geographic distribution also includes operating in multiple legal jurisdictions, and possible in multiple nations. These impose requirements of various kinds on systems and thus also on the systems responsible for maintaining and changing them.
It feels like a &ldquo;large&rdquo; problem to you If none of the other criteria listed so far apply to you, but it still feels like a &ldquo;large&rdquo; problem to you, it probably is one.
Managing Change - An Ongoing Challenge In a perfect world, we could deploy technology solutions that maintain themselves. We would not need to update them; they would know how to update themselves. They could coordinate outage times, and could ensure that they can run successfully on a proposed platform. They would know about their own security vulnerabilities, and know how to fix them. Best of all, they could take requests from their users, turn those into systematic improvements, and deploy them without any other interaction.
What is the Edge? Are you laughing yet? Most of the work of IT administrators is done in one of the areas listed above. Even very competent IT organizations sometimes struggle balancing some of these priorities. One aspect of the current computing environment is the prominence of Edge computing, which places its focus on the devices and applications that use computing resources far from central clouds and data centers - these compute resources run in retail stores, in pharmacies, hospitals, and warehouses; they run in cars and sometimes even spacecraft. In Edge environments, compute resources have to deal with intermittent network connectivity, if they have network connectivity at all. Sometimes, groups of devices have access to local server resources - as might the case in a large retail store, or in a factory or warehouse - but sometimes the closest &ldquo;servers&rdquo; are in the cloud and centralized. One interesting aspect of Edge deployments is that there are often many copies of effectively the same deployment. A large chain retail store might deploy the same set of applications to each of its stores. In such an installation, there may be many devices of a single type installed in that location. Think of the number of personal computers or cash registers you can see at a large retail store, for example. It would not be unusual to have ten PCs and twenty cash registers (per store) in this kind of deployment. And a large retail chain could have hundreds or even thousands of locations. Newer technologies, like Internet of Things deployments, require an even higher degree of connectivity - the single retail store example we are considering could have three hundred cameras to manage, which would need to be integrated into its IoT stack. And there could be hundreds, or thousands, of sites just like this one. The scope and scale of systems to manage can get daunting very quickly.
So, some of the defining qualities of Edge environments are: scale (anyone operating some edge installations probably has a lot of edge installations, and the success of their business depends on them operating more of them) and network limitations (whether there is a connection at all, and if so, how reliable it is; bandwidth - how much data it can transfer at a time; and latency - how long it takes to get where it is going). This makes making changes in these environments challenging, because it means keeping track of large numbers of entities in an environment where our ability to contact those entities and verify their status may be limited if it is present at all. But we still must make changes in those environments, because those solutions need maintenance - their platforms may need security updates; their operators may want to update application features and functionality. This requires us to make changes to these devices, and requires technology change management.
Consideration: Workload Capacity Management Workload Capacity Management focuses on what is needed to manage the scale of deployments. With large Edge deployments, the work needs to get done, and that work needs to be replicated on every node in scope for the deployment - so the same job or configuration content may need to apply to hundreds, thousands, or more individual nodes. Since the control point (central or distributed) is different between push and pull based systems, how they distribute the work needed to distribute changes. Push based systems must send the work out directly; but pull-based systems can potentially overwhelm a centralized system with a &ldquo;thundering herd.&rdquo;
Common Consideration: Inventory/Data Aggregation Inventory and Data Aggregation are crucial considerations for both kinds of systems. Inventory is the starting point for determining what systems are in scope for a given unit of work; data aggregation is important to them because as units of work get done, we often need proof or validation that the work was done. With large numbers of edge nodes, there are certain to be exceptions, and the ability to keep track of where the work is crucial to completing the task.
Common Consideration: Authentication/Authorization Since these systems are responsible for making changes on devices, how they interact with authentication and authorization systems is an important aspect of how they work. Is the user who the user claims to be? Which users are allowed to make which changes? Authentication and authorization are things we must consider for systems that make changes. Additionally many large organizations have additional technology requirements for systems that can make changes to other systems.
Common Consideration: Dealing with Network Interruptions In Edge deployments, network connectivity is by definition limited, unreliable, or non-existent. There are differences in how respective types of systems can detect and behave in the presence of network interruptions.
Common Consideration: Eventual Consistency / Idempotence Regardless of whether a system is push-based or pull-based, it is valuable and useful for the configuration units managed by that system to be safe to apply and re-apply at will. This is the common meaning of the term idempotence. One strategy for minimizing the effect of many kinds of problems in large-scale configuration management is writing content that is idempotent, that is, the effect of running the same content multiple times is the same as the effect of running it once. Practically speaking, this means that such systems make changes only when they need to, and do not have &ldquo;side effects&rdquo;. This makes it safe to run the same configuration content on the same device many times, so if it cannot be determined whether a device has received a particular configuration or not, the solution would be to apply the configuration to it, and the devices should then be in the desired, known state when the configuration is done.
Approach 1: Push Strategy The first approach we will consider is the &ldquo;push&rdquo; strategy. In a &ldquo;push&rdquo; strategy, the centralized change management system itself reaches out to managed devices and triggers updates in some way. This could involve making an API call to a device, logging in to a device through SSH, or using a dedicated client/server protocol. Red Hat&rsquo;s Ansible operates as a push-based system, where a central console reaches out to devices and manages them.
Push Consideration: Workload Capacity Management A push based system has much more control over how it parcels out configuration workload, since it is in control of how configuration workloads are driven. It can more easily perceive its own load state, and potentially &ldquo;back off&rdquo; or &ldquo;throttle&rdquo; if it is processing too much work too quickly - or increase it if the scope of the desired change is smaller than the total designed capacity of the system. It is easier to influence the &ldquo;rate of change&rdquo; on a push-based system for this reason.
Push Consideration: Dealing with Network Interruptions Push-based systems are at a disadvantage when dealing with network interruptions and limitations. The most common network failure scenarios are ambiguous: if an attempt to reach an individual device fails, is that because there was a problem with the device, or a problem with the network path to reach the device? The push-based system can only know things about the devices it manages when it is told. An additional potential with network interruption is that a device can successfully apply a unit of configuration change but can fail to report that because of a network problem - the report is dropped, for example, because of a network path outage or problem, or the central collection infrastructure was overwhelmed. In such a situation, it is best to have the option to re-apply the configuration, for which it is best if you can have the confidence that such configuration will not have any undesired side-effects, and will only make the changes it needs to make.
Approach 2: Pull Strategy The second approach we will consider is the &ldquo;pull&rdquo; strategy. The key difference in the &ldquo;pull&rdquo; strategy is that devices themselves initiate communication with the central management system. They can do this by making a request to the management system (which can be a notification, API call, or some other mechanism). That is to say - the central management system &ldquo;waits&rdquo; for check-ins from the managed devices. Client-server Puppet is a pull-based system, in which managed devices reach out to server endpoints, which give the devices instructions on what configurations to apply to themselves. Puppet also has options for operating in a push-based model; historically this could be done through puppet kick, mcollective orchestration, application orchestration, or bolt.
Pull Consideration: Workload Capacity Management Pull-based systems have some challenges in regard to workload capacity for the pieces that need to be centralized (particularly reporting and inventory functions). The reason for this is that the devices managed will not have a direct source of information about the load level of centralized infrastructure, unless this is provided by an API; some load balancing schemes can do this in a rudimentary way by directing new requests to an instance via a &ldquo;least connection&rdquo; balancing scheme. Large deployments typically have to design a system to stagger check-ins to ensure the system does not get overwhelmed by incoming requests.
Pull Consideration: Authentication/Authorization Pull-based systems typically have agents that run on the systems that are managed, and as such are simpler to operate from an authentication and authorization standpoint. Agents on devices can often be given administrative privilege, and the practical authentication/authorization problems have to do with access to the central management console, and the ability to change the configurations distributed or see the inventory and reports of attempts to configure devices.
Pull Consideration: Dealing with Network Interruptions Pull-based systems have a distinct advantage when they encounter network interruptions. While it is in no way safe to assume that a managed device is still present or relevant from the standpoint of central infrastructure, it is almost always safe for a device, when it finds it cannot connect central infrastructure, to assume that it is experiencing a temporary network outage, and to simply retry the operation later. Care must be taken, especially in large deployments, not to overwhelm central infrastructure with requests. Additionally, we must remember that since network interruption can occur at any time on the edge, that the operation we are interested in may indeed have completed successfully, but the device was simply unable to report this to us for some reason. As was the case for push-based systems, the best cure for this is to ensure that content can be safely re-applied as needed or desired.
Conclusions Push and Pull based systems have different scaling challenges as they grow Push and Pull-based systems have different tradeoffs. It can be easier to manage a push-based system for smaller numbers of managed devices; some of the challenges of both styles clearly increase as systems grow to multiple thousands of nodes.
Meanwhile, both push and pull based systems, as a practical matter, have to make sense and be usable for small installations as well as large, and grow and scale as smoothly as possible. Many installations will never face some or maybe even any of these challenges - and systems of both types must be easy to understand and learn, or else they will not be used.
Pull-based systems are better for Edge uses despite scaling challenges Pull based systems can deal better with the network problems that are inherent with edge devices. When connectivity to central infrastructure is unreliable, pull-based systems can still operate. Pull-based systems can safely assume that a network partition is temporary, and thus do not suffer from the inherent ambiguity of &ldquo;could not reach target system&rdquo; kinds of errors.
Idempotence matters more than whether a system is push or pull based The fix for nearly all the operational problems in large scale configurations management problems is to be able to apply the same configuration to the same device multiple times and expect the same result. This takes discipline and effort, but that effort pays off well in the end.
To help scale, introduce a messaging or queuing layer to hold on to data in flight if possible Many of the operational considerations are related to limited network connectivity or overtaxing centralized infrastructure. Both of these problems can be mitigated significantly by introducing a messaging or queuing layer in the configuration management system to hold on to reports, results, and inventory updates until the system can confirm receipt and processing of those elements.
`,url:"https://validatedpatterns.io/blog/2022-07-15-push-vs-pull/",breadcrumb:"/blog/2022-07-15-push-vs-pull/"},"https://validatedpatterns.io/blog/2022-06-30-ansible-edge-gitops/":{title:"Ansible Edge GitOps",tags:[],content:`Validated Pattern: Ansible Edge GitOps Ansible Edge GitOps: The Why and What As we have been working on new validated patterns and the pattern framework, we have seen a need and interest from the community in expanding the use cases covered by the framework to include other parts of the portfolio besides OpenShift. We understand the Edge computing environments are very complex, and while OpenShift may be the right choice for some Edge environments, it will not be feasible or practical for all of them. Can other environments besides Kubernetes-native ones benefit from GitOps? If so, what would those look like? This pattern works to answer those questions.
GitOps is currently a hot topic in technology. It is a natural outgrowth of the Kubernetes approach in particular, and is informed by now decades of practice in managing large fleets of systems. But is GitOps a concept that is only for Kubernetes? Or can we use the techniques and patterns of GitOps in other systems as well? We believe that by applying specific practices and techniques to Ansible code, and using a Git repo as the authoritative source for configuration results, that we can do exactly that.
One of the first problems we knew we would have to solve in developing this pattern was to work out how to model an Edge environment that was running Virtual Machines. We started with the assumption that we were going to use the Ansible Automation Platform Operator for OpenShift to manage these VMs. But how should we run the VMs themselves?
It is certainly possible to use the different public cloud offerings to spin up instances within the clouds, but that would require a lot of maintenance to the pattern over the long haul to pay attention to different image types and to address any changes to the provisioning schemes the different clouds might make. Additionally, since the purpose of including VMs in this pattern is to model an Edge environment, modeling them as ordinary public cloud instances might seem odd. As a practical matter, the pattern user would have to keep track of the instances and spin them down when spinning down the pattern.
To begin solving these problems, this pattern introduces OpenShift Virtualization to the pattern framework. While OpenShift Virtualization today supports AWS and on-prem baremetal clusters, we hope that it will also bring support to GCP and Azure in the not too distant future. The use of OpenShift Virtualization enables the simulated Edge environment to be modeled entirely in a single cluster, and any instances will be destroyed along with the cluster.
The pattern itself focuses on the installation of a containerized application (Inductive Automation Ignition) on simulated kiosks running RHEL 8 in kiosk mode. This installation pattern is based on work Red Hat did with a customer in the Petrochemical industry.
Highlight: Imperative and Declarative Automation, and GitOps The validated patterns framework has been committed to GitOps as a philosophy and operational practice since the beginning. The framework&rsquo;s use of ArgoCD as a mechanism for deploying applications and components is proof of our commitment to GitOps core principles of having a declared desired end state, and a designated agent to bring about that end state.
Many decades of automation practice that focus on individual OS instances (whether they be virtual machines or baremetal) may lead us to believe that the only way to manage such instances is imperatively - that is, focusing on the steps required to configure a machine to the state you want it to be in as opposed to the actual end state you want.
By way of example, consider a situation where you want an individual OS instance to synchronize its time to a source you specify. The imperative way to do this would be to write a script that does some or all of the following:
Install the software that manages system time synchronization Write a configuration file for the service that specifies the time source in question If the configuration file or other configuration mechanism that influences the service has changed, restart the time synchronization service. Along the way, there are subtle differences between different operating systems, such as the name of the time synchronization package (ntp or chrony, for example); differences in which package manager to use; differences in configuration file formats; differences in service names. It is all rather a lot to consider, and the kinds of scripts that managed these sorts of things at scale, when written in Shell or Perl, could get quite convoluted.
Meanwhile, would it not be great if we could put the focus on end state, instead of on the steps required to get to that end state? So we could specify what we want, and we could trust the framework to &ldquo;make it so&rdquo; for us? Languages that have this capability rose to the forefront of IT consciousness this century and became wildly popular - languages like Puppet, Chef, Salt and, of course, Ansible. (And yes, they all owe quite a lot to CFEngine, which has been around, and is still around.) The development and practices that grew up around these languages significantly influenced Kubernetes and its development in turn.
Because these languages all provide a kind of hybrid model, they all have mechanisms that allow you to violate one or more of the core tenets of GitOps. For example, while many people run their configuration management code from a Git repository, none of these languages specifically require that, and all provide mechanisms to run in an ad-hoc mode. And yet, all of these languages have a fairly strong declarative flavor that can be used to specify configurations with them; again, this is not mandatory, but it is still quite common. So maybe there is a way to apply the stricter definitions of GitOps to these languages and include their use in a larger GitOps system.
Even within Kubernetes, where have clearly have first-class support for declarative systems, there are aspects of configuration that we may want to make deterministic, but not explicitly code into a git repository. For example, best practice for cluster availability is to spread a worker pool over three different availability zones in a public cloud region. Which three zones should they be? Those decisions are bound to the region the cluster is installed in. Do the AZs themselves really matter, or is the only important constraint that there be three? These kinds of things are state that matters to operators, and an imperative framework for dealing with questions like this can vastly simplify the task of cluster administrators, who can then use this automation to create clusters in multiple regions and clouds and trust that resources will be optimized for maximum availability.
Another crucial point of Declarative and Imperative systems is that it is impossible to conceive of a declarative system that does not have or require reconciliation loops. These reconciliation loops are by definition imperative processes. They often have additional conventions that apply - for example the convention that in Kubernetes Operators the reconciliation loop will change one thing, and then retry - but those processes are still inherently imperative.
A final crucial point on Declarative and Imperative systems is that, especially when we are talking about Edge installations, many of the systems that are important parts of those ecosystems do not have the same level of support for declarative-style configuration management that server operating systems and layers like Kubernetes have. Here we consider crucial elements of Edge environments like routers, switches, access points, and other network gear; as we consider IoT sensors like IP Cameras, it seems unlikely that we will have a Kubernetes-native way to manage devices like these in the foreseeable future.
With these points in mind, it seems that if we cannot bring devices to GitOps, perhaps we should bring GitOps to devices. Ansible has long been recognized for its ability to orchestrate and manage devices in an agentless model. Is there a way to run Ansible that we can recognize as a GitOps mechanism? We believe that there is, by using it with the Ansible Automation Platform components (formerly known as Tower), and recording the desired state in the git repository or repositories that the system uses. In doing so, we believe that we can and should bring GitOps to Edge environments.
Highlight: Including Ansible in Validated Patterns A new element of this pattern is the use of the Ansible Automation Platform Operator, which we install in the hub cluster of the pattern.
The Ansible Automation Platform Operator is the Kubernetes-native way of running AAP. It provides the Controller function, which supports Execution Environments. The pattern provides its own Execution Environment (with the definition files, so that you can see what is in it or customize it if you like), and loads its own Ansible content into the AAP instance. It uses a dynamic inventory technique to deal with certain aspects of running the VMs it manages under Kubernetes.
The key function of AAP in this pattern is to configure and manage the kiosks. The included content takes the fresh templates, registers them to the Red Hat CDN, installs Firefox, configures kiosk mode, and then downloads and manages the Ignition application container so that both firefox and the application container start at boot time.
The playbook that configures the kiosks is configured to run every 10 minutes on all kiosks, so that if there is some temporary error on the kiosk the configuration will simply attempt configuration again when the schedule tells it to.
Highlight: Including Virtualization in Validated Patterns As discussed above, another key element in this pattern is the introduction of of OpenShift Virtualization to model the Edge environment with kiosks. The pattern installs the OpenShift Virtualization operator, configures it, and provisions a metal node in order to run the virtual machines. It is possible to emulate hardware acceleration, but the resulting VMs have terrible performance.
The virtual machines we build as part of this pattern are x86_64 RHEL machines, but it should be straightforward to extend this pattern to model other architectures, or other operating systems or versions.
The chart used to define the virtual machines is designed to be open and flexible - replacing the values.yaml file in the chart&rsquo;s directory will allow you to define different kinds of virtual machine sets; the chart may give you some ideas on how to manage virtual machines under OpenShift in a GitOps way.
Highlight: Including RHEL in Validated Patterns One of the highlights of this pattern is the use of RHEL in it. There are a number of interesting developments in RHEL that we have been working on, and we expect to highlight more of these in future patterns. We expect that this pattern will be the basis for future patterns that include RHEL, Ansible, and/or Virtualization.
Where do we go from here? We believe this pattern breaks some new and interesting ground in bringing Ansible, Virtualization, and RHEL to the validated pattern framework. Like all of our patterns, this pattern is Open Source, and we encourage you to use it, tinker with it, and submit your ideas, changes and fixes.
Documentation for how to install the pattern is here, where there are detailed installation instructions, more technical details on the different components in the pattern (especially the use of AAP and OpenShift Virtualization), and some ideas for customization.
`,url:"https://validatedpatterns.io/blog/2022-06-30-ansible-edge-gitops/",breadcrumb:"/blog/2022-06-30-ansible-edge-gitops/"},"https://validatedpatterns.io/patterns/ansible-edge-gitops/":{title:"Ansible Edge GitOps",tags:[],content:`Ansible Edge GitOps Background Organizations are interested in accelerating their deployment speeds and improving delivery quality in their Edge environments, where many devices may not fully or even partially embrace the GitOps philosophy. Further, there are VMs and other devices that can and should be managed with Ansible. This pattern explores some of the possibilities of using an OpenShift-based Ansible Automated Platform deployment and managing Edge devices, based on work done with a partner in the Chemical space.
This pattern uses OpenShift Virtualization (the productization of Kubevirt) to simulate the Edge environment for VMs.
Solution elements How to use a GitOps approach to manage virtual machines, either in public clouds (limited to AWS for technical reasons) or on-prem OpenShift installations How to integrate AAP into OpenShift How to manage Edge devices using AAP hosted in OpenShift Red Hat Technologies Red Hat OpenShift Container Platform (Kubernetes) Red Hat Ansible Automation Platform (formerly known as &ldquo;Ansible Tower&rdquo;) Red Hat OpenShift GitOps (ArgoCD) OpenShift Virtualization (Kubevirt) Red Hat Enterprise Linux 8 Other Technologies this Pattern Uses Hashicorp Vault External Secrets Operator Inductive Automation Ignition Architecture Similar to other patterns, this pattern starts with a central management hub, which hosts the AAP and Vault components.
Logical architecture Physical Architecture TBD
Recorded Demo TBD
Other Presentations Featuring this Pattern Registration Required What Next Getting Started: Deploying and Validating the Pattern `,url:"https://validatedpatterns.io/patterns/ansible-edge-gitops/",breadcrumb:"/patterns/ansible-edge-gitops/"},"https://validatedpatterns.io/patterns/devsecops/":{title:"Multicluster DevSecOps",tags:[],content:`Multicluster DevSecOps Background With this Pattern, we demonstrate a horizontal solution for multicluster DevSecOps use cases.
It is derived from the multi-cloud GitOps pattern with added products to provide a complete DevSecOps workflow. This includes CI/CD pipelines with security gates; image scanning, signing and storage in a secure registry; deployment to secured clusters that provide advanced security monitoring and alerting.
Solution elements How to use a GitOps approach to keep in control of configuration and operations How to centrally manage multiple clusters, including workloads How to build and deploy workloads across clusters using modern CI/CD How to deploy different security products into the pattern Red Hat Products Red Hat OpenShift Container Platform (Kubernetes) Red Hat Advanced Cluster Management (Open Cluster Management) Red Hat OpenShift GitOps (ArgoCD) Red Hat OpenShift Pipelines (Tekton) Red Hat Quay (container image registry with security features enabled) Red Hat Open Data Foundation (highly available storage) Red Hat Advanced Cluster Security (scanning and monitoring) Other technologies and products Hashicorp Vault community edition (secrets management) Context on Multicluster DevSecOps Effective cloud native DevSecOps is about securing both the platform and the applications deployed to the platform. Securing the applications deployed is also about securing the supply chain. Not all applications are built in-house. Confidence in external applications and technologies is critical. OpenShift Platform Plus enables DevSecOps for both platform and supply chain.
OpenShift Platform Plus includes OpenShift Container Platform, Advanced Cluster Management, Advanced Cluster Security, OpenShift Data Foundation and Red Hat Quay. The capabilities delivered across these components combine to provide policy-based cluster lifecycle management and policy based risk and security management across your fleet of clusters. You can see the flow at the bottom of this graphic.
The Hub cluster is where lifecycle, deployment, security, compliance and risk management policies are defined and is the central management point across clusters. DevSecOps for the platform includes pulling images from Red Hat’s registry, pulling day two configuration code from Git via our integration with ArgoCD, and ensuring that all optional operators are deployed and configured.
Policy based deployment also specifies which admission controllers should be deployed to which clusters, including the ACS admission controller. The Hub also provides a unified view of health, security, risk and compliance across your fleet. We have many of these capabilities in place today, however, they each have their own UI. Over the next few releases, we will be working to provide an integrated multi-cluster user experience for admin, security and developer persona in the OpenShift Console.
Demo Scenario The Multicluster DevSecOps Pattern / Demo Scenario reflects this by having 3 layers:
Managed/Secured edge - the edge or production a more controlled environment. Devel - where AppDev, Testing etc. is happening Central Data Center / Hub - the cloud/core, (and ERP systems of course, not part of the demo). There are ways of combing these three clusters into a two cluster (hub/devel and secured/edge) and single cluster (all in one). The documentation provides instructions (TBD Link).
Pattern Logical Architecture The following diagram explains how different roles have different concerns and focus when working with this distributed AL/ML architecture.
In the Multi-Cluster DevSecOps architecture there are three logical types of sites.
The Hub. This is where the cloud native infrastructure is monitored and managed. It performs cluster management, advanced cluster security and a secure image registry. Devel. This is where the development pipeline is hosted. Developers submit code builds to the pipeline and various security tools are used in the pipeline to mitigate the risk of harmful applications or code being deployed in production. Secured Production. This is where applications are securely deployed and monitored. Pattern Architecture Schema The following diagram shows various management functions, including products and components, that are deployed on central hub and managed clusters (Devel. and Prod.) in order to maintain secure clusters. Consider this the GitOps schema.
The following diagram shows various development pipeline functions, including products and components, that are deployed on the central hub and development (Devel) clusters in order to provide security features and services for development pipelines. Consider this the DevSecOps schema.
`,url:"https://validatedpatterns.io/patterns/devsecops/",breadcrumb:"/patterns/devsecops/"},"https://validatedpatterns.io/blog/2022-03-30-multicloud-gitops/":{title:"Multi-Cloud GitOps",tags:[],content:`Validated Pattern: Multi-Cloud GitOps Hybrid Cloud Patterns: The Story so far Our first foray into the realm of Hybrid Cloud Patterns was the adaptation of the MANUela application and its associated tooling to ArgoCD and Tekton, to demonstrate the deployment of a fairly involved IoT application designed to monitor industrial equipment and use AI/ML techniques to predict failure. This resulted in the Industrial Edge validated pattern, which you can see here.
This was our first use of a framework to deploy a significant application, and we learned a lot by doing it. It was good to be faced with a number of problems in the “real world” before taking a look at what is really essential for the framework and why.
All patterns have at least two parts: A “common” element (which we expect to be the basic framework that nearly all of our patterns will share) and a pattern-specific element, which uses the common pattern and expands on it with pattern-specific content. In the case of Industrial Edge, the common component included secret handling, installation of the GitOps operator, and installation of Red Hat Advanced Cluster Management. The pattern-specific components included the OpenShift Pipelines Operator, the AMQ Broker and Streams operators, Camel-K, the Seldon Operator, OpenDataHub, and Jupyter Notebooks and S3 storage buckets.
Multi-Cloud GitOps: The Why and What After finishing with Industrial Edge, we recognized that there were some specific areas that we needed to tell a better story in. There were several areas where we thought we could improve the user experience in working with our tools and repos. And we recognized that the pattern might be a lot clearer in form and design to those of us who worked on it than an interested user from the open Internet.
So we had several categories of work to do as we scoped this pattern:
Make a clear starting point: Make a clear “entry point” into pattern development, and define the features that we think should be common to all patterns. This pattern should be usable as a template for both us and other users to be able to clone as a starting point for future pattern development. Make “common” common: Since this pattern is going to be foundational to future patterns, remove elements from the common framework that are not expected to be truly common to all future patterns (or at least a large subset of them). Many elements specific to Industrial Edge found their way into common; in some cases we thought those elements truly were common and later re-thought them. Improve secrets handling: Provide a secure credential store such that we can manage secrets in that store rather than primarily as YAML files on a developer workstation. Broker access to that secret store via the External Secrets Operator to ensure a level of modularity and allow users to choose different secret stores if they wish. We also want to integrate the usage of that secret store into the cluster and demonstrate how to use it. Improve support for cluster scalability: For the Industrial Edge pattern, the edge cluster was technically optional (we have a supported model where both the datacenter and factory applications can run on the same cluster). We want these patterns to be more clearly scalable, and we identified two categories of that kind of scalability: Clusters vary only in name, but run the same applications and the same workloads Clusters vary in workloads, sizing, and configuration, and allow for considerable variability. Many of the elements needed to support these were present in the initial framework, but it may not have been completely clear how to use these features, or they were couched in terms that only made sense to the people who worked on the pattern. This will now be clearer for future patterns, and we will continue to evolve the model with user and customer feedback.
Key Learning: Submodules are hard, and probably not worth it Copy and Paste Git Submodules Git Subtrees We rejected the notion of copy and paste because we reasoned that once patterns diverged in their “common” layer it would be too difficult and painful to bring them back together later. More importantly, there would be no motivation to do so.
In Industrial Edge, we decided to make common a git submodule. Git submodules have been a feature of git for a long time, originally intended to make compiling a large project with multiple libraries more straightforward, by having a parent repo and an arbitrary set of submodule repos. Git submodule requires a number of exceptions to the “typical” git workflow - the initial clone works differently, and keeping the submodule updated to date can trip users up. Most importantly, it requires the practical management of multiple repositories, which can make life difficult in disconnected environments, which are important to us to support. It was confusing for our engineers to understand how to contribute code to the submodule repository. Finally, user response to the exceptions they had to make because of submodules was universally negative.
So going forward, because it is still important to have a common basis for patterns, and a clear mechanism and technical path to get updates to the common layer, we have moved to the subtree model as a mechanism for including common. This allows consumers of the pattern to treat the repo as a single entity instead of two, and does not require special syntax or commands to be run when switching branches, updating or, in many cases, contributing to common itself.
Key Learning: Secrets Management One of our biggest challenges in following GitOps principles for the deployment of workloads is the handling of secrets. GitOps tells us that the git repo should be the source of truth - but we know that we should not store secrets directly in publicly accessible repositories. Previously, our patterns standardized the use of YAML files on the developer workstation as the de-facto authoritative secret store. This can be problematic for at least two reasons: for one, if two people are working on the same repo, which secret store is “right”? Secondly, it might be easier to retrieve credentials from a developer workstation due to information breach or theft. Our systems will not work without secrets, and we need to have a better way of working with them.
Highlight: Multi-cloud GitOps is the “Minimum Viable Pattern” One of the goals of this pattern is to provide the minimal pattern that both demonstrates the goals, aims and purpose of the framework and does something that we think users will find interesting and valuable. We plan to use it as a starting point for our own future pattern development; and as such we can point to it as the pattern to clone and start with if a user wants to start their own pattern development from scratch.
New Feature: Hub Cluster Vault instance In this pattern, we introduce the ability to reference upstream helm charts, and pass overrides to them in a native ArgoCD way. The first application we are treating this way is Hashicorp Vault. The use of Vault also allows us to make Vault the authoritative source of truth for secrets in the framework. This also improves our security posture by making it significantly easier to rotate secrets and have OpenShift “do the right thing” by re-deploying and re-starting workloads as necessary.
For the purposes of shipping this pattern as a runnable demonstration, we take certain shortcuts with security that we understand are not best practices - storing the vault keys unencrypted on a developer drive, for example. If you intend to run code derived from this pattern in production, we strongly recommend you consider and follow the practices documented here.
New Feature: External Secrets Operator While it is important to improve the story for secret handling in the pattern space overall, it is also important to provide space for multiple solutions inside patterns. Because of this, we include the external secrets operator, which in the pattern uses vault, but can be used to support a number of other secret providers, and can be extended to support secrets providers that it does not already support. Furthermore, the external secrets approach is less disruptive to existing applications, since it works by managing the secret objects applications are already used to consuming. Vault provides different options for integration, including the agent injector, but this approach is very specific to Vault and not clearly portable.
In a similar note to the feature about Vault and secrets: the approach we take in this release of the pattern has some known security deficiencies. In RHACM prior to 2.5, policies containing secrets will not properly cloak the secrets in the policy objects, and will not properly encrypt the secrets at rest. RHACM 2.5+ includes a fromSecret function that will secure these secrets in transit and at rest in both of these ways. (Of course, any entity with cluster admin access can recover the contents of a secret object in the cluster.) One additional deficiency of this approach is that the lookup function we use in the policies to copy secrets only runs when the policy object is created or refreshed - which means there is not a mechanism within RHACM presently to detect when a secret has changed and the policy needs to be refreshed. We are hoping this functionality will be included in RHACM 2.6.
New Feature: clusterGroups can have multiple cluster members Using Advanced Cluster Management, we can inject per-cluster configuration into the ArgoCD application definition. We do this, for example, with the global.hubClusterDomain and global.localClusterDomain variables, which are available to use in helm templates in applications that use the framework.
This enables one of our key new features, the ability to deploy multiple clusters that differ only in local, cluster-defined ways (such as the FQDNs that they would publish for their routes). This is a need we determined when we were working on Industrial Edge, where we had to add the FQDN of the local cluster to a config map, for use in a browser application that was defined in kubernetes, but runs in a user’s browser.
The config-demo namespace uses a deployment of the Red Hat Universal Base Image of httpd to demonstrate how to use the framework to pass variables from application definition to actual use in config maps. The config-demo app shows the management of a secret defined and securely transferred from the hub cluster to remote clusters, as well as allowing for the use of the hub cluster base domain and the local cluster base domain in configuration of applications running on either the hub or managed clusters.
Where do we go from here? One of the next things we are committed to delivering in the new year is a pattern to extend the concept of GitOps to include elements that are outside of OpenShift and Kubernetes - specifically Red Hat Enterprise Linux nodes, including Red Hat Enterprise Linux For Edge nodes, as well as Red Hat Ansible Automation Platform.
We plan on developing a number of new patterns throughout the new year, which will showcase various technologies. Keep watching this space for updates, and if you would like to get involved, visit our site at https://hybrid-cloud-patterns.io!
`,url:"https://validatedpatterns.io/blog/2022-03-30-multicloud-gitops/",breadcrumb:"/blog/2022-03-30-multicloud-gitops/"},"https://validatedpatterns.io/blog/2022-03-23-acm-mustonlyhave/":{title:"To musthave or to mustonlyhave",tags:[],content:`Recently a user reported an issue when using the multicloud-gitops pattern: Namely, after testing changes in a feature branch (adding a helm application), said changes were not appearing on the remote clusters.
Preamble In the multicloud gitops pattern each cluster has to ArgoCD instances: the &ldquo;main&rdquo; one which has additional rights and a &ldquo;secondary&rdquo; one which is in charge to keep the user applications in sync. The workflow of the initial pattern deployment is the following:
All helm charts and yaml files below are referenced directly from a remote git repo (the GitOps way). The branch being chosen is the one set to locally when running make install. In order to switch branch on an existing pattern, the user needs to run make upgrade. This will trigger a change in a helm variable pointing to the git revision which will then propagate throughout the cluster. make install gets invoked which calls helm install common/install. This will install the main ArgoCD instance on the HUB cluster. Step 1. will also do a helm install of the common/clustergroup chart. This will install a number of helm charts thanks to the customizable content defined in values-hub.yaml. Amongst other things it will install ACM, HashiCorp Vault, the External Secrets operator (and a few more) The helm chart for ACM will install it and push out some cluster policies in order to add the necessary yaml files to configure ArgoCD on the remote clusters that will join the ACM hub. It will create the main ArgoCD instance on the remote cluster and run the common/clustergroup helm chart The common/clustergroup chart will do it&rsquo;s thing like on the hub, except this time it will be the values-region-one.yaml file driving the list of things to be installed. The problem The problem manifested itself in the following way: The user deployed the pattern from the main branch on to the clusters. Then the git branched was changed to something else (feature-branch), a new helm chart/application was added to the regional cluster (so in values-region-one.yaml) and the changes were pushed (git push and make upgrade). After the push, the application would never show up on the regional cluster.
The symptoms After a short investigation, it was clear that something was off when ACM was pushing the common/clustergroup Argo Application on to the regional cluster. We could observe the following yaml:
$ oc get -n openshift-gitops application multicloud-gitops-region-one -o yaml ... project: default source: repoURL: &#39;https://github.com/mbaldessari/multicloud-gitops.git&#39; path: common/clustergroup targetRevision: feature-branch helm: valueFiles: - &gt;- https://github.com/mbaldessari/multicloud-gitops/raw/feature-branch/values-global.yaml - &gt;- https://github.com/mbaldessari/multicloud-gitops/raw/feature-branch/values-region-one.yaml - &gt;- https://github.com/mbaldessari/multicloud-gitops/raw/main/values-global.yaml - &gt;- https://github.com/mbaldessari/multicloud-gitops/raw/main/values-region-one.yaml ... That list under the valueFiles attribute was wrong. It still contained references to the old main branch. To make matters worse they were listed after feature-branch making the value files from the new branch effectively useless. It really seemed like ACM was not really pushing out the changed policy fully. It&rsquo;s as if a merge happened when an existing application was changed.
Resolution The problem turned out to be in how we pushed out the ArgoCD Application via ACM. We did this:
{% raw %}
apiVersion: policy.open-cluster-management.io/v1 kind: Policy metadata: name: {{ .name }}-clustergroup-policy annotations: argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true argocd.argoproj.io/compare-options: IgnoreExtraneous spec: remediationAction: enforce disabled: false policy-templates: - objectDefinition: apiVersion: policy.open-cluster-management.io/v1 kind: ConfigurationPolicy metadata: name: {{ .name }}-clustergroup-config spec: remediationAction: enforce severity: med namespaceSelector: exclude: - kube-* include: - default object-templates: - complianceType: musthave objectDefinition: apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: {{ $.Values.global.pattern }}-{{ .name }} namespace: openshift-gitops finalizers: - argoproj.io/finalizer spec: project: default source: repoURL: {{ coalesce .repoURL $.Values.global.repoURL }} targetRevision: {{ coalesce .targetRevision $.Values.global.targetRevision }} path: {{ default &#34;common/clustergroup&#34; .path }} helm: valueFiles: - &#34;{{ coalesce .valuesDirectoryURL $.Values.global.valuesDirectoryURL }}/values-global.yaml&#34; - &#34;{{ coalesce .valuesDirectoryURL $.Values.global.valuesDirectoryURL }}/values-{{ .name }}.yaml&#34; parameters: - name: global.repoURL value: $ARGOCD_APP_SOURCE_REPO_URL ... {% endraw %}
The problem was entirely into the complianceType: musthave. Quoting the docs at https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.4/html-single/governance/index#configuration-policy-yaml-table we have three possibilities:
mustonlyhave Indicates that an object must exist with the exact name and relevant fields. musthave Indicates an object must exist with the same name as specified object-template. The other fields in the template are a subset of what exists in the object. \`mustnothave\`\` Indicated that an object with the same name or labels cannot exist and need to be deleted, regardless of the specification or rules. So musthave does not imply that the object being applied is identical to what is specified in the policy. The actual consequence of that on a real deployment is the following:
Existing object
- foo: - a - b If the above template gets changed in ACM:
- foo: - c - d The end result in case of &lsquo;musthave&rsquo; complianceType will be:
- foo: - a - b - c - d Changing the complianceType to mustonlyhave fixed the issue as it enforced the template fully on the remote cluster and solved this issue.
Thanks Special thanks to Ilkka Tengvall and Christian Stark for their help and patience.
`,url:"https://validatedpatterns.io/blog/2022-03-23-acm-mustonlyhave/",breadcrumb:"/blog/2022-03-23-acm-mustonlyhave/"},"https://validatedpatterns.io/blog/2021-12-31-medical-diagnosis/":{title:"Medical Diagnosis",tags:[],content:`Validated Pattern: Medical Diagnosis Our team recently completed the development of a validated pattern that showcases the capabilities we have at our fingertips when we combine OpenShift and other cutting edge Red Hat technologies to deliver a solution.
We&rsquo;ve taken an application defined imperatively in an Ansible playbook and converted it into GitOps style declarative kubernetes resources. Using the validated pattern framework we are able to deploy, manage and integrate with multiple cutting edge Red Hat technologies, and provide a capability that the initial deployment strategy didn&rsquo;t have available to it: a lifecycle. Everything you need to take this pattern for a spin is in git.
Pattern Workflow The purpose of this pattern is to show how medical facilities can take full advantage of trained AI/ML models to identify anomalies in the body like pneumonia. From the medical personnel point of view it works with medical imagery equipment submitting an X-ray image into the application to start the workflow.
The image is uploaded to an S3-compatible object storage. This upload triggers an event from the storage, “a new image has been uploaded”, that is sent into a Kafka topic. This topic is consumed by a KNative Eventing listener that triggers the launch of a KNative Serving instance. This instance is a containerimage with the AI/ML model and the needed processing functions. Based on the information from the event it received, the container retrieves the image from the object store, pre-processes it, makes a prediction on the risk of pneumonia using the AI/ML model, and saves the result. A notification of those results is sent to the medical staff as well.
For a recorded demo deploying the pattern and seeing the dashboards available to the user, check out our docs page!
Pattern Deployment To deploy this pattern, follow the instructions outlined on the getting-started page.
What&rsquo;s happening? During the bootstrapping of the pattern, the initial openshift-gitops operator is deployed with the necessary custom resource definitions, and custom resources to deploy the datacenter-&lt;validated-pattern&gt; with references to the appropriate git repository and branch. Once the argoCD application deploys it will create all of the common resources which include advanced cluster manager, vault, and openshift-gitops. The pattern deployment begins with argo applying the helm templates to the cluster, ultimately resulting in all resources deploying and the xraylab dashboard being available via its route.
The charts for the pattern deployment are located: $GIT_REPO_DIR/charts/datacenter/
Pattern Deployed Technology Operator Upstream Project openshift data foundation (odf) ceph, rook, noobaa openshift-gitops argoCD openshift serverless knative amq streams kafka opendatahub opendatahub grafana grafana Challenges With the imperative dependence on the originating content, there were some resources that didn&rsquo;t align 1:1 and needed to be overcome. For example, there are a number of tasks that are interrogating the cluster for information to transform into a variable and finally apply that variable to some resource. As you can imagine, this can be very challenging when you&rsquo;re declaring the state of your cluster. In order to maneuver around these imperative actions we took what we could and created openshift jobs to execute the task.
Conclusion Speed, accuracy, efficiency all come to mind when considering what this pattern provides. Patients get the treatment they need, when they need it because we&rsquo;re able to use technology to quickly and accurately diagnosis anomalies detected in X-rays. The validated patterns framework enables administrators to quickly meet their user demands by providing solutions that only require them to bring their own data to complete the last 20-25% of the architecture.
`,url:"https://validatedpatterns.io/blog/2021-12-31-medical-diagnosis/",breadcrumb:"/blog/2021-12-31-medical-diagnosis/"},"https://validatedpatterns.io/patterns/multicloud-gitops/":{title:"Multicloud GitOps",tags:[],content:` About the Multicloud GitOps pattern Use case Use a GitOps approach to manage hybrid and multi-cloud deployments across both public and private clouds.
Enable cross-cluster governance and application lifecycle management.
Securely manage secrets across the deployment.
Based on the requirements of a specific implementation, certain details might differ. However, all validated patterns that are based on a portfolio architecture, generalize one or more successful deployments of a use case.
Background Organizations are aiming to develop, deploy, and operate applications on an open hybrid cloud in a stable, simple, and secure way. This hybrid strategy includes multi-cloud deployments where workloads might be running on multiple clusters and on multiple clouds, private or public. This strategy requires an infrastructure-as-code approach: GitOps. GitOps uses Git repositories as a single source of truth to deliver infrastructure-as-code. Submitted code checks the continuous integration (CI) process, while the continuous delivery (CD) process checks and applies requirements for things like security, infrastructure-as-code, or any other boundaries set for the application framework. All changes to code are tracked, making updates easy while also providing version control should a rollback be needed.
About the solution This architecture covers hybrid and multi-cloud management with GitOps as shown in following figure. At a high level this requires a management hub, for DevOps and GitOps, and infrastructure that extends to one or more managed clusters running on private or public clouds. The automated infrastructure-as-code approach can manage the versioning of components and deploy according to the infrastructure-as-code configuration.
Benefits of Hybrid Multicloud management with GitOps:
Unify management across cloud environments.
Dynamic infrastructure security.
Infrastructural continuous delivery best practices.
Figure 1. Overview of the solution including the business drivers, management hub, and the clusters under management In the following figure, logically, this solution can be viewed as being composed of an automation component, unified management including secrets management, and the clusters under management, all running on top of a user-chosen mixture of on-premise data centers and public clouds.
Figure 2. Logical diagram of hybrid multi-cloud management with GitOps About the technology The following technologies are used in this solution:
Red Hat OpenShift Platform An enterprise-ready Kubernetes container platform built for an open hybrid cloud strategy. It provides a consistent application platform to manage hybrid cloud, public cloud, and edge deployments. It delivers a complete application platform for both traditional and cloud-native applications, allowing them to run anywhere. OpenShift has a pre-configured, pre-installed, and self-updating monitoring stack that provides monitoring for core platform components. It also enables the use of external secret management systems, for example, HashiCorp Vault in this case, to securely add secrets into the OpenShift platform.
Red Hat OpenShift GitOps A declarative application continuous delivery tool for Kubernetes based on the ArgoCD project. Application definitions, configurations, and environments are declarative and version controlled in Git. It can automatically push the desired application state into a cluster, quickly find out if the application state is in sync with the desired state, and manage applications in multi-cluster environments.
Red Hat Advanced Cluster Management for Kubernetes Controls clusters and applications from a single console, with built-in security policies. Extends the value of Red Hat OpenShift by deploying apps, managing multiple clusters, and enforcing policies across multiple clusters at scale.
Red Hat Ansible Automation Platform Provides an enterprise framework for building and operating IT automation at scale across hybrid clouds including edge deployments. It enables users across an organization to create, share, and manage automation, from development and operations to security and network teams.
Hashicorp Vault Provides a secure centralized store for dynamic infrastructure and applications across clusters, including over low-trust networks between clouds and data centers.
This solution also uses a variety of observability tools including the Prometheus monitoring and Grafana dashboard that are integrated with OpenShift as well as components of the Observatorium meta-project which includes Thanos and the Loki API.
Overview of the architectures The following figure provides a schematic diagram overview of the complete solution including both components and data flows.
Figure 3. Overview schematic diagram of the complete solution Subsequent schematic diagrams provide details on:
Bootstrapping the management hub (Figure 4)
Hybrid multi-cloud GitOps (Figure 5)
Dynamic security management (Figure 6)
Bootstrapping the management hub The following figure provides a schematic diagram showing the setup of the management hub using Ansible playbooks.
Figure 4. Schematic diagram of bootstrapping the management hub Set up the OpenShift Container Platform that hosts the Management Hub. The OpenShift installation program provides flexible ways to install OpenShift. An Ansible playbook starts the installation with necessary configurations.
Ansible playbooks deploy and configure Red Hat Advanced Cluster Management for Kubernetes and, later, other supporting components such as external secrets management, on top of the provisioned OpenShift cluster.
Another Ansible playbook installs HashiCorp Vault, a Red Hat partner product chosen for this solution that can be used to manage secrets for OpenShift clusters.
An Ansible playbook configures and installs the Openshift GitOps operator on the hub cluster. This deploys the Openshift GitOps instance to enable continuous delivery.
Hybrid Multicloud GitOps The following figure provides a schematic diagram showing remaining activities associated with setting up the management hub and clusters using Red Hat Advanced Cluster Management.
Figure 5. Schematic diagram of hybrid multi-cloud management with GitOps Manifest and configuration are set as code template in the form of a Kustomization YAML file. The file describes the desired end state of the managed cluster. When complete, the Kustomization YAML file is pushed into the source control management repository with a version assigned to each update.
OpenShift GitOps monitors the repository and detects changes in the repository.
OpenShift GitOps creates and updates the manifest by creating Kubernetes objects on top of Red Hat Advanced Cluster Management.
Red Hat Advanced Cluster Management provisions, updates, or deletes managed clusters and configuration according to the manifest. In the manifest, you can configure what cloud provider the cluster will be on, the name of the cluster, infrastructure node details and worker node. Governance policy can also be applied as well as provision an agent in the cluster as the bridge between the control center and the managed cluster.
OpenShift GitOps continuously monitors the code repository and the status of the clusters reported back to Red Hat Advanced Cluster Management. Any configuration drift or in case of any failure, OpenShift GitOps will automatically try to remediate by applying the manifest or by displaying alerts for manual intervention.
Dynamic security management The following figure provides a schematic diagram showing how secrets are handled in this solution.
Figure 6. Schematic showing the setup and use of external secrets management During setup, the token to securely access HashiCorp Vault is stored in Ansible Vault. It is encrypted to protect sensitive content.
Red Hat Advanced Cluster Management for Kubernetes acquires the token from Ansible Vault during install and distributes it among the clusters. As a result, you have centralized control over the managed clusters through RHACM.
To allow the cluster access to the external vault, you must set up the external secret management with Helm in this study. OpenShift Gitops is used to deploy the external secret object to a managed cluster.
External secret management fetches secrets from HashiCorp Vault by using the token that was generated in step 2 and constantly monitors for updates.
Secrets are created in each namespace, where applications can use them.
Additional resources View and download all of the diagrams above from the Red Hat Portfolio Architecture open source tooling site.
Next steps Deploy the management hub using Helm.
Add a managed cluster to deploy the managed cluster piece using ACM.
`,url:"https://validatedpatterns.io/patterns/multicloud-gitops/",breadcrumb:"/patterns/multicloud-gitops/"},"https://validatedpatterns.io/patterns/industrial-edge/":{title:"Industrial Edge",tags:[],content:`Industrial Edge Pattern NOTE
Industrial Edge on OpenShift Container Platform 4.12 fails CI due to a Seldon issue. This only affects the Anomaly Detection AI/ML portion of the pattern. The rest of the pattern functions as designed. For more information on the Seldon issue, see https://github.com/SeldonIO/seldon-core/issues/4339.
Red Hat Validated Patterns are detailed deployments created for different use cases. These pre-defined computing configurations bring together the Red Hat portfolio and technology ecosystem to help you stand up your architectures faster. Example application code is provided as a demonstration, along with the various open source projects and Red Hat products required for the deployment to work. Users can then modify the pattern for their own specific application.
Use Case: Boosting manufacturing efficiency and product quality with artificial intelligence/machine learning (AI/ML) out to the edge of the network.
Background: Microcontrollers and other types of simple computers have long been widely used on factory floors and processing plants to monitor and control the many machines required to implement the many machines required to implement many modern manufacturing workflows. The manufacturing industry has consistently used technology to fuel innovation, production optimization, and operations. However, historically, control systems were mostly “dumb” in that they mostly took actions in response to pre-programmed triggers and heuristics. For example, predictive maintenance commonly took place on either a set length of time or the number of hours was in service. Supervisory control and data acquisition (SCADA) has often been used to collectively describe these hardware and software systems, which mostly functioned independently of the company’s information technology (IT) systems. Companies increasingly see the benefit of bridging these operational technology (OT) systems with their IT. Factory systems can be much more flexible as a result. They can also benefit from newer technologies such as AI/ML, thereby allowing for tasks like maintenance to be scheduled based on multiple real-time measurements rather than simple programmed triggers while bringing processing power closer to data.
Solution Overview Figure 1. Industrial edge solution overview.
Figure 1 provides an overview of the industrial edge solution. It is applicable across a number of verticals including manufacturing.
This solution:
Provides real-time insights from the edge to the core datacenter Secures GitOps and DevOps management across core and factory sites Provides AI/ML tools that can reduce maintenance costs Different roles within an organization have different concerns and areas of focus when working with this distributed AL/ML architecture across two logical types of sites: the core datacenter and the factories. (As shown in Figure 2.)
The core datacenter. This is where data scientists, developers, and operations personnel apply the changes to their models, application code, and configurations. The factories. This is where new applications, updates and operational changes are deployed to improve quality and efficiency in the factory.. Figure 2. Mapping of organizational roles to architectural areas.
Figure 3. Overall data flows of solution.
Figure 3 provides a different high-level view of the solution with a focus on the two major dataflow streams.
Moving sensor data and events from the operational/shop floor edge towards the core. The idea is to centralize as much as possible, but decentralize as needed. For example, sensitive production data might not be allowed to leave the premises. Think of a temperature curve of an industrial oven; it might be considered crucial intellectual property of the customer. Or the sheer amount of raw data (maybe 10,000 events per second) might be too expensive to transfer to a cloud datacenter. In the above diagram, this is from left to right. In other diagrams the edge / operational level is usually at the bottom and the enterprise/cloud level at the top. Thus, this is also referred to as northbound traffic.
Push code, configuration, master data, and machine learning models from the core (where development, testing, and training is happening) towards the edge / shop floors. As there might be 100 plants with 1000s of lines, automation and consistency is key. In the above diagram, this is from right to left, in a top/down view, it is called southbound traffic.
Logical Diagrams Figure 4: Industrial Edge solution as logically and physically distributed across multiple sites.
The following technology was chosen for this solution as depicted logically in Figure 4.
The Technology Red Hat OpenShift is an enterprise-ready Kubernetes container platform built for an open hybrid cloud strategy. It provides a consistent application platform to manage hybrid cloud, public cloud, and edge deployments. It delivers a complete application platform for both traditional and cloud-native applications, allowing them to run anywhere.
Red Hat Application Foundations (also sold as Red Hat Integration) includes frameworks and capabilities for designing, building, deploying, connecting, securing, and scaling cloud-native applications, including foundational patterns like microservices, API-first, and data streaming. When combined with Red Hat OpenShift, Application Foundations creates a hybrid cloud platform for development and operations teams to build and modernize applications efficiently and with attention to security, while balancing developer choice and flexibility with operational control. It includes, among other components::
Red Hat Runtimes is a set of products, tools, and components for developing and maintaining cloud-native applications. It offers lightweight runtimes and frameworks for highly distributed cloud architectures, such as microservices. Built on proven open source technologies, it provides development teams with multiple modernization options to enable a smooth transition to the cloud for existing applications.
Red Hat AMQ is a massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. It offers a distributed backbone that allows microservices and other applications to share data with high throughput and low latency.
Red Hat Data Foundation is software-defined storage for containers. Engineered as the data and storage services platform for Red Hat OpenShift, Red Hat Data Foundation helps teams develop and deploy applications quickly and efficiently across clouds. It is based on the open source Ceph, Rook, and Noobaa projects.
Red Hat Advanced Cluster Management for Kubernetes (RHACM) controls clusters and applications from a single console, with built-in security policies. It extends the value of Red Hat OpenShift by deploying applications, managing multiple clusters, and enforcing policies across multiple clusters at scale.
Red Hat Enterprise Linux is the world’s leading enterprise Linux platform. It’s an open source operating system (OS). It’s the foundation from which you can scale existing apps—and roll out emerging technologies—across bare-metal, virtual, container, and all types of cloud environments.
Architectures Edge manufacturing with messaging and ML Figure 5: Industrial Edge solution showing messaging and ML components schematically.
As shown in Figure 5, data coming from sensors is transmitted over MQTT (Message Queuing Telemetry Transport) to Red Hat AMQ, which routes sensor data for two purposes: model development in the core data center and live inference in the factory data centers. The data is then relayed on to Red Hat AMQ for further distribution within the factory datacenter and out to the core datacenter. MQTT is the most commonly used messaging protocol for Internet of Things (IoT) applications.
The lightweight Apache Camel K, a lightweight integration framework built on Apache Camel that runs natively on Kubernetes, provides MQTT (Message Queuing Telemetry Transport) integration that normalizes and routes sensor data to the other components.
That sensor data is mirrored into a data lake that is provided by Red Hat OpenShift Data Foundation. Data scientists then use various tools from the open source Open Data Hub project to perform model development and training, pulling and analyzing content from the data lake into notebooks where they can apply ML frameworks.
Once the models have been tuned and are deemed ready for production, the artifacts are committed to git which kicks off an image build of the model using OpenShift Pipelines (based on the upstream Tekton), a serverless CI/CD system that runs pipelines with all the required dependencies in isolated containers.
The model image is pushed into OpenShift’s integrated registry running in the core datacenter which is then pushed back down to the factory datacenter for use in inference.
Figure 6: Industrial Edge solution showing network flows schematically.
As shown in Figure 6, in order to protect the factories and operations infrastructure from cyber attacks, the operations network needs to be segregated from the enterprise IT network and the public internet. The factory machinery, controllers, and devices need to be further segregated from the factory data center and need to be protected behind a firewall.
Edge manufacturing with GitOps Figure 7: Industrial Edge solution showing a schematic view of the GitOps workflows.
GitOps is an operational framework that takes DevOps best practices used for application development such as version control, collaboration, compliance, and CI/CD, and applies them to infrastructure automation. Figure 6 shows how, for these industrial edge manufacturing environments, GitOps provides a consistent, declarative approach to managing individual cluster changes and upgrades across the centralized and edge sites. Any changes to configuration and applications can be automatically pushed into operational systems at the factory.
Secrets exchange and management Figure 8: Schematic view of secrets exchange and management in an Industrial Edge solution.
Authentication is used to securely deploy and update components across multiple locations. The credentials are stored using a secrets management solution like Hashicorp Vault. The external secrets component is used to integrate various secrets management tools (AWS Secrets Manager, Google Secrets Manager, Azure Key Vault). As shown in Figure 7, these secrets are then passed to Red Hat Advanced Cluster Management for Kubernetes (RHACM) which pushes the secrets to the RHACM agent at the edge clusters based on policy. RHACM is also responsible for providing secrets to OpenShift for GitOps workflows( using Tekton and Argo CD).
For logical, physical and dataflow diagrams, please see excellent work done by the Red Hat Portfolio Architecture team
Demo Scenario This scenario is derived from the MANUela work done by Red Hat Middleware Solution Architects in Germany in 2019/20. The name MANUela stands for MANUfacturing Edge Lightweight Accelerator, you will see this acronym in a lot of artifacts. It was developed on a platform called stormshift.
The demo has been updated 2021 with an advanced GitOps framework.
Figure 9. High-level demo summary. The specific example is machine condition monitoring based on sensor data in an industrial setting, using AI/ML. It could be easily extended to other use cases such as predictive maintenance, or other verticals.
The demo scenario reflects the data flows described earlier and shown in Figure 3 by having three layers.
Line Data Server: the far edge, at the shop floor level.
Factory Data Center: the near edge, at the plant, but in a more controlled environment.
Central Data Center: the cloud/core, where ML model training, application development, testing, and related work happens. (Along with ERP systems and other centralized functions that are not part of this demo.)
The northbound traffic of sensor data is visible in Figure 9. It flows from the sensor at the bottom via MQTT to the factory, where it is split into two streams: one to be fed into an ML model for anomaly detection and another one to be streamed up to the central data center via event streaming (using Kafka) to be stored for model training.
The southbound traffic is abstracted in the App-Dev / Pipeline box at the top. This is where GitOps kicks in to push config or version changes down into the factories.
Download diagrams View and download all of the diagrams above in our open source tooling site.
[Open Diagrams]
Pattern Structure `,url:"https://validatedpatterns.io/patterns/industrial-edge/",breadcrumb:"/patterns/industrial-edge/"},"https://validatedpatterns.io/patterns/medical-diagnosis/":{title:"Medical Diagnosis",tags:[],content:` Background This Validated Pattern is based on a demo implementation of an automated data pipeline for chest Xray analysis previously developed by Red Hat. The original demo can be found here. It was developed for the US Department of Veteran Affairs.
This validated pattern includes the same functionality as the original demonstration. The difference is that we use the GitOps framework to deploy the pattern including operators, creation of namespaces, and cluster configuration. Using GitOps provides a much more efficient means of doing continuous deployment.
What does this pattern do?:
Ingest chest Xrays from a simulated Xray machine and puts them into an objectStore based on Ceph.
The objectStore sends a notification to a Kafka topic.
A KNative Eventing Listener to the topic triggers a KNative Serving function.
An ML-trained model running in a container makes a risk assessment of Pneumonia for incoming images.
A Grafana dashboard displays the pipeline in real time, along with images incoming, processed and anonymized, as well as full metrics collected from Prometheus.
This pipeline is showcased in this video.
This validated pattern is still under development. Any questions or concerns please contact Jonny Rickard or Lester Claudio.
Solution elements How to use a GitOps approach to keep in control of configuration and operations
How to deploy AI/ML technologies for medical diagnosis using GitOps.
Red Hat Technologies Red Hat OpenShift Container Platform (Kubernetes)
Red Hat OpenShift GitOps (ArgoCD)
Red Hat AMQ Streams (Apache Kafka Event Broker)
Red Hat OpenShift Serverless (Knative Eventing, Knative Serving)
Red Hat OpenShift Data Foundations (Cloud Native storage)
Grafana dashboard (OpenShift Grafana Operator)
Open Data Hub
S3 storage
Architecture In this iteration of the pattern there is no edge component . Future releases have planned Edge deployment capabilities as part of the pattern architecture.
Components are running on OpenShift either at the data center or at the medical facility (or public cloud running OpenShift).
Physical Schema The diagram below shows the components that are deployed with the various networks that connect them.
The diagram below shows the components that are deployed with the the data flows and API calls between them.
Recorded Demo What Next Getting started Deploy the Pattern
Visit the repository
`,url:"https://validatedpatterns.io/patterns/medical-diagnosis/",breadcrumb:"/patterns/medical-diagnosis/"},"https://validatedpatterns.io/ci/":{title:"CI Status",tags:[],content:`These are the latest results of the Validated Patterns CI test runs.
Note Note Industrial Edge is known to be broken on 4.13 due to an unavailable dependency (Seldon core). No ETA `,url:"https://validatedpatterns.io/ci/",breadcrumb:"/ci/"},"https://validatedpatterns.io/patterns/cockroachdb/":{title:"Cockroach",tags:[],content:`Cockroach A multicloud pattern using cockroachdb and submariner, deployed via RHACM.
Repo
`,url:"https://validatedpatterns.io/patterns/cockroachdb/",breadcrumb:"/patterns/cockroachdb/"},"https://validatedpatterns.io/patterns/connected-vehicle-architecture/":{title:"Connected Vehicle Architecture",tags:[],content:`Connected Vehicle Architecture A distributed cloud-native application that implements key aspects of a modern IoT architecture.
Repo
`,url:"https://validatedpatterns.io/patterns/connected-vehicle-architecture/",breadcrumb:"/patterns/connected-vehicle-architecture/"},"https://validatedpatterns.io/contribute/":{title:"Contribute to Validated Patterns",tags:[],content:`Find out how you can contribute to the Validated Patterns project.
`,url:"https://validatedpatterns.io/contribute/",breadcrumb:"/contribute/"},"https://validatedpatterns.io/ci/internal/":{title:"Internal CI Status",tags:[],content:`These are the latest results of the Validated Patterns CI test runs. The links on this page are internal Red Hat links and require a valid Red Hat login to access them.
`,url:"https://validatedpatterns.io/ci/internal/",breadcrumb:"/ci/internal/"},"https://validatedpatterns.io/patterns/kong-gateway/":{title:"Kong",tags:[],content:`Kong A pattern for Kong Gateway Control Plane and Data Plane demo
Repo
`,url:"https://validatedpatterns.io/patterns/kong-gateway/",breadcrumb:"/patterns/kong-gateway/"},"https://validatedpatterns.io/learn/":{title:"Learn about Validated Patterns",tags:[],content:`Find out more information about Validated Patterns and how they work.
`,url:"https://validatedpatterns.io/learn/",breadcrumb:"/learn/"},"https://validatedpatterns.io/patterns/multicloud-gitops-portworx/":{title:"Multicloud GitOps with Portworx Enterprise",tags:[],content:`Multicloud GitOps with Portworx Enterprise Some details will differ based on the requirements of a specific implementation but all validated patterns, based on a portfolio architecture, generalize one or more successful deployments of a use case.
Use case:
Use a GitOps approach to manage hybrid and multi-cloud deployments across both public and private clouds. Enable cross-cluster governance and application lifecycle management. Securely manage secrets across the deployment. Deploy and configure Portworx Enterprise persistent storage for stateful applications. Background: Organizations are aiming to develop, deploy, and operate applications on an open hybrid cloud in a stable, simple, and secure way. This hybrid strategy includes multi-cloud deployments where workloads might be running on multiple clusters and on multiple clouds—private or public. It include Portworx Enterprise for persistent storage and Kubernetes data services required for stateful applications.
This strategy requires an infrastructure-as-code approach: GitOps. GitOps uses Git repositories as a single source of truth to deliver infrastructure-as-code. Submitted code checks the continuous integration (CI) process, while the continuous delivery (CD) process checks and applies requirements for things like security, infrastructure-as-code, or any other boundaries set for the application framework. All changes to code are tracked, making updates easy while also providing version control should a rollback be needed.
Solution overview This architecture covers hybrid and multi-cloud management with GitOps as shown in Figure 1. At a high level this requires a management hub, for DevOps and GitOps, and infrastructure that extends to one or more managed clusters running on private or public clouds. The automated infrastructure-as-code approach can manage the versioning of components and deploy according to the infrastructure-as-code configuration.
Why Hybrid Multicloud management with GitOps ?
Unify management across cloud environments. Dynamic infrastructure security. Infrastructural continuous delivery best practices. Figure 1 shows a high-level overview of the solution including the business drivers, management hub, and the clusters under management.
Logical diagram Figure 2. Logical diagram of hybrid multi-cloud management with GitOps.
As you can see in Figure 2, logically this solution can be viewed as being composed of an automation component, unified management (including secrets management), and the cluster(s) under management—all running on top of a user-chosen mixture of on-prem data center(s) and public cloud(s).
The technology The following technology was chosen for this solution.
Red Hat OpenShift Platform is an enterprise-ready Kubernetes container platform built for an open hybrid cloud strategy. It provides a consistent application platform to manage hybrid cloud, public cloud, and edge deployments. It delivers a complete application platform for both traditional and cloud-native applications, allowing them to run anywhere. OpenShift has a pre-configured, pre-installed, and self-updating monitoring stack that provides monitoring for core platform components. It also enables the use of external secret management systems (like HashiCorp Vault in this case) to securely add secrets into the OpenShift platform.
Red Hat OpenShift GitOps is a declarative application continuous delivery tool for Kubernetes based on the ArgoCD project. Application definitions, configurations, and environments are declarative and version controlled in Git. It can automatically push the desired application state into a cluster, quickly find out if the application state is in sync with the desired state, and manage applications in multi-cluster environments.
Red Hat Advanced Cluster Management for Kubernetes controls clusters and applications from a single console, with built-in security policies. Extends the value of Red Hat OpenShift by deploying apps, managing multiple clusters, and enforcing policies across multiple clusters at scale.
Red Hat Ansible Automation Platform provides an enterprise framework for building and operating IT automation at scale across hybrid clouds including edge deployments. It enables users across an organization to create, share, and manage automation—from development and operations to security and network teams.
Portworx Enterprise provides persistent storage and Kubernetes data services to Red Hat OpenShift. Persistence is necessary for stateful applications in Kubernetes environments. Portworx also provides business continuity with Portworx Backup and Portworx DR products that will be incorporated in a future GitOps pattern.
Hashicorp Vault provides a secure centralized store for dynamic infrastructure and applications across clusters, including over low-trust networks between clouds and data centers.
This solution also uses a variety of observability tools including the Prometheus monitoring and Grafana dashboard that are integrated with OpenShift as well as components of the Observatorium meta-project which includes Thanos and the Loki API.
Architectures Figure 3 provides a schematic diagram overview of the complete solution including both components and data flows.
Subsequent schematic diagrams go into more detail on:
Bootstrapping the management hub (Figure 4) Hybrid multi-cloud GitOps (Figure 5) Dynamic security management (Figure 6) Observability in hybrid multi-cloud environments (Figure 7) Figure 3. Overview schematic diagram of the complete solution.
Bootstrapping the management hub Figure 4. Schematic diagram of bootstrapping the management hub.
As detailed below, Figure 4 provides a schematic diagram showing the setup of the management hub using Ansible playbooks.
Set up the Red Hat OpenShift Platform that hosts the Management Hub. The OpenShift installation program provides flexible ways to install OpenShift. An Ansible playbook kicks off the installation with necessary configurations. Ansible playbooks are again used to deploy and configure Red Hat Advanced Cluster Management for Kubernetes and, later, other supporting components (such as external secrets management) on top of the provisioned OpenShift cluster. Another Ansible playbook installs HashiCorp Vault, a Red Hat partner product chosen for this solution that can be used to manage secrets for OpenShift clusters. An Ansible playbook is used again to configure and trigger the Openshift GitOps operator on the hub cluster. This deploys the Openshift GitOps instance to enable continuous delivery. Hybrid multicloud GitOps Figure 5. Schematic diagram of hybrid multi-cloud management with GitOps.
As detailed below, Figure 5 provides a schematic diagram showing remaining activities associated with setting up the management hub and clusters using Red Hat Advanced Cluster Management.
Manifest and configuration are set as code template in the form of “Kustomization” yaml. It describes the end desire state of how the managed cluster is going to be like. When done, it is pushed into the source control management repository with a version assigned to each update. OpenShift GitOps watches the repository and detects changes in the repository. OpenShift GitOps creates/updates the manifest by creating Kubernetes objects on top of Red Hat Advanced Cluster Management. Red Hat Advanced Cluster Management provision/update/delete managed clusters and configuration according to the manifest. In the manifest, you can configure what cloud provider the cluster will be on, the name of the cluster, infra node details and worker node. Governance policy can also be applied as well as provision an agent in the cluster as the bridge between the control center and the managed cluster. OpenShift GitOps will continuously watch between the code repository and status of the clusters reported back to Red Hat Advanced Cluster Management. Any configuration drift or in case of any failure, it will automatically try to remediate by applying the manifest (Or showing alerts for manual intervention). Dynamic security management Figure 6. Schematic showing the setup and use of external secrets management.
As detailed below, Figure 6 provides a schematic diagram showing how secrets are handled in this solution.
During setup, the token to securely access HashiCorp Vault is stored in Ansible Vault. It is encrypted to protect sensitive content.
Red Hat Advanced Cluster Management for Kubernetes allows us to have centralized control over the managed clusters. It acquires the token from Ansible Vault during install and distributes it among the clusters.
To allow the cluster access to the external vault, we need to set up the external secret management (with Helm in this study). OpenShift Gitops is used to deploy the external secret object to a managed cluster.
External secret management fetches secrets from HashiCorp Vault using the token we created in step 2 and constantly watches for updates. Secrets are created in each namespace, where applications can use them.
Demo Scenario Download diagrams View and download all of the diagrams above in our open source tooling site.
[Open Diagrams]
What Next Deploy the management hub using Helm Add a managed cluster to deploy the managed cluster piece using ACM `,url:"https://validatedpatterns.io/patterns/multicloud-gitops-portworx/",breadcrumb:"/patterns/multicloud-gitops-portworx/"}}</script><script src=/js/lunr.js></script>
<script src=/js/search.js></script></footer></main></div></body></html>