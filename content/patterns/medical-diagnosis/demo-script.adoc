---
title: Demo Script
weight: 60
aliases: /medical-diagnosis/demo/
---

:toc:
:imagesdir: /images
:_content-type: REFERENCE
include::modules/comm-attributes.adoc[]

[id="demo-intro"]

== Introduction
The medical diagnosis pattern integrates multiple Red Hat and Open Source technologies together to create an AI/ML workflow that is able to identify signs of pnuemonia in x-ray images. Within this demo a dashboard is automatically created that provides the CPU and Memory metrics for the pod running the risk assessment application. The dashboard also provides visual representation of the AI/ML workflow from the images being generated at the remote medical facitility to running through the image anonymizer, it also includes the image being scanned along with statistics from the workflow - indicating the probability in which a patient may or may not have pnuemonia. 

[NOTE]
====
We simulate the function of the remote medical facility with an application called `image-generator`
====

[id="demo-objectives"]

== Objectives

In this demo you will complete the following:

* Prepare your local workstation
* Update the pattern repo with your cluster values
* Deploy the pattern
* Access the dashboard


[id="getting-started"]

== Getting Started

* Follow the link:../getting-started[Getting Started Guide] to ensure that you have met all of the prequisites
* Review link:../getting-started/#preparing-for-deployment[Preparing for Deployment] for updating the pattern with your cluster values

[NOTE]
====
This demo begins after `./pattern.sh make install` has been executed
====

[id="demo"]

== Demo

Now that we have deployed the pattern onto our cluster, we can begin to discover what has changed, and then move onto the dashboard.

[id="admin-view"]

=== Administrator View - Review Changes to cluster

Login to your cluster's console with the `kubeadmin` user

Let's check out what operators were installed - In the accordian menu on the left:
* click Operators
* click Installed Operators

Ensure that `All Projects` is selected

// Jonny insert screenshot of installed operators //

If you started with a new cluster then there were no layered products or operators installed. With the Validated Patterns framework we describe or declare what our cluster's desired state is and the GitOps engine does the rest. This includes creating the instance of the operator and any additional configuration between other API's to ensure everything is working together nicely.


[id="dev-view"]

=== Developer View - Review Changes to cluster

Let’s switch to the developer context by click on `Administrator` in the top left corner of the accordian menu then click `Developer`

// Jonny insert screenshot of context switch //

* Change projects to `xraylab-1`
* Click on `topologies`

Look at all of the resources that have been created for this demo application. What we see in this interface is the collection of all components required for this AI/ML workflow to properly execute. There are even more resources and configurations that get deployed but because we don't directly interact with them we won't worry too much about them. The take away here is when you utilize the framework you are able to build in automation just like this which allows your developers to focus on their important developer things.


[id="scale-up"]

=== Scale up the deployment

As we mentioned earlier, we don't have an x-ray machine hanging around that we can use for this demo, so we emulate one by creating an s3 bucket and hosting the x-ray images within it. 

In the "real world" an x-ray would be taken at an edge medical facility and then uploaded to an OpenShift Data Foundations (ODF) S3 compatible bucket in the Core Hospital, triggering the AI/ML workflow. 

To emulate the edge medical facility we use an application called image-generator which (when scaled up) will download the x-rays from s3 and put them in an ODF s3 bucket in the cluster, triggering the AI/ML workflow.



Search for xraylab, then click topologies. When we zoom out we can see the individual resources that are created to make up this application. Let’s start the demonstration by scaling the image-generator deploymentddConfig up to 1. We’ll discuss why we’re doing this in just a moment.

In the “Filter by resource” drop down, select DeploymentConfig and then in the Name search box type “image”

Click on the icon for image-generator, then click details, then scale the pod to 1 by clicking the up arrow next to the pod donut.

Now let’s jump over to the dashboard - Select “Grafana” in the drop down for Filter by resource, then click the grafana icon > open url to go open a browser for the grafana dashboard.

In the grafana dashboard click the dashboards icon, then click Manage, select xraylab-1, and finally select XRay Lab folder

Since we don’t have an xray machine to take these images, we emulate this with an application called image-generator (which we previously scaled up). Using the dashboard as a reference, we see the xrays in the lower left corner, these are images hosted in s3 that image-generator places into the objectstore in the cluster; that objectstore sends a notification to a kafka topic, which triggers a knative eventing function which starts a knative serving function that runs the risk assessment pipeline. 

The dashboard on the right shows the images that have been uploaded, and as they are processed, meaning the AI/ML model has processed them, they then go through an anonymizer which removes an Personally Identifiable Information or PII so they can be reused for further training the model. 

Beneath the dashboard we can see the distribution of images that are normal, unsure or pneumonia has been detected.  We can also see the number of risk assessment containers running as well as the cpu and memory metrics for the pods. 

Because we’re using openshift-serverless, as soon as we stop sending images to the object store, the model will shut down. Let’s jump back over to the openshift console and scale the image-generator back to 0.

Let’s filter for risk assessment and soon we will see the pods starting to terminate on their own. 

That’s it for this demo, I hope that you enjoyed it! For more information on medical diagnosis or the validated patterns please check out our website at https://hybrid-cloud-patterns.io

[id="demo"]

== Demo

This demo will show all of these technologies working together to deliver an AI/ML pipeline for pneumonia detection. Let’s head over to our OpenShift cluster and take a look at the resources that have been deployed.

Log in to the OpenShift Console
Select “Developer” in the left hand accordion menu
Click “Skip Tour”
Under Project, search for and select “xraylab-1”
In the left hand menu, click “Topology”
in the search box, type “image-generator”
click details 
click the “up” arrow to scale the dc to 1 pod
next click the filter by resource box, and select grafana
click the larger icon 

Let’s take a look at our Grafana dashboard -

An imaging-machine at a medical facility takes an x-ray and then sends that x-ray to an objectstore bucket provided by OpenShift Data Foundations, the objectstore sends a notification to a kafka topic, and then a knative-eventing listener configured to listen to that topic triggers a knative-serving function that scales the riskAssessment deployment. The RiskAssessment application is a containerized ML-model trained for object detection and to make an assessment of probability as to if the person in the image may or may not have pneumonia. The data is persisted in the pattern with a mariadb database and is made available to the Grafana dashboard.

[id="demo-wrap-up"]

== Wrap-Up / Summary
The medical diagnosis pattern is more than just the identification and detection of pneumonia in x-ray images. It is an object detection and classification model built on top of Red Hat OpenShift and can be transformed to fit multiple use-cases within the object classification paradigm These use cases include detecting nefarious objects in packages at mail distribution centers, or contraband items hidden in luggage at airports. 

If you have experience installing operators then you know that installing the operator subscription isn't enough by itself, that just enables the API in the cluster. You must also provision an instance of that operator to get the desired effect. With the Validated Patterns framework, we take care of the operator deployment, creating the instance and adding any additional configuration code necessary for the operator and application to be completely functional. This level of integration between applications and operators allows us to
